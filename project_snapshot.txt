PROJE KOD SNAPSHOT (TAM)
Toplam 99 dosya bulundu ve eklendi.
Dahil Edilen Dizinler: .
Dahil Edilen UzantÄ±lar: .toml, .py, .yaml, .yml, .json, .md, .txt, .html, .bat, .sh, .jsx, .js, .json, .css, Dockerfile, docker-compose.yml, Makefile, Procfile, .env, .gitignore, .dockerignore
HariÃ§ Tutulan Desenler/Yollar: __pycache__, .git, .venv, .vscode, .idea, build, dist, *.egg-info, *.pyc, *.so, *.pyd, .pytest_cache, .mypy_cache, .dataset, dataset, .logs, logs, .output, output, inputs, outputs, .tmp, checkpoints, reports, docs/_build, site, node_modules, .DS_Store, Thumbs.db, package-lock.json, yarn.lock, *.lock, *.min.js, *.min.css
================================================================================

========== FILE: docker-compose.yml ==========
services:
  # 1. Redis Servisi
  redis:
    image: redis:alpine
    container_name: azuraforge_redis
    ports: ["6379:6379"]
    volumes: ["redis_data:/data"]

  # 2. PostgreSQL VeritabanÄ± Servisi
  postgres:
    image: postgres:15-alpine
    container_name: azuraforge_postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    restart: always
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 5s
      timeout: 5s
      retries: 5

  # --- ANA PLATFORM SERVÄ°SLERÄ° ---

  # 3. API Servisi
  api:
    container_name: azuraforge_api
    build:
      context: ./api
      dockerfile: Dockerfile
    # === DEÄÄ°ÅÄ°KLÄ°K: Bu satÄ±r kaldÄ±rÄ±ldÄ±. ArtÄ±k Dockerfile'daki CMD kullanÄ±lacak. ===
    # command: start-api 
    ports: ["8000:8000"]
    volumes:
      - ./api:/app
      - ${REPORTS_DIR}:/app/reports
      - ${CACHE_DIR}:/app/.cache
    environment:
      - REDIS_URL=${REDIS_URL}
      - REPORTS_DIR=/app/reports
      - CACHE_DIR=/app/.cache
      - DATABASE_URL=${DATABASE_URL}
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started

  # 4. Worker Servisi
  worker:
    container_name: azuraforge_worker
    build:
      context: ./worker
      dockerfile: Dockerfile
    # === DEÄÄ°ÅÄ°KLÄ°K: Bu satÄ±r kaldÄ±rÄ±ldÄ±. ArtÄ±k Dockerfile'daki CMD kullanÄ±lacak. ===
    # command: start-worker
    volumes:
      - ./worker:/app
      - ${REPORTS_DIR}:/app/reports
      - ${CACHE_DIR}:/app/.cache
    environment:
      - REDIS_URL=${REDIS_URL}
      - REPORTS_DIR=/app/reports
      - CACHE_DIR=/app/.cache
      - AZURAFORGE_DEVICE=gpu
      - DATABASE_URL=${DATABASE_URL}
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # 5. Dashboard Servisi
  dashboard:
    container_name: azuraforge_dashboard
    build:
      context: ./dashboard
      dockerfile: Dockerfile
    # Bu servis Node.js tabanlÄ± olduÄŸu iÃ§in command'Ä±n kalmasÄ± doÄŸru.
    command: npm run dev -- --host 0.0.0.0
    ports: ["5173:5173"]
    volumes:
      - ./dashboard:/app
      - /app/node_modules
    depends_on: [api]

volumes:
  redis_data:
  postgres_data:
========== FILE: README.md ==========
# AzuraForge Platform ğŸš€

**AzuraForge**, yapay zeka modellerini sÄ±fÄ±rdan oluÅŸturmak, eÄŸitmek, canlÄ± olarak takip etmek ve sonuÃ§larÄ±nÄ± interaktif raporlarla analiz etmek iÃ§in tasarlanmÄ±ÅŸ, **olay gÃ¼dÃ¼mlÃ¼, eklenti tabanlÄ± ve daÄŸÄ±tÄ±k bir MLOps platformudur.**

Bu depo, AzuraForge ekosistemindeki tÃ¼m ana servisleri ve kÃ¼tÃ¼phaneleri bir araya getiren **orkestrasyon katmanÄ±dÄ±r**.

## ğŸ›ï¸ Platform Mimarisi ve Felsefesi

AzuraForge, basit bir araÃ§ setinden daha fazlasÄ±dÄ±r; modern yapay zeka sistemlerinin nasÄ±l inÅŸa edilmesi gerektiÄŸine dair bir felsefeyi temsil eder. Bu felsefeyi, yol haritamÄ±zÄ± ve projenin geliÅŸim hikayesini derinlemesine anlamak iÃ§in **[Proje Vizyonu ve Yol HaritasÄ±](./docs/VISION_AND_ROADMAP.md)** belgemizi inceleyin.

Platformumuz dÃ¶rt temel prensip Ã¼zerine kuruludur: **"The AzuraForge Way"**

1.  **SÄ±fÄ±rdan Ä°nÅŸa ve Derin AnlayÄ±ÅŸ:**
    Temel algoritmalarÄ± (`Tensor`, `LSTM` vb.) sÄ±fÄ±rdan yazarak sistem Ã¼zerinde tam kontrol, ÅŸeffaflÄ±k ve derinlemesine bir "know-how" saÄŸlÄ±yoruz. DÄ±ÅŸ dÃ¼nyaya minimal baÄŸÄ±mlÄ±lÄ±kla, "kara kutu"lardan arÄ±nmÄ±ÅŸ bir yapÄ± hedefliyoruz.

2.  **ModÃ¼ler ve Ã–lÃ§eklenebilir Ekosistem:**
    Her bileÅŸen (`api`, `worker`, `learner` vb.) kendi baÄŸÄ±msÄ±z reposunda yaÅŸar, baÄŸÄ±msÄ±z olarak geliÅŸtirilebilir ve kurulabilir. Bu mikroservis yaklaÅŸÄ±mÄ±, projenin bakÄ±mÄ±nÄ± ve Ã¶lÃ§eklenmesini kolaylaÅŸtÄ±rÄ±r.

3.  **Olay GÃ¼dÃ¼mlÃ¼ ve Asenkron AkÄ±ÅŸ:**
    `Celery` ve `Redis Pub/Sub` Ã¼zerine kurulu mimari sayesinde, yoÄŸun model eÄŸitimleri bile sistemi bloklamaz. Bu, platformun en kritik gÃ¼cÃ¼dÃ¼r ve kullanÄ±cÄ±ya akÄ±cÄ±, gerÃ§ek zamanlÄ± bir deneyim sunar. Bu mimarinin detaylarÄ± iÃ§in **[Mimari Belgesi](./docs/ARCHITECTURE.md)**'ne gÃ¶z atÄ±n.

4.  **GeniÅŸletilebilir Eklenti Sistemi:**
    Yeni AI uygulamalarÄ±, platformun Ã§ekirdek koduna dokunmadan, Python'un `entry_points` mekanizmasÄ± kullanÄ±larak sisteme "eklenti" olarak dahil edilebilir. Bu, platformun yeteneklerinin organik olarak bÃ¼yÃ¼mesini saÄŸlar.

---

## âœ¨ Ana Yetenekler

*   **SÄ±fÄ±rdan Ä°nÅŸa EdilmiÅŸ Ã‡ekirdek:** Otomatik tÃ¼rev, `LSTM` gibi geliÅŸmiÅŸ katmanlar ve `Adam` optimizer iÃ§eren, saf Python/NumPy tabanlÄ± bir derin Ã¶ÄŸrenme motoru.
*   **CanlÄ± Deney Takibi:** `WebSocket` aracÄ±lÄ±ÄŸÄ±yla, devam eden bir eÄŸitimin ilerleme Ã§ubuÄŸunu, anlÄ±k kayÄ±p deÄŸerini ve tahmin grafiklerinin canlÄ± evrimini anlÄ±k olarak izleme imkanÄ±.
*   **Dinamik ve Ä°nteraktif Raporlama:** Tamamlanan her deney iÃ§in, `Dashboard` Ã¼zerinden eriÅŸilebilen, `Chart.js` ile Ã§izilmiÅŸ interaktif grafikler ve detaylÄ± metrikler iÃ§eren rapor sayfalarÄ±.
*   **Deney KarÅŸÄ±laÅŸtÄ±rma:** Birden fazla deney sonucunu tek bir arayÃ¼zde gÃ¶rsel olarak karÅŸÄ±laÅŸtÄ±rarak en iyi modeli kolayca belirleme.

---

## ğŸ—ºï¸ Ekosisteme Genel BakÄ±ÅŸ

AzuraForge platformu, aÅŸaÄŸÄ±daki baÄŸÄ±msÄ±z GitHub depolarÄ±ndan oluÅŸur:

| Repo                         | Sorumluluk                                                                       | Teknoloji      |
| ---------------------------- | -------------------------------------------------------------------------------- | -------------- |
| **Ã‡ekirdek KÃ¼tÃ¼phaneler**    |                                                                                  |                |
| `core`                       | Temel tensÃ¶r matematiÄŸi ve otomatik tÃ¼rev (geri yayÄ±lÄ±m) motoru.                   | `Python`, `NumPy` |
| `learner`                    | YÃ¼ksek seviyeli Ã¶ÄŸrenme kÃ¼tÃ¼phanesi (Katmanlar, OptimizatÃ¶rler, Pipeline'lar).     | `Python`       |
| **Uygulama Eklentileri**     |                                                                                  |                |
| `applications`               | Resmi ve test edilmiÅŸ uygulama eklentilerinin katalogunu tutar.                    | `JSON`         |
| `app-stock-predictor`        | GerÃ§ek bir zaman serisi tahmin eklentisi Ã¶rneÄŸi.                                 | `Python`       |
| **Platform Servisleri**      |                                                                                  |                |
| `api`                        | RESTful API ve WebSocket (Pub/Sub) sunan merkezi iletiÅŸim katmanÄ±.                 | `FastAPI`      |
| `worker`                     | Arka plan gÃ¶revlerini (model eÄŸitimi) iÅŸleyen ve raporlarÄ± oluÅŸturan iÅŸÃ§i servisi. | `Celery`, `Redis` |
| `dashboard`                  | React tabanlÄ±, canlÄ± takip ve raporlama yeteneklerine sahip web arayÃ¼zÃ¼.           | `React`, `Vite` |
| **Orkestrasyon (Bu Repo)**   |                                                                                  |                |
| `platform`                   | TÃ¼m servisleri `docker-compose` ile bir araya getirir ve ana dokÃ¼mantasyonu barÄ±ndÄ±rÄ±r. | `Docker`, `YAML` |

---

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§ (Docker Compose ile)

1.  **Docker Desktop'Ä±n yÃ¼klÃ¼ ve Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun.**
2.  **Bu repoyu klonlayÄ±n:** `git clone https://github.com/AzuraForge/platform.git && cd platform`
3.  **.env dosyasÄ±nÄ± oluÅŸturun:** Proje kÃ¶k dizininde `.env.example` dosyasÄ±nÄ± kopyalayarak `.env` adÄ±yla yeni bir dosya oluÅŸturun. (VarsayÄ±lan deÄŸerler genellikle yeterlidir).
    ```bash
    cp .env.example .env
    ```
4.  **Gerekli Dizinleri OluÅŸturun:**
    ```bash
    mkdir -p ./reports ./.cache
    ```
5.  **Platformu baÅŸlatÄ±n:** (Ä°lk baÅŸlatma, imajlar build edileceÄŸi iÃ§in biraz zaman alabilir.)
    ```bash
    docker-compose up --build -d
    ```
6.  **Platforma eriÅŸin:**
    *   **Dashboard:** `http://localhost:5173`
    *   **API DokÃ¼mantasyonu:** `http://localhost:8000/api/v1/docs`
7.  **KeÅŸfedin:** Dashboard'dan bir deney baÅŸlatÄ±n, canlÄ± takip panelini izleyin ve deney bittiÄŸinde "Raporu GÃ¶rÃ¼ntÃ¼le" butonuyla interaktif sonuÃ§larÄ± inceleyin.

---

## ğŸ› ï¸ GeliÅŸtirme ve KatkÄ±da Bulunma

Platformda geliÅŸtirme yapmak, yeni bir eklenti oluÅŸturmak veya projeye katkÄ±da bulunmak iÃ§in aÅŸaÄŸÄ±daki rehberlerimize gÃ¶z atÄ±n. TÃ¼m geliÅŸtirme sÃ¼reÃ§lerimiz, "The AzuraForge Way" prensiplerine dayanmaktadÄ±r.

*   **[GeliÅŸtirme Rehberi](./docs/DEVELOPMENT_GUIDE.md):** Yerel geliÅŸtirme ortamÄ±nÄ±zÄ± nasÄ±l kuracaÄŸÄ±nÄ±zÄ± ve servisleri nasÄ±l Ã§alÄ±ÅŸtÄ±racaÄŸÄ±nÄ±zÄ± Ã¶ÄŸrenin.
*   **[KatkÄ±da Bulunma Rehberi](./docs/CONTRIBUTING.md):** Kodlama standartlarÄ±mÄ±z, commit mesaj formatÄ±mÄ±z ve Pull Request sÃ¼recimiz hakkÄ±nda bilgi edinin.


========== FILE: api/Dockerfile ==========
# Base image olarak Python 3.10'un slim versiyonunu kullan
FROM python:3.10-slim-bullseye

# Gerekli sistem paketlerini kur
RUN apt-get update && \
    apt-get install -y git --no-install-recommends && \
    rm -rf /var/lib/apt/lists/*

# Ã‡alÄ±ÅŸma dizinini ayarla
WORKDIR /app

# === BASÄ°T VE GARANTÄ° YÃ–NTEM ===
# Ã–nce projenin TÃœM dosyalarÄ±nÄ± kopyala
COPY . .

# Åimdi, tÃ¼m dosyalar iÃ§erideyken, tek bir komutla her ÅŸeyi kur.
# -e modu sayesinde scriptler PATH'e eklenir ve baÄŸÄ±mlÄ±lÄ±klar Ã§Ã¶zÃ¼lÃ¼r.
RUN pip install --no-cache-dir -e .[dev]
# === BÄ°TTÄ° ===

# Konteyner baÅŸlatÄ±ldÄ±ÄŸÄ±nda Ã§alÄ±ÅŸtÄ±rÄ±lacak komut
CMD ["start-api"]
========== FILE: api/pyproject.toml ==========
# api/pyproject.toml (TAM KOD - Sadece projeyle ilgili kÄ±sÄ±mlar)
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "azuraforge-api"
version = "0.2.4"
description = "The API server for the AzuraForge Platform."
requires-python = ">=3.8"

dependencies = [
    "azuraforge-learner @ git+https://github.com/AzuraForge/learner.git@v0.1.5",
    "azuraforge-worker @ git+https://github.com/AzuraForge/worker.git@v0.2.4",
    "azuraforge-applications @ git+https://github.com/AzuraForge/applications.git@v0.1.0",
    "fastapi",
    "uvicorn[standard]",
    "pydantic-settings",
    "python-dotenv",
    "pyyaml",
    "redis",
    "SQLAlchemy",
    "psycopg2-binary",
]

[project.scripts]
start-api = "azuraforge_api.main:run_server"

[project.urls]
"Homepage" = "https://github.com/AzuraForge/api"

[project.optional-dependencies]
dev = [
    "pytest",
    "pytest-asyncio",
    "httpx",
]

[tool.pytest.ini_options]
asyncio_mode = "auto"
========== FILE: api/README.md ==========
# AzuraForge API Servisi

Bu servis, AzuraForge platformunun merkezi iletiÅŸim katmanÄ± ve dÄ±ÅŸ dÃ¼nyaya aÃ§Ä±lan aÄŸ geÃ§ididir.

## ğŸ¯ Ana Sorumluluklar

1.  **RESTful API Sunucusu:**
    *   `Dashboard` ve potansiyel diÄŸer istemciler iÃ§in standart HTTP endpoint'leri saÄŸlar (`/experiments`, `/pipelines` vb.).
    *   Gelen istekleri doÄŸrular ve iÅŸlenmesi iÃ§in gÃ¶revleri `Celery` kuyruÄŸuna (Redis) iletir.

2.  **WebSocket Sunucusu:**
    *   Devam eden deneylerin durumunu canlÄ± olarak takip etmek iÃ§in (`/ws/task_status/{task_id}`) WebSocket baÄŸlantÄ±larÄ± sunar.

3.  **Redis Pub/Sub Dinleyicisi:**
    *   `Worker` tarafÄ±ndan yayÄ±nlanan ilerleme mesajlarÄ±nÄ± (`task-progress:*` kanallarÄ±) dinler ve bu mesajlarÄ± ilgili WebSocket istemcisine anÄ±nda iletir.

## ğŸ› ï¸ Yerel GeliÅŸtirme ve Test

Bu servisi yerel ortamda Ã§alÄ±ÅŸtÄ±rmak ve test etmek iÃ§in, ana `platform` reposundaki **[GeliÅŸtirme Rehberi](../../platform/docs/DEVELOPMENT_GUIDE.md)**'ni takip edin.

Servis baÄŸÄ±mlÄ±lÄ±klarÄ± kurulduktan ve sanal ortam aktive edildikten sonra, aÅŸaÄŸÄ±daki komutla API sunucusunu baÅŸlatabilirsiniz:

```bash
# api/ kÃ¶k dizinindeyken
start-api
```

Sunucu `http://localhost:8000` adresinde Ã§alÄ±ÅŸmaya baÅŸlayacaktÄ±r.

**Birim Testleri (YakÄ±nda):**
Birim testlerini Ã§alÄ±ÅŸtÄ±rmak iÃ§in:
```bash
pytest
```

========== FILE: api/setup.py ==========
from setuptools import setup, find_packages

setup(
    # Bu satÄ±r, setuptools'a paketlerin 'src' klasÃ¶rÃ¼nÃ¼n iÃ§inde
    # olduÄŸunu sÃ¶yler.
    package_dir={"": "src"},
    
    # Bu satÄ±r, 'src' klasÃ¶rÃ¼nÃ¼n iÃ§indeki tÃ¼m Python paketlerini
    # (azuraforge_api ve altÄ±ndakiler) otomatik olarak bulur.
    packages=find_packages(where="src"),
)

========== FILE: api/src/azuraforge_api/database.py ==========
# api/src/azuraforge_api/database.py

import os
from sqlalchemy import create_engine, Column, String, JSON, DateTime
from sqlalchemy.orm import sessionmaker, declarative_base
from sqlalchemy.sql import func

DATABASE_URL = os.getenv("DATABASE_URL")
if not DATABASE_URL:
    raise ValueError("API: DATABASE_URL ortam deÄŸiÅŸkeni ayarlanmamÄ±ÅŸ!")

engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# === YENÄ° BÃ–LÃœM: Model tanÄ±mÄ±nÄ± buraya ekliyoruz ===
Base = declarative_base()

class Experiment(Base):
    __tablename__ = "experiments"
    id = Column(String, primary_key=True, index=True)
    task_id = Column(String, index=True, nullable=False)
    batch_id = Column(String, index=True, nullable=True)
    batch_name = Column(String, nullable=True)
    pipeline_name = Column(String, index=True, nullable=False)
    status = Column(String, index=True, default="PENDING")
    config = Column(JSON, nullable=True)
    results = Column(JSON, nullable=True)
    error = Column(JSON, nullable=True)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    completed_at = Column(DateTime(timezone=True), nullable=True)
    failed_at = Column(DateTime(timezone=True), nullable=True)
# === DEÄÄ°ÅÄ°KLÄ°K SONU ===
========== FILE: api/src/azuraforge_api/main.py ==========
# api/src/azuraforge_api/main.py

import uvicorn
from fastapi import FastAPI, APIRouter
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager

from .core.config import settings
from .routes import experiments, pipelines, streaming
# === DEÄÄ°ÅÄ°KLÄ°K BURADA: Kendi veritabanÄ± modÃ¼lÃ¼mÃ¼zden import ediyoruz ===
from .database import Base, engine 
# === DEÄÄ°ÅÄ°KLÄ°K SONU ===

def init_db():
    """VeritabanÄ± tablolarÄ±nÄ± oluÅŸturur."""
    Base.metadata.create_all(bind=engine)

@asynccontextmanager
async def lifespan(app: FastAPI):
    print("API: VeritabanÄ± tablolarÄ± kontrol ediliyor/oluÅŸturuluyor...")
    init_db()
    print("API: VeritabanÄ± hazÄ±r.")
    yield

def create_app() -> FastAPI:
    app = FastAPI(
        title=settings.PROJECT_NAME, 
        version="0.1.0",
        lifespan=lifespan
    )
    
    if settings.CORS_ORIGINS == "*":
        allowed_origins = ["*"]
    else:
        allowed_origins = [origin.strip() for origin in settings.CORS_ORIGINS.split(',')]

    app.add_middleware(
        CORSMiddleware,
        allow_origins=allowed_origins,
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    api_router = APIRouter()
    api_router.include_router(experiments.router)
    api_router.include_router(pipelines.router)
    
    app.include_router(api_router, prefix=settings.API_V1_PREFIX)
    app.include_router(streaming.router)
    
    @app.get("/", tags=["Root"])
    def read_root():
        return {"message": f"Welcome to {settings.PROJECT_NAME}"}
        
    return app

app = create_app()

def run_server():
    print(f"ğŸš€ Starting {settings.PROJECT_NAME}...")
    uvicorn.run("azuraforge_api.main:app", host="0.0.0.0", port=8000, reload=True)
========== FILE: api/src/azuraforge_api/__init__.py ==========

========== FILE: api/src/azuraforge_api/core/config.py ==========
from pydantic_settings import BaseSettings, SettingsConfigDict

class Settings(BaseSettings):
    PROJECT_NAME: str = "AzuraForge API"
    API_V1_PREFIX: str = "/api/v1"
    
    # Yeni CORS ayarÄ±
    # VirgÃ¼lle ayrÄ±lmÄ±ÅŸ URL'ler veya tÃ¼mÃ¼ne izin vermek iÃ§in "*"
    CORS_ORIGINS: str = "*" # VarsayÄ±lan olarak tÃ¼mÃ¼ne izin ver (geliÅŸtirme iÃ§in)
    
    model_config = SettingsConfigDict(env_file=".env", env_file_encoding='utf-8')

settings = Settings()

========== FILE: api/src/azuraforge_api/core/__init__.py ==========

========== FILE: api/src/azuraforge_api/routes/experiments.py ==========
# api/src/azuraforge_api/routes/experiments.py

from fastapi import APIRouter, HTTPException
from typing import List, Dict, Any
from ..services import experiment_service

router = APIRouter(tags=["Experiments"])

@router.get("/experiments", response_model=List[Dict[str, Any]])
def get_all_experiments():
    return experiment_service.list_experiments()

@router.post("/experiments", status_code=202, response_model=Dict[str, Any])
def create_new_experiment(config: Dict[str, Any]):
    return experiment_service.start_experiment(config)

@router.get("/experiments/{task_id}/status", response_model=Dict[str, Any])
def get_experiment_status(task_id: str):
    # Bu endpoint artÄ±k Ã§ok gerekli deÄŸil ama kalabilir.
    return experiment_service.get_task_status(task_id)

# YENÄ° ENDPOINT (read_experiment_report yerine)
@router.get("/experiments/{experiment_id}/details", response_model=Dict[str, Any])
def read_experiment_details(experiment_id: str):
    """
    Belirli bir deneyin tÃ¼m detaylarÄ±nÄ± (config, results, metrics, history)
     iÃ§eren JSON verisini dÃ¶ndÃ¼rÃ¼r.
    """
    try:
        return experiment_service.get_experiment_details(experiment_id)
    except HTTPException as e:
        raise e
========== FILE: api/src/azuraforge_api/routes/pipelines.py ==========
from fastapi import APIRouter, HTTPException
from typing import List, Dict, Any
from ..services import experiment_service

router = APIRouter(tags=["Pipelines"])

@router.get("/pipelines", response_model=List[Dict[str, Any]])
def get_all_available_pipelines():
    return experiment_service.get_available_pipelines()

@router.get("/pipelines/{pipeline_id}/config", response_model=Dict[str, Any])
def get_pipeline_default_config(pipeline_id: str):
    try:
        return experiment_service.get_default_pipeline_config(pipeline_id)
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
========== FILE: api/src/azuraforge_api/routes/streaming.py ==========
import asyncio
import logging
import json
import os
from fastapi import APIRouter, WebSocket, WebSocketDisconnect
import redis.asyncio as redis

router = APIRouter()

async def redis_listener(websocket: WebSocket, task_id: str):
    """Redis Pub/Sub kanalÄ±nÄ± dinler ve gelen mesajlarÄ± WebSocket'e iletir."""
    redis_url = os.environ.get("REDIS_URL", "redis://localhost:6379/0")
    r = await redis.from_url(redis_url)
    pubsub = r.pubsub()
    channel = f"task-progress:{task_id}"
    await pubsub.subscribe(channel)
    
    try:
        while True:
            # `listen` bir coroutine'dir, bu yÃ¼zden `await` edilmelidir.
            message = await pubsub.get_message(ignore_subscribe_messages=True, timeout=1.0)
            if message and message.get("type") == "message":
                # Gelen veri bytes, string'e Ã§evirip JSON olarak parse et.
                data_str = message['data'].decode('utf-8')
                progress_data = json.loads(data_str)
                # UI'Ä±n beklediÄŸi formatla gÃ¶nder
                await websocket.send_json({
                    "state": "PROGRESS",
                    "details": progress_data
                })
            # WebSocket baÄŸlantÄ±sÄ± hala aÃ§Ä±k mÄ± kontrol et
            # Bu, istemci baÄŸlantÄ±yÄ± kapattÄ±ÄŸÄ±nda dÃ¶ngÃ¼den Ã§Ä±kmayÄ± saÄŸlar.
            await asyncio.sleep(0.1) # CPU'yu yormamak iÃ§in kÄ±sa bir bekleme
    except asyncio.CancelledError:
        logging.info(f"Redis listener for task {task_id} cancelled.")
    except Exception as e:
        logging.error(f"Redis listener error for task {task_id}: {e}")
    finally:
        await pubsub.unsubscribe(channel)
        await r.close()
        logging.info(f"Redis listener for task {task_id} cleaned up.")

@router.websocket("/ws/task_status/{task_id}")
async def websocket_task_status(websocket: WebSocket, task_id: str):
    await websocket.accept()
    logging.info(f"WebSocket connection accepted for task: {task_id}")
    
    # Redis dinleyicisini bir arka plan gÃ¶revi olarak baÅŸlat
    listener_task = asyncio.create_task(redis_listener(websocket, task_id))
    
    try:
        # Ä°stemcinin baÄŸlantÄ±yÄ± kapatmasÄ±nÄ± bekle
        # Bu dÃ¶ngÃ¼, baÄŸlantÄ± aÃ§Ä±k olduÄŸu sÃ¼rece Ã§alÄ±ÅŸÄ±r.
        while True:
            await websocket.receive_text() # Bu satÄ±r aslÄ±nda istemciden mesaj beklemez,
                                           # sadece baÄŸlantÄ±nÄ±n kopup kopmadÄ±ÄŸÄ±nÄ± kontrol eder.
    except WebSocketDisconnect:
        logging.warning(f"WebSocket disconnected by client for task: {task_id}")
    finally:
        # Ä°stemci baÄŸlantÄ±yÄ± kapattÄ±ÄŸÄ±nda, arka plandaki Redis dinleyicisini iptal et
        listener_task.cancel()
        # GÃ¶revin bitmesini bekle (kaynaklarÄ±n temizlenmesi iÃ§in)
        await listener_task
        logging.info(f"Closing WebSocket connection for task {task_id}")
========== FILE: api/src/azuraforge_api/routes/__init__.py ==========

========== FILE: api/src/azuraforge_api/services/experiment_service.py ==========
# api/src/azuraforge_api/services/experiment_service.py

import json
import itertools
import uuid
from datetime import datetime
from importlib import resources
from typing import List, Dict, Any, Generator
from fastapi import HTTPException
from celery.result import AsyncResult
from sqlalchemy import desc

from ..database import SessionLocal, Experiment
from azuraforge_worker import celery_app
# Worker'daki pipeline keÅŸfini kullanmak iÃ§in import
# Bu Ã¶nemli! Worker'Ä±n keÅŸfettiÄŸi pipeline'lar burada da API tarafÄ±ndan kullanÄ±labilir olmalÄ±.
# Bunun iÃ§in worker pyproject.toml'da `azuraforge-worker`'Ä± bir baÄŸÄ±mlÄ±lÄ±k olarak gÃ¶sterir ve
# worker'Ä±n `__init__.py`'si `celery_app` ve `AVAILABLE_PIPELINES_AND_CONFIGS`'i export eder.
from azuraforge_worker.tasks.training_tasks import AVAILABLE_PIPELINES_AND_CONFIGS


# --- Helper Fonksiyonlar ---

def _generate_config_combinations(config: Dict[str, Any]) -> Generator[Dict[str, Any], None, None]:
    """
    KonfigÃ¼rasyon iÃ§indeki listeleri tespit eder ve tÃ¼m olasÄ± kombinasyonlarÄ± Ã¼retir.
    """
    varying_params = {}
    static_params = {}
    
    def _traverse_and_split(conf, path=""):
        for key, value in conf.items():
            new_path = f"{path}.{key}" if path else key
            if isinstance(value, list):
                varying_params[new_path] = value
            elif isinstance(value, dict):
                _traverse_and_split(value, new_path)
            else:
                static_params[new_path] = value

    _traverse_and_split(config)

    if not varying_params:
        yield config
        return

    param_names = list(varying_params.keys())
    param_values = list(varying_params.values())
    
    for combo in itertools.product(*param_values):
        new_config = {}
        for key_path, value in static_params.items():
            keys = key_path.split('.')
            d = new_config
            for k in keys[:-1]:
                d = d.setdefault(k, {})
            d[keys[-1]] = value
            
        for i, key_path in enumerate(param_names):
            keys = key_path.split('.')
            d = new_config
            for k in keys[:-1]:
                d = d.setdefault(k, {})
            d[keys[-1]] = combo[i]
            
        yield new_config


# --- API Servis FonksiyonlarÄ± ---

def get_available_pipelines() -> List[Dict[str, Any]]:
    """YÃ¼klÃ¼ tÃ¼m pipeline eklentilerini ve varsayÄ±lan konfigÃ¼rasyon fonksiyonlarÄ±nÄ± dÃ¶ndÃ¼rÃ¼r."""
    official_apps_data = []
    try:
        # azuraforge_applications paketi iÃ§indeki official_apps.json dosyasÄ±nÄ± oku
        with resources.open_text("azuraforge_applications", "official_apps.json") as f:
            official_apps_data = json.load(f)
    except (FileNotFoundError, ModuleNotFoundError) as e:
        # EÄŸer dosya veya modÃ¼l bulunamazsa, logla ve boÅŸ liste dÃ¶n
        print(f"Warning: Could not load official_apps.json or azuraforge_applications module: {e}")
        official_apps_data = []
    
    # Sadece keÅŸfedilmiÅŸ pipeline'larÄ± listele
    available_pipelines = [app for app in official_apps_data if app.get("id") in AVAILABLE_PIPELINES_AND_CONFIGS]
    return available_pipelines

def get_default_pipeline_config(pipeline_id: str) -> Dict[str, Any]:
    """Belirli bir pipeline'Ä±n varsayÄ±lan konfigÃ¼rasyonunu dÃ¶ndÃ¼rÃ¼r."""
    pipeline_info = AVAILABLE_PIPELINES_AND_CONFIGS.get(pipeline_id)
    if not pipeline_info:
        raise ValueError(f"Pipeline '{pipeline_id}' not found.")
    
    get_config_func = pipeline_info.get('get_config_func')
    if not get_config_func:
        return {"message": "No specific default configuration available."}
    return get_config_func()


def start_experiment(config: Dict[str, Any]) -> Dict[str, Any]:
    """Yeni bir veya birden fazla deney baÅŸlatÄ±r (batch)."""
    task_ids = []
    batch_id = str(uuid.uuid4())
    batch_name = config.pop("batch_name", f"Batch-{datetime.now().strftime('%Y-%m-%d-%H%M%S')}")
    
    combinations = list(_generate_config_combinations(config))
    num_combinations = len(combinations)

    for single_config in combinations:
        if num_combinations > 1:
            single_config['batch_id'] = batch_id
            single_config['batch_name'] = batch_name
        else:
            single_config['batch_id'] = None
            single_config['batch_name'] = None
            
        # Celery gÃ¶revini gÃ¶nder
        task = celery_app.send_task("start_training_pipeline", args=[single_config])
        task_ids.append(task.id)

    if num_combinations > 1:
        return {
            "message": f"{num_combinations} experiments submitted as a batch.",
            "batch_id": batch_id,
            "task_ids": task_ids
        }
    else:
        return {"message": "Experiment submitted to worker.", "task_id": task_ids[0]}

def list_experiments() -> List[Dict[str, Any]]:
    """
    VeritabanÄ±ndaki tÃ¼m deneylerin Ã¶zetini ve tam detaylarÄ±nÄ± (config, results)
    en yeniden eskiye doÄŸru listeler.
    """
    db = SessionLocal()
    try:
        # TÃ¼m Experiment objelerini Ã§ek
        experiments_from_db = db.query(Experiment).order_by(desc(Experiment.created_at)).all()
        
        all_experiments_data = []
        for exp in experiments_from_db:
            summary = {
                "experiment_id": exp.id,
                "task_id": exp.task_id,
                "pipeline_name": exp.pipeline_name,
                "status": exp.status,
                "created_at": exp.created_at.isoformat() if exp.created_at else None,
                "completed_at": exp.completed_at.isoformat() if exp.completed_at else None,
                "failed_at": exp.failed_at.isoformat() if exp.failed_at else None,
                "batch_id": exp.batch_id,
                "batch_name": exp.batch_name,
                # config_summary alanÄ±nÄ±, full config'den tÃ¼retelim
                "config_summary": {
                    "ticker": exp.config.get("data_sourcing", {}).get("ticker", "N/A") if exp.config else "N/A",
                    # Epochs ve LR iÃ§in liste veya tekil deÄŸer alabilen uyumlu Ã¶zet
                    "epochs": exp.config.get("training_params", {}).get("epochs", "N/A") if exp.config else "N/A",
                    "lr": exp.config.get("training_params", {}).get("lr", "N/A") if exp.config else "N/A",
                },
                "results_summary": {
                    "final_loss": exp.results.get("final_loss") if exp.results else None,
                    "r2_score": exp.results.get("metrics", {}).get("r2_score") if exp.results else None
                },
                # Tam config, results ve error objelerini doÄŸrudan ekliyoruz!
                "config": exp.config,
                "results": exp.results,
                "error": exp.error
            }
            all_experiments_data.append(summary)
        return all_experiments_data
    finally:
        db.close()

def get_experiment_details(experiment_id: str) -> Dict[str, Any]:
    """Belirli bir deneyin tÃ¼m detaylarÄ±nÄ± veritabanÄ±ndan Ã§eker."""
    db = SessionLocal()
    try:
        exp = db.query(Experiment).filter(Experiment.id == experiment_id).first()
        if not exp:
            raise HTTPException(status_code=404, detail=f"Experiment '{experiment_id}' not found.")
        
        # Bu fonksiyon da zaten tam detay dÃ¶nÃ¼yordu, ama API'nin list_experiments'i gÃ¼ncellendiÄŸi iÃ§in
        # UI'da bu fonksiyona gerek kalmayacak. Ancak yine de geriye dÃ¶nÃ¼k uyumluluk iÃ§in burada bÄ±rakalÄ±m.
        return {
            "experiment_id": exp.id, "task_id": exp.task_id, "pipeline_name": exp.pipeline_name,
            "status": exp.status, "config": exp.config, "results": exp.results, "error": exp.error,
            "created_at": exp.created_at.isoformat() if exp.created_at else None,
            "completed_at": exp.completed_at.isoformat() if exp.completed_at else None,
            "failed_at": exp.failed_at.isoformat() if exp.failed_at else None,
            "batch_id": exp.batch_id, "batch_name": exp.batch_name,
        }
    finally:
        db.close()

def get_task_status(task_id: str) -> Dict[str, Any]:
    """Belirli bir Celery gÃ¶revinin anlÄ±k durumunu dÃ¶ndÃ¼rÃ¼r (Celery'den doÄŸrudan sorgu)."""
    # Bu fonksiyon da UI'da artÄ±k kullanÄ±lmayacak, Ã§Ã¼nkÃ¼ task_progress doÄŸrudan WebSocket'ten geliyor
    task_result = AsyncResult(task_id, app=celery_app)
    return {"status": task_result.state, "details": task_result.info}

========== FILE: api/src/azuraforge_api/services/__init__.py ==========

========== FILE: api/src/azuraforge_api/tasks/training_tasks.py ==========

========== FILE: api/src/azuraforge_api/tasks/__init__.py ==========

========== FILE: api/tests/azuraforge_api/test_api_endpoints.py ==========
import pytest
from httpx import AsyncClient, ASGITransport
from unittest.mock import patch

# Test edilecek FastAPI uygulamasÄ±nÄ± import et
from azuraforge_api.main import app

# pytest'in asenkron testleri Ã§alÄ±ÅŸtÄ±rmasÄ±nÄ± saÄŸlar
pytestmark = pytest.mark.asyncio

# === DEÄÄ°ÅÄ°KLÄ°K BURADA ===
# API'ye yapÄ±lan tÃ¼m Ã§aÄŸrÄ±lar iÃ§in bir istemci oluÅŸturalÄ±m
# 'app' yerine ASGITransport kullanarak istemcinin doÄŸrudan uygulama ile konuÅŸmasÄ±nÄ± saÄŸlÄ±yoruz.
@pytest.fixture
async def async_client():
    transport = ASGITransport(app=app)
    async with AsyncClient(transport=transport, base_url="http://test") as client:
        yield client
# === DEÄÄ°ÅÄ°KLÄ°K SONU ===

async def test_read_root(async_client: AsyncClient):
    """KÃ¶k endpoint'in doÄŸru mesajÄ± dÃ¶ndÃ¼rdÃ¼ÄŸÃ¼nÃ¼ test eder."""
    response = await async_client.get("/")
    assert response.status_code == 200
    assert response.json() == {"message": "Welcome to AzuraForge API"}

@patch('azuraforge_api.services.experiment_service.get_available_pipelines')
async def test_get_all_pipelines(mock_get_pipelines, async_client: AsyncClient):
    """/pipelines endpoint'inin doÄŸru veriyi ve 200 kodunu dÃ¶ndÃ¼rdÃ¼ÄŸÃ¼nÃ¼ test eder."""
    # Servis katmanÄ±nÄ± mock'layarak veritabanÄ± veya dosya sistemi baÄŸÄ±mlÄ±lÄ±ÄŸÄ±nÄ± ortadan kaldÄ±rÄ±yoruz.
    mock_pipelines_data = [
        {"id": "stock_predictor", "name": "Hisse Senedi Fiyat Tahmini"},
        {"id": "weather_forecaster", "name": "Hava Durumu Tahmini"}
    ]
    mock_get_pipelines.return_value = mock_pipelines_data

    response = await async_client.get("/api/v1/pipelines")
    
    assert response.status_code == 200
    assert response.json() == mock_pipelines_data
    # Servis fonksiyonunun Ã§aÄŸrÄ±ldÄ±ÄŸÄ±nÄ± doÄŸrula
    mock_get_pipelines.assert_called_once()


@patch('azuraforge_api.services.experiment_service.start_experiment')
async def test_create_experiment_success(mock_start_experiment, async_client: AsyncClient):
    """Bir deney baÅŸarÄ±yla gÃ¶nderildiÄŸinde 202 kodunu ve task_id'yi dÃ¶ndÃ¼rdÃ¼ÄŸÃ¼nÃ¼ test eder."""
    test_config = {"pipeline_name": "stock_predictor", "data_sourcing": {"ticker": "GOOG"}}
    mock_start_experiment.return_value = {"message": "Experiment submitted", "task_id": "fake-task-id-123"}

    response = await async_client.post("/api/v1/experiments", json=test_config)
    
    assert response.status_code == 202 # Accepted
    assert response.json()["task_id"] == "fake-task-id-123"
    mock_start_experiment.assert_called_once_with(test_config)
========== FILE: app-stock-predictor/Dockerfile ==========
# ========== GÃœNCELLEME: app-stock-predictor/Dockerfile ==========
# Stage 1: Builder
FROM python:3.10-slim-bullseye AS builder

RUN apt-get update && apt-get install -y git --no-install-recommends && rm -rf /var/lib/apt/lists/*

# KRÄ°TÄ°K DÃœZELTME: Ã‡alÄ±ÅŸma dizinini doÄŸrudan 'src' klasÃ¶rÃ¼nÃ¼n iÃ§ine ayarla
WORKDIR /app/src 

# src klasÃ¶rÃ¼nÃ¼n iÃ§eriÄŸini (yani azuraforge_stockapp klasÃ¶rÃ¼nÃ¼) mevcut WORKDIR'e kopyala
COPY src ./src

# pyproject.toml ve setup.py'Ä± bir Ã¼st dizine (/app) kopyala, 
# Ã§Ã¼nkÃ¼ pip install oradan Ã§alÄ±ÅŸacak.
COPY pyproject.toml /app/
COPY setup.py /app/

# pip install komutunu ana paketin kÃ¶k dizininden (/app) Ã§alÄ±ÅŸtÄ±r.
# Bu, setup.py'Ä±n package_dir={"": "src"} ayarÄ±nÄ± doÄŸru algÄ±lamasÄ±nÄ± saÄŸlar.
RUN --mount=type=cache,target=/root/.cache/pip pip install --no-cache-dir /app

# Stage 2: Runtime
FROM python:3.10-slim-bullseye AS runtime

# Runtime'da da aynÄ± Ã§alÄ±ÅŸma dizinini koru
WORKDIR /app/src

# Builder aÅŸamasÄ±ndan kurulu paketi kopyala
COPY --from=builder /usr/local/lib/python3.10/site-packages /usr/local/lib/python3.10/site-packages
# src klasÃ¶rÃ¼nÃ¼n iÃ§eriÄŸini kopyala
COPY --from=builder /app/src ./src

CMD ["python", "-c", "print('AzuraForge App Stock Predictor built successfully!')"]
========== FILE: app-stock-predictor/pyproject.toml ==========
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "azuraforge-app-stock-predictor"
version = "0.1.3" # Versiyonu artÄ±rÄ±yoruz
description = "A stock prediction pipeline application for the AzuraForge platform."
requires-python = ">=3.8"
dependencies = [
    # === DEÄÄ°ÅÄ°KLÄ°K BURADA: learner'Ä±n en son versiyonuna iÅŸaret ediyor ===
    "azuraforge-learner @ git+https://github.com/AzuraForge/learner.git@v0.1.5",
    "yfinance",
    "pandas",
    "scikit-learn",
    "PyYAML",
]

[project.entry-points]
"azuraforge.pipelines" = { stock_predictor = "azuraforge_stockapp.pipeline:StockPredictionPipeline" }
"azuraforge.configs" = { stock_predictor = "azuraforge_stockapp.pipeline:get_default_config" }
========== FILE: app-stock-predictor/README.md ==========
# app-stock-predictor

========== FILE: app-stock-predictor/setup.py ==========
from setuptools import setup, find_packages
setup(
    package_dir={"": "src"},
    packages=find_packages(where="src"),
    # DÃœZELTME: Paket kurulduÄŸunda .yml gibi Python dÄ±ÅŸÄ± dosyalarÄ±n da
    # kopyalanmasÄ±nÄ± saÄŸlar. Bu, API ve Worker loglarÄ±ndaki hatayÄ± Ã§Ã¶zer.
    include_package_data=True, 
    package_data={
        # "azuraforge_stockapp" paketi iÃ§indeki tÃ¼m .yml dosyalarÄ±nÄ± dahil et.
        "azuraforge_stockapp": ["config/*.yml"], 
    },
)
========== FILE: app-stock-predictor/src/azuraforge_stockapp/pipeline.py ==========
# app-stock-predictor/src/azuraforge_stockapp/pipeline.py

import logging
from typing import Any, Dict, Tuple, List

import yaml
from importlib import resources
import pandas as pd
import yfinance as yf

from azuraforge_learner import Sequential, LSTM, Linear
from azuraforge_learner.pipelines import TimeSeriesPipeline

def get_default_config() -> Dict[str, Any]:
    try:
        with resources.open_text("azuraforge_stockapp.config", "stock_predictor_config.yml") as f:
            return yaml.safe_load(f)
    except Exception as e:
        # Bu log, worker'Ä±n ana sÃ¼recinde gÃ¶rÃ¼nmeli
        logging.error(f"Hisse senedi uygulamasÄ± iÃ§in varsayÄ±lan config yÃ¼klenemedi: {e}", exc_info=True)
        return {"error": f"VarsayÄ±lan konfigÃ¼rasyon yÃ¼klenemedi: {e}"}

class StockPredictionPipeline(TimeSeriesPipeline):
    # DÃœZELTME: __init__ metodunu, temel sÄ±nÄ±fÄ± Ã§aÄŸÄ±rmak ve loglamayÄ± doÄŸrulamak iÃ§in geri ekliyoruz.
    def __init__(self, config: Dict[str, Any]):
        # Bu, BasePipeline'in __init__'ini Ã§aÄŸÄ±racak ve self.logger'Ä± ayarlayacaktÄ±r.
        super().__init__(config)
        self.logger.info(f"StockPredictionPipeline (Eklenti) baÅŸarÄ±yla baÅŸlatÄ±ldÄ±.")

    def _load_data_from_source(self) -> pd.DataFrame:
        """Sadece yfinance'ten veri Ã§ekme iÅŸini yapar. Caching bu metodun dÄ±ÅŸÄ±ndadÄ±r."""
        ticker = self.config.get("data_sourcing", {}).get("ticker", "MSFT")
        self.logger.info(f"'_load_data_from_source' Ã§aÄŸrÄ±ldÄ±. Ticker: {ticker}")
        
        data = yf.download(ticker, period="max", progress=False, actions=False, auto_adjust=True)
        if data.empty:
            self.logger.error(f"'{ticker}' iÃ§in yfinance'ten boÅŸ veri dÃ¶ndÃ¼.")
            raise ValueError(f"'{ticker}' iÃ§in veri indirilemedi.")
            
        self.logger.info(f"{len(data)} satÄ±r veri indirildi.")
        return data

    def get_caching_params(self) -> Dict[str, Any]:
        """Ã–nbellek anahtarÄ± iÃ§in sadece ticker'Ä±n yeterli olduÄŸunu belirtir."""
        ticker = self.config.get("data_sourcing", {}).get("ticker", "MSFT")
        self.logger.info(f"'get_caching_params' Ã§aÄŸrÄ±ldÄ±. Ticker: {ticker}")
        return {"ticker": ticker}

    def _get_target_and_feature_cols(self) -> Tuple[str, List[str]]:
        """Bu basit model iÃ§in hedef ve Ã¶zellik aynÄ± sÃ¼tundur: 'Close'."""
        self.logger.info("'_get_target_and_feature_cols' Ã§aÄŸrÄ±ldÄ±. Hedef: Close")
        return "Close", ["Close"]

    def _create_model(self, input_shape: Tuple) -> Sequential:
        """LSTM ve bir Linear katmandan oluÅŸan modeli oluÅŸturur."""
        self.logger.info(f"'_create_model' Ã§aÄŸrÄ±ldÄ±. Girdi ÅŸekli: {input_shape}")
        input_size = input_shape[2] 
        hidden_size = self.config.get("model_params", {}).get("hidden_size", 50)
        
        model = Sequential(
            LSTM(input_size=input_size, hidden_size=hidden_size),
            Linear(hidden_size, 1)
        )
        self.logger.info("LSTM modeli baÅŸarÄ±yla oluÅŸturuldu.")
        return model
========== FILE: app-stock-predictor/src/azuraforge_stockapp/__init__.py ==========

========== FILE: app-stock-predictor/src/azuraforge_stockapp/config/stock_predictor_config.yml ==========
# app-stock-predictor/src/azuraforge_stockapp/config/stock_predictor_config.yml

pipeline_name: "stock_predictor"

data_sourcing:
  ticker: "MSFT"

# YENÄ°: Ã–zellik mÃ¼hendisliÄŸi ve dÃ¶nÃ¼ÅŸÃ¼m ayarlarÄ±
feature_engineering:
  target_col_transform: "log" # "log" veya "none" olabilir

model_params:
  sequence_length: 60
  hidden_size: 5

training_params:
  epochs: 50
  lr: 0.001
  optimizer: "adam"
  test_size: 0.2
  validate_every: 5

system:
  caching_enabled: true
  cache_max_age_hours: 24
========== FILE: app-stock-predictor/src/azuraforge_stockapp/config/__init__.py ==========

========== FILE: applications/Dockerfile ==========
# ========== GÃœNCELLEME: applications/Dockerfile ==========
# Stage 1: Builder
FROM python:3.10-slim-bullseye AS builder

RUN apt-get update && apt-get install -y git --no-install-recommends && rm -rf /var/lib/apt/lists/*

WORKDIR /app/src

COPY src ./src
COPY pyproject.toml /app/
COPY setup.py /app/

RUN --mount=type=cache,target=/root/.cache/pip pip install --no-cache-dir /app

# Stage 2: Runtime
FROM python:3.10-slim-bullseye AS runtime

WORKDIR /app/src

COPY --from=builder /usr/local/lib/python3.10/site-packages /usr/local/lib/python3.10/site-packages
COPY --from=builder /app/src ./src

CMD ["python", "-c", "print('AzuraForge Applications Catalog built successfully!')"]
========== FILE: applications/pyproject.toml ==========
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "azuraforge-applications"
version = "0.1.0"
description = "A catalog of official applications for the AzuraForge platform."

========== FILE: applications/README.md ==========
# applications

========== FILE: applications/setup.py ==========
from setuptools import setup, find_packages

setup(
    package_dir={"": "src"},
    packages=find_packages(where="src"),
    # EN Ã–NEMLÄ° KISIM: Paket kurulduÄŸunda .json dosyasÄ±nÄ±n da kopyalanmasÄ±nÄ± saÄŸlar
    include_package_data=True, 
    package_data={
        "azuraforge_applications": ["*.json"], # "azuraforge_apps_catalog" -> "azuraforge_applications"
    },
)


========== FILE: applications/src/azuraforge_applications/official_apps.json ==========
[
  {
    "id": "stock_predictor",
    "name": "Hisse Senedi Fiyat Tahmini",
    "repository": "https://github.com/AzuraForge/app-stock-predictor",
    "description": "LSTM tabanlÄ± hisse senedi fiyat tahmini yapar."
  },
  {
    "id": "weather_forecaster",
    "name": "Hava Durumu Tahmini",
    "repository": "https://github.com/AzuraForge/app-weather-forecaster",
    "description": "Gelecekteki hava durumunu tahmin eder (HenÃ¼z GeliÅŸtirilmedi)."
  }
]

========== FILE: applications/src/azuraforge_applications/__init__.py ==========


========== FILE: core/Dockerfile ==========
# ========== GÃœNCELLEME: core/Dockerfile ==========
# Stage 1: Builder
FROM python:3.10-slim-bullseye AS builder

RUN apt-get update && apt-get install -y git --no-install-recommends && rm -rf /var/lib/apt/lists/*

# KRÄ°TÄ°K DÃœZELTME: Ã‡alÄ±ÅŸma dizinini doÄŸrudan 'src' klasÃ¶rÃ¼nÃ¼n iÃ§ine ayarla
WORKDIR /app/src 

# src klasÃ¶rÃ¼nÃ¼n iÃ§eriÄŸini (yani azuraforge_core klasÃ¶rÃ¼nÃ¼) mevcut WORKDIR'e kopyala
COPY src ./src

# pyproject.toml ve setup.py'Ä± bir Ã¼st dizine (/app) kopyala, 
# Ã§Ã¼nkÃ¼ pip install oradan Ã§alÄ±ÅŸacak.
COPY pyproject.toml /app/
COPY setup.py /app/

# pip install komutunu ana paketin kÃ¶k dizininden (/app) Ã§alÄ±ÅŸtÄ±r.
# Bu, setup.py'Ä±n package_dir={"": "src"} ayarÄ±nÄ± doÄŸru algÄ±lamasÄ±nÄ± saÄŸlar.
RUN --mount=type=cache,target=/root/.cache/pip pip install --no-cache-dir /app

# Stage 2: Runtime
FROM python:3.10-slim-bullseye AS runtime

# Runtime'da da aynÄ± Ã§alÄ±ÅŸma dizinini koru
WORKDIR /app/src

# Builder aÅŸamasÄ±ndan kurulu paketi kopyala
COPY --from=builder /usr/local/lib/python3.10/site-packages /usr/local/lib/python3.10/site-packages
# src klasÃ¶rÃ¼nÃ¼n iÃ§eriÄŸini kopyala
COPY --from=builder /app/src ./src

CMD ["python", "-c", "print('AzuraForge Core library image built successfully!')"]
========== FILE: core/pyproject.toml ==========
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "azuraforge-core"
version = "0.1.3"
authors = [{ name = "Azmi Sahin" }]
description = "The core automatic differentiation engine (Tensor object) for the AzuraForge ecosystem."
readme = "README.md"
requires-python = ">=3.8"
license = { text = "MIT" }
classifiers = ["Programming Language :: Python :: 3"]
dependencies = ["numpy"]

# --- YENÄ° BÃ–LÃœM ---
[project.optional-dependencies]
dev = ["pytest"]

========== FILE: core/README.md ==========
# core

========== FILE: core/setup.py ==========
from setuptools import setup, find_packages
setup(
    package_dir={"": "src"},
    packages=find_packages(where="src"),
)

========== FILE: core/src/azuraforge_core/tensor.py ==========
import os
from typing import Callable, List, Optional, Set, Tuple, Union, Any
import numpy as np
import logging # Loglama iÃ§in import et

# === DEÄÄ°ÅÄ°KLÄ°K BURADA: Cihaz algÄ±landÄ±ÄŸÄ±nda logla ===
# LoglamayÄ± yapÄ±landÄ±r
logging.basicConfig(level=logging.INFO, format='%(asctime)s - CORE - %(levelname)s - %(message)s')

DEVICE = os.environ.get("AZURAFORGE_DEVICE", "cpu").lower()

xp: Any
if DEVICE == "gpu":
    try:
        import cupy
        xp = cupy
        logging.info("âœ… AzuraForge Core: CuPy (GPU) backend successfully loaded.")
    except ImportError:
        import numpy
        xp = numpy
        logging.warning("âš ï¸ AzuraForge Core: AZURAFORGE_DEVICE set to 'gpu' but CuPy not found. Falling back to NumPy (CPU).")
        DEVICE = "cpu"
else:
    import numpy
    xp = numpy
    logging.info("â„¹ï¸ AzuraForge Core: NumPy (CPU) backend is active.")
# === DEÄÄ°ÅÄ°KLÄ°K SONU ===

ArrayType = Any
ScalarType = Union[int, float, bool, np.number, xp.number]

def _empty_backward_op() -> None: pass

class Tensor:
    def __init__(self, data: Any, _children: Tuple["Tensor", ...] = (), _op: str = "", requires_grad: bool = False):
        if isinstance(data, Tensor): self.data = data.data.copy()
        else: 
            # Veriyi doÄŸru cihaza taÅŸÄ±
            try:
                # EÄŸer xp, cupy ise bu veriyi GPU'ya taÅŸÄ±r.
                self.data = xp.array(data, dtype=np.float64)
            except Exception as e:
                # GPU'ya taÅŸÄ±ma sÄ±rasÄ±nda hata olursa (Ã¶rn. CUDA context hatasÄ±) logla
                logging.error(f"Error transferring data to device '{DEVICE}': {e}. Falling back to CPU.")
                self.data = np.array(data, dtype=np.float64)

        self.requires_grad = requires_grad
        self.grad: Optional[ArrayType] = xp.zeros_like(self.data) if requires_grad else None
        self._backward: Callable[[], None] = _empty_backward_op
        self._prev: Set["Tensor"] = set(_children)
        self._op: str = _op

    def backward(self, grad_output: Optional[ArrayType] = None) -> None:
        if not self.requires_grad: return
        topo: List[Tensor] = []
        visited: Set[Tensor] = set()
        def build_topo(v):
            if v not in visited:
                visited.add(v); [build_topo(child) for child in v._prev]; topo.append(v)
        build_topo(self)
        for t in topo:
            if t.grad is not None:
                t.grad.fill(0.0)
        
        self.grad = xp.ones_like(self.data) if grad_output is None else xp.asarray(grad_output, dtype=np.float64).reshape(self.data.shape)
        
        for v in reversed(topo):
            v._backward()

    def to_cpu(self) -> np.ndarray:
        if hasattr(self.data, 'get'): return self.data.get()
        return np.array(self.data, copy=True)

    def __add__(self, other: Any) -> "Tensor":
        other = _ensure_tensor(other)
        out = Tensor(self.data + other.data, (self, other), "+", self.requires_grad or other.requires_grad)
        def _backward():
            if self.requires_grad and self.grad is not None: self.grad += _unbroadcast_to(self.data.shape, out.grad)
            if other.requires_grad and other.grad is not None: other.grad += _unbroadcast_to(other.data.shape, out.grad)
        out._backward = _backward
        return out

    def __mul__(self, other: Any) -> "Tensor":
        other = _ensure_tensor(other)
        out = Tensor(self.data * other.data, (self, other), "*", self.requires_grad or other.requires_grad)
        def _backward():
            if self.requires_grad and self.grad is not None: self.grad += _unbroadcast_to(self.data.shape, other.data * out.grad)
            if other.requires_grad and other.grad is not None: other.grad += _unbroadcast_to(other.data.shape, self.data * out.grad)
        out._backward = _backward
        return out

    def __pow__(self, power: float) -> "Tensor":
        out = Tensor(self.data ** power, (self,), f"**{power}", self.requires_grad)
        def _backward():
            if self.requires_grad and self.grad is not None: self.grad += (power * (self.data ** (power - 1))) * out.grad
        out._backward = _backward
        return out

    def dot(self, other: "Tensor") -> "Tensor":
        other = _ensure_tensor(other)
        out = Tensor(self.data @ other.data, (self, other), "@", self.requires_grad or other.requires_grad)
        def _backward():
            if self.requires_grad and self.grad is not None: self.grad += out.grad @ other.data.T
            if other.requires_grad and other.grad is not None: other.grad += self.data.T @ out.grad
        out._backward = _backward
        return out

    def sum(self, axis=None, keepdims=False) -> "Tensor":
        out = Tensor(xp.sum(self.data, axis=axis, keepdims=keepdims), (self,), "sum", self.requires_grad)
        def _backward(_axis=axis, _keepdims=keepdims):
            if self.requires_grad and self.grad is not None:
                grad_val = out.grad
                if _axis is not None and not _keepdims:
                    grad_val = xp.expand_dims(grad_val, axis=_axis)
                self.grad += xp.ones_like(self.data) * grad_val
        out._backward = _backward
        return out

    def mean(self, axis=None, keepdims=False) -> "Tensor":
        sum_val = self.sum(axis=axis, keepdims=keepdims)
        num_elements = float(np.prod(self.data.shape) / np.prod(sum_val.data.shape))
        return sum_val * (1.0 / num_elements)
    
    def relu(self) -> "Tensor":
        out = Tensor(xp.maximum(0, self.data), (self,), "ReLU", self.requires_grad)
        def _backward():
            if self.requires_grad and self.grad is not None: self.grad += (self.data > 0) * out.grad
        out._backward = _backward
        return out

    def sigmoid(self) -> "Tensor":
        s = 1 / (1 + xp.exp(-self.data))
        out = Tensor(s, (self,), "Sigmoid", self.requires_grad)
        def _backward():
            if self.requires_grad and self.grad is not None: self.grad += out.data * (1 - out.data) * out.grad
        out._backward = _backward
        return out
    
    def tanh(self) -> "Tensor":
        t = xp.tanh(self.data)
        out = Tensor(t, (self,), "Tanh", self.requires_grad)
        def _backward():
            if self.requires_grad and self.grad is not None:
                self.grad += (1 - t**2) * out.grad
        out._backward = _backward
        return out
        
    def __repr__(self): return f"Tensor(data={self.data}, requires_grad={self.requires_grad})"
    def __neg__(self): return self * -1
    def __sub__(self, other): return self + (-other)
    def __truediv__(self, other): return self * (_ensure_tensor(other) ** -1)
    __radd__ = __add__
    def __rmul__(self, other): return self * other
    def __rsub__(self, other): return _ensure_tensor(other) - self
    def __rtruediv__(self, other): return _ensure_tensor(other) / self

def _ensure_tensor(val: Any) -> "Tensor":
    return val if isinstance(val, Tensor) else Tensor(val)

def _unbroadcast_to(target_shape: Tuple[int, ...], grad: ArrayType) -> ArrayType:
    if target_shape == grad.shape:
        return grad
    
    ndim_diff = grad.ndim - len(target_shape)
    if ndim_diff > 0:
        grad = grad.sum(axis=tuple(range(ndim_diff)))

    axes_to_sum = []
    for i, dim in enumerate(target_shape):
        if dim == 1 and grad.shape[i] > 1:
            axes_to_sum.append(i)
    
    if axes_to_sum:
        grad = grad.sum(axis=tuple(axes_to_sum), keepdims=True)
        
    return grad
========== FILE: core/src/azuraforge_core/__init__.py ==========
from .tensor import Tensor, xp, DEVICE, ArrayType, ScalarType, _unbroadcast_to

__all__ = ["Tensor", "xp", "DEVICE", "ArrayType", "ScalarType", "_unbroadcast_to"]

========== FILE: core/tests/azuraforge_core/test_tensor.py ==========
import pytest
import numpy as np

# Test edilecek paketi import et
from azuraforge_core import Tensor

def test_tensor_creation_and_defaults():
    """Tensor nesnesinin doÄŸru ÅŸekilde ve varsayÄ±lan deÄŸerlerle oluÅŸturulduÄŸunu test eder."""
    t = Tensor([1, 2, 3])
    assert isinstance(t.data, np.ndarray)
    assert t.requires_grad is False
    assert t.grad is None

def test_tensor_requires_grad():
    """`requires_grad=True` olduÄŸunda gradyan dizisinin oluÅŸturulduÄŸunu test eder."""
    t = Tensor([1, 2], requires_grad=True)
    assert t.requires_grad is True
    assert isinstance(t.grad, np.ndarray)
    assert np.array_equal(t.grad, np.array([0.0, 0.0]))

def test_addition_backward():
    """Basit toplama iÅŸlemi iÃ§in geri yayÄ±lÄ±mÄ±n doÄŸru Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± test eder."""
    a = Tensor([1, 2, 3], requires_grad=True)
    b = Tensor(5, requires_grad=True)
    
    # c = a.sum() + b  ->  dc/da = [1, 1, 1], dc/db = 1
    c = a.sum() + b
    
    c.backward()

    assert a.grad is not None
    assert b.grad is not None
    assert np.array_equal(a.grad, [1, 1, 1])
    assert b.grad == 1.0

def test_multiplication_backward():
    """Basit Ã§arpma iÅŸlemi iÃ§in geri yayÄ±lÄ±mÄ±n doÄŸru Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± test eder."""
    x = Tensor(2.0, requires_grad=True)
    y = Tensor(3.0, requires_grad=True)
    
    z = x * y
    
    z.backward() # dz/dx = y = 3,  dz/dy = x = 2

    assert x.grad == 3.0
    assert y.grad == 2.0

def test_chained_rule_backward():
    """Zincir kuralÄ±nÄ±n birden Ã§ok iÅŸlemde doÄŸru Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± test eder."""
    x = Tensor(2.0, requires_grad=True)
    y = Tensor(3.0, requires_grad=True)

    z = x * y  # dz/dx = y, dz/dy = x
    q = z + x  # dq/dz = 1, dq/dx = 1
    
    # Zincir KuralÄ±:
    # dq/dx = (dq/dz * dz/dx) + dq/dx = (1 * y) + 1 = 3 + 1 = 4
    # dq/dy = (dq/dz * dz/dy) = 1 * x = 2
    q.backward()

    assert x.grad == 4.0
    assert y.grad == 2.0

def test_dot_product_backward():
    """Matris Ã§arpÄ±mÄ± iÃ§in geri yayÄ±lÄ±mÄ± test eder."""
    a_data = np.random.randn(2, 3)
    b_data = np.random.randn(3, 4)
    a = Tensor(a_data, requires_grad=True)
    b = Tensor(b_data, requires_grad=True)
    
    c = a.dot(b)
    
    # GradyanÄ± 1'lerden oluÅŸan bir matrisle baÅŸlat
    c.backward(np.ones_like(c.data))
    
    # Manuel olarak hesaplanan gradyanlar
    grad_a_manual = np.ones_like(c.data) @ b_data.T
    grad_b_manual = a_data.T @ np.ones_like(c.data)
    
    assert np.allclose(a.grad, grad_a_manual)
    assert np.allclose(b.grad, grad_b_manual)


def test_relu_backward():
    """ReLU aktivasyonu iÃ§in geri yayÄ±lÄ±mÄ± test eder."""
    a = Tensor([-1, 0, 5], requires_grad=True)
    r = a.relu()
    r.backward(np.array([10, 20, 30]))

    # Gradyan sadece pozitif deÄŸerler iÃ§in akar (self.data > 0)
    assert np.array_equal(a.grad, [0, 0, 30])

========== FILE: dashboard/Dockerfile ==========
# ========== GÃœNCEL VE KAPSAMLI DÃœZELTME: dashboard/Dockerfile ==========

# Node.js LTS sÃ¼rÃ¼mÃ¼nÃ¼ Alpine Linux Ã¼zerinde kullan
FROM node:22-alpine AS dashboard_builder

# Ã‡alÄ±ÅŸma dizinini ayarla
WORKDIR /app

# package.json ve package-lock.json dosyalarÄ±nÄ± kopyala
# Bu adÄ±m, host makinenizdeki dosyalarÄ± Docker container'Ä±na taÅŸÄ±r.
COPY package.json package-lock.json ./

# KRÄ°TÄ°K DÃœZELTME 1: Host'tan gelen package-lock.json'Ä± sil.
# Bu, npm'in container'Ä±n kendi Linux ortamÄ±na uygun, yeni bir package-lock.json oluÅŸturmasÄ±nÄ± saÄŸlar.
# Bu adÄ±mÄ±n cache tarafÄ±ndan atlanmadÄ±ÄŸÄ±ndan emin olmak iÃ§in sonraki build adÄ±mlarÄ± Ã¶nemlidir.
RUN rm -f package-lock.json

# KRÄ°TÄ°K DÃœZELTME 2: npm cache'ini temizle
# Bu, npm'in eski veya bozuk cache verilerini kullanmasÄ±nÄ± engeller.
RUN npm cache clean --force

# KRÄ°TÄ°K DÃœZELTME 3: BaÄŸÄ±mlÄ±lÄ±klarÄ± yÃ¼kle.
# Bu adÄ±m, container ortamÄ±nda sÄ±fÄ±rdan bir kurulum yapar ve Rollup'Ä±n doÄŸru binary'sini indirmesini saÄŸlar.
# Bu iÅŸlem, yeni ve doÄŸru bir package-lock.json dosyasÄ± da oluÅŸturacaktÄ±r.
RUN npm install --verbose

# Geri kalan uygulama kodunu kopyala
# .dockerignore dosyasÄ± varsa, buraya kopyalanacaklar filtrelecektir.
COPY . .

# Vite geliÅŸtirme sunucusunu baÅŸlatmak iÃ§in komut.
# docker-compose.yml tarafÄ±ndan override edildiÄŸi iÃ§in burada CMD satÄ±rÄ± aslÄ±nda Ã§alÄ±ÅŸmaz.
# Ancak iyi bir practice olarak tutulur.
CMD ["npm", "run", "dev", "--", "--host", "0.0.0.0"]

# Dashboard servisi 5173 portunda Ã§alÄ±ÅŸÄ±r
EXPOSE 5173
========== FILE: dashboard/eslint.config.js ==========
import js from '@eslint/js'
import globals from 'globals'
import reactHooks from 'eslint-plugin-react-hooks'
import reactRefresh from 'eslint-plugin-react-refresh'
import { defineConfig, globalIgnores } from 'eslint/config'

export default defineConfig([
  globalIgnores(['dist']),
  {
    files: ['**/*.{js,jsx}'],
    extends: [
      js.configs.recommended,
      reactHooks.configs['recommended-latest'],
      reactRefresh.configs.vite,
    ],
    languageOptions: {
      ecmaVersion: 2020,
      globals: globals.browser,
      parserOptions: {
        ecmaVersion: 'latest',
        ecmaFeatures: { jsx: true },
        sourceType: 'module',
      },
    },
    rules: {
      'no-unused-vars': ['error', { varsIgnorePattern: '^[A-Z_]' }],
    },
  },
])

========== FILE: dashboard/index.html ==========
<!doctype html>
<html lang="tr">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/favicon.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>AzuraForge | MLOps Dashboard</title>
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.jsx"></script>
  </body>
</html>
========== FILE: dashboard/package.json ==========
{
  "name": "dashboard",
  "private": true,
  "version": "0.0.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "vite build",
    "lint": "eslint .",
    "preview": "vite preview"
  },
  "dependencies": {
    "@fontsource/inter": "^5.2.6",
    "axios": "^1.10.0",
    "chart.js": "^4.5.0",
    "chartjs-adapter-date-fns": "^3.0.0",
    "chartjs-plugin-annotation": "^3.1.0",
    "chartjs-plugin-zoom": "^2.2.0",
    "date-fns": "^4.1.0",
    "prop-types": "^15.8.1",
    "react": "^19.1.0",
    "react-chartjs-2": "^5.3.0",
    "react-dom": "^19.1.0",
    "react-markdown": "^10.1.0",
    "react-router-dom": "^7.6.3",
    "react-toastify": "^11.0.5",
    "remark-gfm": "^4.0.1"
  },
  "devDependencies": {
    "@eslint/js": "^9.30.0",
    "@types/react": "^19.1.8",
    "@types/react-dom": "^19.1.6",
    "@vitejs/plugin-react": "^4.6.0",
    "eslint": "^9.30.0",
    "eslint-plugin-react-hooks": "^5.2.0",
    "eslint-plugin-react-refresh": "^0.4.20",
    "globals": "^16.2.0",
    "npm-check-updates": "^18.0.1",
    "vite": "^7.0.0"
  }
}

========== FILE: dashboard/README.md ==========
# AzuraForge Dashboard

Bu proje, AzuraForge platformunun kullanÄ±cÄ± arayÃ¼zÃ¼dÃ¼r. React ve Vite kullanÄ±larak geliÅŸtirilmiÅŸtir.

## âœ¨ Ana Yetenekler

*   Sistemde mevcut olan tÃ¼m AI pipeline'larÄ±nÄ± listeleme ve yeni deneyler baÅŸlatma.
*   GeÃ§miÅŸte Ã§alÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ tÃ¼m deneylerin sonuÃ§larÄ±nÄ± gÃ¶rÃ¼ntÃ¼leme ve filtreleme.
*   SeÃ§ilen deneyleri tek bir ekranda karÅŸÄ±laÅŸtÄ±rarak performanslarÄ±nÄ± analiz etme.
*   Devam eden bir deneyi, anlÄ±k kayÄ±p ve tahmin grafikleriyle **canlÄ± olarak takip etme**.
*   Tamamlanan deneyler iÃ§in oluÅŸturulmuÅŸ interaktif raporlarÄ± gÃ¶rÃ¼ntÃ¼leme.

## ğŸš€ Yerel GeliÅŸtirme OrtamÄ±

Bu projeyi yerel ortamda Ã§alÄ±ÅŸtÄ±rmak iÃ§in, ana `platform` reposundaki **[GeliÅŸtirme Rehberi](../../platform/docs/DEVELOPMENT_GUIDE.md)**'ni takip edin.

Proje baÄŸÄ±mlÄ±lÄ±klarÄ± kurulduktan sonra, aÅŸaÄŸÄ±daki komutlarla geliÅŸtirme sunucusunu baÅŸlatabilir ve testleri Ã§alÄ±ÅŸtÄ±rabilirsiniz.

**GeliÅŸtirme Sunucusunu BaÅŸlatma:**
```bash
# dashboard/ kÃ¶k dizinindeyken
npm run dev
```
Uygulama `http://localhost:5173` adresinde eriÅŸilebilir olacaktÄ±r.

**Lint KontrolÃ¼:**
```bash
npm run lint
```

**Not:** Dashboard'un tam olarak Ã§alÄ±ÅŸabilmesi iÃ§in `api` servisinin `http://localhost:8000` adresinde Ã§alÄ±ÅŸÄ±yor olmasÄ± gerekmektedir.
```

========== FILE: dashboard/vite.config.js ==========
import { defineConfig } from 'vite'
import react from '@vitejs/plugin-react'

// https://vite.dev/config/
export default defineConfig({
  plugins: [react()],
})

========== FILE: dashboard/src/App.css ==========
/* ==========================================================================
   1. KÃ–K DEÄÄ°ÅKENLER (TasarÄ±m Sistemi)
   ========================================================================== */
:root {
  /* Koyu Tema (VarsayÄ±lan) */
  --primary-color: #42b983; --primary-color-dark: #369c70; --secondary-color: #3b82f6;
  --text-color: #e2e8f0; --text-color-darker: #94a3b8; --text-inverse: #ffffff;
  --bg-color: #0f172a; --content-bg: #1e293b; --border-color: #334155; --hover-bg: #2a3a52;
  --success-color: #22c55e; --error-color: #ef4444; --warning-color: #f59e0b; --info-color: #3b82f6;
  --font-sans: 'Inter', system-ui, sans-serif; --font-mono: ui-monospace, Menlo, monospace;
  --shadow-md: 0 4px 6px -1px rgb(0 0 0 / 0.1), 0 2px 4px -2px rgb(0 0 0 / 0.1);
  --shadow-lg: 0 10px 15px -3px rgb(0 0 0 / 0.1), 0 4px 6px -2px rgb(0 0 0 / 0.1);
  --border-radius: 12px;
}
body.light-theme {
  /* AÃ§Ä±k Tema */
  --text-color: #1e293b; --text-color-darker: #475569; --bg-color: #f8fafc;
  --content-bg: #ffffff; --border-color: #e2e8f0; --hover-bg: #f1f5f9;
  --success-color: #16a34a; --error-color: #dc2626; --warning-color: #d97706; --info-color: #2563eb;
}
/* ==========================================================================
   2. TEMEL RESET VE GLOBAL STÄ°LLER
   ========================================================================== */
*, *::before, *::after { box-sizing: border-box; }
body {
  margin: 0; font-family: var(--font-sans); background-color: var(--bg-color); color: var(--text-color);
  font-synthesis: none; text-rendering: optimizeLegibility; -webkit-font-smoothing: antialiased;
  transition: background-color 0.3s, color 0.3s;
}
#root, .app-layout { 
  display: flex; 
  flex-direction: column; /* Dikey dÃ¼zen: navbar Ã¼stte, main iÃ§erik altta */
  width: 100vw; 
  height: 100vh; 
  overflow: hidden; 
  position: relative; /* Yan panel iÃ§in */
}
/* ==========================================================================
   3. ANA YAPI (Layout) - SÄ°DEBAR KALDIRILDI, YENÄ° NAV VE PANEL VAR
   ========================================================================== */
/* Eski .sidebar ve .sidebar nav stilleri kaldÄ±rÄ±ldÄ± */

.main-content { 
  flex-grow: 1; 
  padding: 30px; 
  overflow-y: auto; 
  position: relative; 
  z-index: 10; /* Panelin altÄ±nda kalmasÄ± iÃ§in */
}
.page-header { margin-bottom: 30px; }
.page-header h1 {
  font-size: 2.2em; font-weight: 700; color: var(--text-color); margin: 0 0 5px 0;
  display: flex; align-items: center; gap: 15px; transition: color 0.3s;
}
.page-header p { color: var(--text-color-darker); font-size: 1.1em; margin: 0; }
/* ==========================================================================
   4. LOGO VE TEMA DEÄÄ°ÅTÄ°RME - ARTIK TOPNAVBAR'DA
   ========================================================================== */
.logo-container {
  display: flex; align-items: center; justify-content: flex-start; gap: 12px;
  margin-bottom: 0; /* ArtÄ±k sidebar'da deÄŸil, Ã¶zel bir margin yok */
  padding: 0; 
}
.logo-container h1 {
  color: var(--text-color); font-size: 1.6em; font-weight: 600; margin: 0;
  letter-spacing: 0.5px; transition: color 0.3s;
}
.theme-toggle-button {
  background-color: var(--bg-color); color: var(--text-color-darker); border: 1px solid var(--border-color);
  border-radius: 8px; padding: 8px; cursor: pointer; display: flex; align-items: center;
  justify-content: center; margin-top: 0; /* ArtÄ±k sidebar'da deÄŸil */
  transition: all 0.2s ease;
}
.theme-toggle-button:hover { border-color: var(--primary-color); color: var(--primary-color); }
/* ==========================================================================
   5. BÄ°LEÅENLER (Components)
   ========================================================================== */
.card, .table-container, .form-container, .pipeline-details {
  background-color: var(--content-bg); border: 1px solid var(--border-color);
  border-radius: var(--border-radius); box-shadow: var(--shadow-md);
  transition: background-color 0.3s, border-color 0.3s;
}
.table-container { padding: 0; }
.card { padding: 25px; }
.button-primary {
  background-color: var(--primary-color); color: var(--text-inverse); padding: 10px 20px; border: none;
  border-radius: 8px; cursor: pointer; font-size: 1em; font-weight: 600;
  transition: background-color 0.2s ease, transform 0.2s ease; display: inline-flex;
  align-items: center; gap: 8px;
}
.button-primary:hover:not(:disabled) { background-color: var(--primary-color-dark); transform: translateY(-2px); }
.button-primary:disabled { background-color: var(--border-color); cursor: not-allowed; opacity: 0.6; transform: none; }
.status-badge {
  display: inline-flex; align-items: center; gap: 6px; padding: 4px 12px; border-radius: 9999px;
  font-size: 0.8em; font-weight: 600; text-transform: uppercase; justify-content: center;
}
.status-badge::before { content: ''; display: inline-block; width: 8px; height: 8px; border-radius: 50%; }
.status-badge.status-started, .status-badge.status-progress, .status-badge.status-connecting { background-color: rgba(59, 130, 246, 0.2); color: #60a5fa; }
.status-badge.status-started::before, .status-badge.status-progress::before, .status-badge.status-connecting::before { background-color: var(--info-color); animation: pulse 2s infinite; }
@keyframes pulse { 0%, 100% { opacity: 1; } 50% { opacity: 0.5; } }
.status-badge.status-success { background-color: rgba(34, 197, 94, 0.2); color: #4ade80; }
.status-badge.status-success::before { background-color: var(--success-color); }
.status-badge.status-failure, .status-badge.status-error { background-color: rgba(239, 68, 68, 0.2); color: #f87171; }
.status-badge.status-failure::before, .status-badge.status-error::before { background-color: var(--error-color); }
.status-badge.status-pending, .status-badge.status-unknown, .status-badge.status-disconnected { background-color: rgba(148, 163, 184, 0.2); color: var(--text-color-darker); }
.status-badge.status-pending::before, .status-badge.status-unknown::before, .status-badge.status-disconnected::before { background-color: var(--text-color-darker); }
.form-group { margin-bottom: 20px; }
.form-group label { display: block; margin-bottom: 8px; font-weight: 500; color: var(--text-color-darker); }
.form-group input, .form-group select {
  width: 100%; padding: 12px; background-color: var(--bg-color); border: 1px solid var(--border-color);
  border-radius: 8px; color: var(--text-color); font-size: 1em; font-family: var(--font-sans);
  transition: background-color 0.3s, border-color 0.3s, color 0.3s;
}
.form-group input:focus, .form-group select:focus {
  outline: none; border-color: var(--primary-color); box-shadow: 0 0 0 2px color-mix(in srgb, var(--primary-color) 30%, transparent);
}
/* Tablo ile ilgili eski stiller kaldÄ±rÄ±ldÄ±, yerine kartlar var */
/* .table-container, table, table th, table td, .actions-cell kaldÄ±rÄ±ldÄ± */

/* Ortak aksiyon menÃ¼sÃ¼ stilleri */
.actions-cell { 
  position: relative; 
  text-align: right; /* SaÄŸa hizalÄ± olsun */
  flex-shrink: 0; 
}
.actions-button {
  background: none; border: none; font-size: 24px; line-height: 1; color: var(--text-color-darker);
  cursor: pointer; border-radius: 4px; padding: 0 8px;
}
.actions-button:hover { background-color: var(--border-color); color: var(--text-color); }
.actions-menu {
  position: absolute; right: 0; top: 35px; /* Konum ayarlandÄ± */
  background-color: var(--hover-bg); border: 1px solid var(--border-color);
  border-radius: 8px; box-shadow: var(--shadow-lg); z-index: 100; display: flex;
  flex-direction: column; padding: 8px; min-width: 180px;
}
.actions-menu button, .actions-menu-button {
  background: none; border: none; color: var(--text-color); padding: 10px 15px; text-align: left;
  cursor: pointer; border-radius: 6px; display: flex; align-items: center; gap: 10px; font-size: 0.9em;
  width: 100%; text-decoration: none;
}
.actions-menu button:hover, .actions-menu-button:hover { background-color: var(--secondary-color); color: var(--text-inverse); }

/* ==========================================================================
   6. Ã–ZEL PANELLER VE MODALLAR (LiveTrackerPane ve ReportViewer kaldÄ±rÄ±ldÄ±)
   ========================================================================== */
.comparison-modal-overlay {
  position: fixed; top: 0; left: 0; right: 0; bottom: 0;
  background-color: color-mix(in srgb, var(--bg-color) 70%, transparent); backdrop-filter: blur(8px);
  z-index: 5000; display: flex; align-items: center; justify-content: center;
}
.comparison-modal-content {
  background-color: var(--bg-color); border: 1px solid var(--border-color); border-radius: 16px;
  width: 90%; max-width: 1200px; height: 90vh; box-shadow: var(--shadow-lg);
  display: flex; flex-direction: column;
}
.comparison-header {
  display: flex; justify-content: space-between; align-items: center; padding: 20px 30px;
  border-bottom: 1px solid var(--border-color); flex-shrink: 0;
}
.comparison-header h2 { margin: 0; font-size: 1.5em; }
.close-button {
  background: none; border: none; font-size: 24px; color: var(--text-color-darker);
  cursor: pointer; transition: color 0.2s;
}
.close-button:hover { color: var(--text-color); }
.comparison-body {
  padding: 30px; flex-grow: 1; overflow-y: auto; display: flex;
  flex-direction: column; gap: 30px;
}
.comparison-chart-container { height: 400px; min-height: 300px; width: 100%; position: relative; }
.chart-instructions {
  position: absolute; bottom: 5px; right: 10px; font-size: 0.75em;
  color: var(--text-color-darker); background-color: color-mix(in srgb, var(--content-bg) 80%, transparent);
  padding: 2px 8px; border-radius: 4px; opacity: 0.7;
}
.color-indicator {
  display: inline-block; width: 12px; height: 12px; border-radius: 50%;
  margin-right: 10px; vertical-align: middle;
}
/* ==========================================================================
   7. ÃœÃ‡ÃœNCÃœ PARTÄ° KÃœTÃœPHANE STÄ°LLERÄ°
   ========================================================================== */
.Toastify__toast {
  background-color: var(--content-bg) !important; color: var(--text-color) !important;
  border: 1px solid var(--border-color) !important; border-radius: 8px !important;
  font-family: var(--font-sans) !important;
}
.Toastify__progress-bar { background: var(--primary-color) !important; }
.Toastify__close-button { color: var(--text-color) !important; }

/* ==========================================================================
   8. YENÄ° DENEY SAYFASI / PANELÄ° STÄ°LLERÄ°
   ========================================================================== */

/* Yeni deney panelinin kendi iÃ§inde bir form yapÄ±sÄ± olacak */
.new-experiment-panel-form {
  display: flex;
  flex-direction: column;
  height: 100%; /* Panelin tamamÄ±nÄ± kaplasÄ±n */
}

.new-experiment-panel-form-content {
  flex-grow: 1; /* Kalan tÃ¼m alanÄ± kapla */
  overflow-y: auto; /* Ä°Ã§erik taÅŸmasÄ±nÄ± engelle */
  padding: 25px; /* Ä°Ã§ padding */
}


.form-action-bar {
  flex-shrink: 0; 
  padding: 15px 25px;
  background-color: var(--bg-color);
  border-top: 1px solid var(--border-color);
  display: flex;
  justify-content: space-between;
  align-items: center;
  transition: background-color 0.3s, border-color 0.3s;
}

.pipeline-info {
  display: flex;
  align-items: center;
  gap: 10px;
  color: var(--text-color-darker);
}
.pipeline-info span {
  font-weight: 600;
  color: var(--text-color);
}

.form-fieldset {
  border: 1px solid var(--border-color);
  border-radius: 8px;
  padding: 20px;
  margin: 0 0 20px 0;
}
.form-fieldset legend {
  padding: 0 10px;
  font-weight: 600;
  color: var(--text-color);
}

/* ==========================================================================
   9. FORM HATA STÄ°LLERÄ°
   ========================================================================== */
.form-group input.input-error,
.form-group select.input-error,
.form-group textarea.input-error {
  border-color: var(--error-color);
  box-shadow: 0 0 0 2px color-mix(in srgb, var(--error-color) 30%, transparent);
}
.form-error-message {
  color: var(--error-color);
  font-size: 0.85em;
  margin-top: 5px;
  display: block;
}

/* ==========================================================================
   10. KATLANABÄ°LÄ°R BÃ–LÃœMLER (Form Ä°Ã§in)
   ========================================================================== */
.collapsible-fieldset {
  margin-bottom: 20px;
}

.collapsible-header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: 10px 15px;
  background-color: var(--hover-bg);
  border-radius: 8px 8px 0 0;
  cursor: pointer;
  font-weight: 600;
  color: var(--text-color);
  border-bottom: 1px solid var(--border-color);
  transition: background-color 0.2s ease;
}

.collapsible-header:hover {
  background-color: color-mix(in srgb, var(--primary-color) 10%, var(--hover-bg));
}

.collapsible-header .icon {
  transition: transform 0.2s ease;
}

.collapsible-header.collapsed .icon {
  transform: rotate(-90deg);
}

.collapsible-content {
  max-height: 0;
  overflow: hidden;
  transition: max-height 0.3s ease-out, padding 0.3s ease-out;
  padding: 0 20px;
  background-color: var(--content-bg);
  border-radius: 0 0 var(--border-radius) var(--border-radius);
}

.collapsible-fieldset.expanded .collapsible-content {
  max-height: 1000px; 
  transition: max-height 0.3s ease-in, padding 0.3s ease-in;
  padding: 20px;
}

/* Ä°lk fieldset varsayÄ±lan olarak aÃ§Ä±k */
.collapsible-fieldset:first-of-type .collapsible-content {
    max-height: 1000px;
    padding: 20px;
}
.collapsible-fieldset:first-of-type .collapsible-header .icon {
    transform: rotate(0deg);
}
.collapsible-fieldset:first-of-type .collapsible-header.collapsed .icon {
    transform: rotate(0deg);
}

/* ==========================================================================
   11. DENEY KARTLARI VE LÄ°STE DÃœZENÄ°
   ========================================================================== */
.experiments-list-container {
  display: flex;
  flex-direction: column; /* KartlarÄ± dikey olarak alt alta sÄ±rala */
  gap: 25px; /* Kartlar arasÄ± boÅŸluk */
}

.experiment-card {
  display: flex;
  flex-direction: column; /* Ãœst bÃ¶lÃ¼m, gÃ¶vde ve detaylar dikeyde sÄ±ralanÄ±r */
  background-color: var(--content-bg);
  border: 1px solid var(--border-color);
  border-radius: var(--border-radius);
  box-shadow: var(--shadow-md);
  transition: all 0.2s ease;
  overflow: hidden; /* Ä°Ã§indeki Ã¶ÄŸelerin taÅŸmasÄ±nÄ± engeller */
}

.experiment-card:hover {
  border-color: var(--primary-color);
  box-shadow: var(--shadow-lg);
  transform: translateY(-2px);
}

.experiment-card.selected-card {
  border-color: var(--secondary-color);
  box-shadow: 0 0 0 3px color-mix(in srgb, var(--secondary-color) 30%, transparent);
}

.experiment-card .card-top-section {
    display: flex;
    align-items: center;
    justify-content: space-between;
    padding: 15px 20px;
    border-bottom: 1px solid var(--border-color);
    background-color: var(--bg-color); /* BaÅŸlÄ±k iÃ§in biraz daha koyu */
    gap: 15px; /* Ã–ÄŸeler arasÄ± boÅŸluk */
}

.experiment-card .card-checkbox-status {
    display: flex;
    align-items: center;
    gap: 10px;
    flex-shrink: 0; /* KÃ¼Ã§Ã¼lmesini engelle */
}

.experiment-card .card-main-info {
    flex-grow: 1; /* Ortadaki bilgilerin alanÄ± doldurmasÄ±nÄ± saÄŸlar */
    display: flex;
    flex-direction: column; /* Ä°sim, ID, Batch alt alta gelsin */
    align-items: flex-start;
    overflow: hidden; /* Uzun isimler iÃ§in taÅŸmayÄ± engelle */
}

.experiment-card .card-main-info .pipeline-name {
  margin: 0;
  font-size: 1.1em;
  font-weight: 600;
  color: var(--text-color);
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  max-width: 100%; /* Ä°Ã§eriÄŸin geniÅŸliÄŸini sÄ±nÄ±rlama */
}

.experiment-card .card-main-info .experiment-id,
.experiment-card .card-main-info .batch-name {
    font-family: var(--font-mono);
    font-size: 0.8em;
    color: var(--text-color-darker);
    white-space: nowrap;
    overflow: hidden;
    text-overflow: ellipsis;
    max-width: 100%;
}


.experiment-card .card-body {
  padding: 20px;
  display: flex;
  flex-wrap: wrap; /* EÄŸer sÄ±ÄŸmazsa alt satÄ±ra geÃ§sin */
  gap: 20px; /* Metrikler ve grafikler arasÄ± boÅŸluk */
  align-items: flex-start; /* Ãœstten baÅŸlasÄ±nlar */
}

.experiment-card .card-metrics-summary {
  display: grid; /* Metrikleri dÃ¼zenli gÃ¶stermek iÃ§in grid kullan */
  grid-template-columns: auto 1fr; /* Etiket ve deÄŸer yan yana */
  column-gap: 15px;
  row-gap: 5px;
  flex-basis: 300px; /* Bu bÃ¶lÃ¼mÃ¼n minimum geniÅŸliÄŸi */
  flex-grow: 1; /* BÃ¼yÃ¼yebilsin */
  min-width: 250px; /* Daha dar ekranlarda kÃ¼Ã§Ã¼lmesini engelle */
}

.experiment-card .card-metrics-summary p {
  margin: 0;
  font-size: 0.9em;
  color: var(--text-color-darker);
  white-space: nowrap; /* Metni tek satÄ±rda tut */
  overflow: hidden;
  text-overflow: ellipsis;
}
.experiment-card .card-metrics-summary p strong {
  color: var(--text-color);
}
/* Grid'de etiketlerin saÄŸa hizalanmasÄ± */
.experiment-card .card-metrics-summary p:nth-child(odd) {
    text-align: right;
    color: var(--text-color); /* Etiketleri biraz daha belirgin yap */
    font-weight: 500;
}
.experiment-card .card-metrics-summary p:nth-child(even) {
    text-align: left;
}


.experiment-card .card-charts-section {
  display: flex;
  flex-wrap: wrap; /* SÄ±ÄŸmazsa alt satÄ±ra geÃ§sin */
  gap: 15px; /* Grafikler arasÄ± boÅŸluk */
  flex-grow: 2; /* Metriklerden daha fazla alan kaplasÄ±n */
  min-width: 400px; /* Minimum grafik alanÄ± geniÅŸliÄŸi */
  justify-content: center; /* Grafikleri ortala */
}

.experiment-card .single-chart-container {
  flex-basis: 220px; /* Her bir grafiÄŸin temel geniÅŸliÄŸi */
  flex-grow: 1; /* BÃ¼yÃ¼yebilsin */
  height: 120px; /* GrafiÄŸin yÃ¼ksekliÄŸi */
  position: relative; /* instructions iÃ§in */
  background-color: var(--bg-color); /* Grafik arkaplanÄ± */
  border-radius: 8px;
  padding: 5px;
  box-shadow: inset 0 0 5px rgba(0,0,0,0.1);
}
.experiment-card .no-chart-data-message {
    display: flex;
    align-items: center;
    justify-content: center;
    height: 100%;
    color: var(--text-color-darker);
    font-size: 0.8em;
}

/* Detaylar BÃ¶lÃ¼mÃ¼ (Katlanabilir) */
.card-expanded-details {
  max-height: 0;
  overflow: hidden;
  transition: max-height 0.3s ease-out, padding 0.3s ease-out;
  padding: 0 20px; 
  border-top: 1px solid var(--border-color); /* Ãœst Ã§izgi */
}

/* GeniÅŸletilmiÅŸ durumda */
.experiment-card.expanded .card-expanded-details {
  max-height: 500px; /* Yeterince bÃ¼yÃ¼k bir deÄŸer */
  transition: max-height 0.3s ease-in, padding 0.3s ease-in;
  padding: 20px; /* AÃ§Ä±ldÄ±ÄŸÄ±nda padding ver */
}


/* CanlÄ± ilerleme Ã§ubuÄŸu stili (ExperimentCard iÃ§inde) */
.live-progress-bar-section {
    width: 100%;
    margin-top: 10px;
    padding-top: 10px;
    border-top: 1px solid var(--border-color);
}
.live-progress-bar-section progress {
    width: 100%;
    height: 8px;
    -webkit-appearance: none;
    appearance: none;
    border-radius: 4px;
    overflow: hidden;
}
.live-progress-bar-section progress::-webkit-progress-bar {
    background-color: var(--border-color);
    border-radius: 4px;
}
.live-progress-bar-section progress::-webkit-progress-value {
    background-color: var(--info-color); /* Mavi tonu */
    border-radius: 4px;
    transition: width 0.3s ease;
}
.live-progress-bar-section progress::-moz-progress-bar {
    background-color: var(--info-color);
    border-radius: 4px;
    transition: width 0.3s ease;
}

/* ==========================================================================
   12. ÃœST NAVÄ°GASYON Ã‡UBUÄU (TOPNAVBAR) VE YAN PANEL (NEWEXPERIMENTPANEL)
   ========================================================================== */
.top-navbar {
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: 15px 30px;
  background-color: var(--content-bg);
  border-bottom: 1px solid var(--border-color);
  box-shadow: var(--shadow-md);
  flex-shrink: 0; /* KÃ¼Ã§Ã¼lmesini engelle */
  z-index: 100; /* Main content'in Ã¼zerinde olsun */
}

.new-experiment-panel-overlay {
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background-color: rgba(0,0,0,0.5); /* Koyu arkaplan */
  backdrop-filter: blur(5px);
  z-index: 2000;
  display: flex;
  justify-content: flex-end; /* Paneli saÄŸa yasla */
  opacity: 0; /* BaÅŸlangÄ±Ã§ta gizli */
  visibility: hidden; /* BaÅŸlangÄ±Ã§ta gÃ¶rÃ¼nmez */
  transition: opacity 0.3s ease, visibility 0.3s ease;
}

.new-experiment-panel-overlay.open {
  opacity: 1;
  visibility: visible;
}

.new-experiment-panel {
  width: 500px; /* Sabit geniÅŸlik */
  max-width: 90%; /* KÃ¼Ã§Ã¼k ekranlarda taÅŸmasÄ±nÄ± engelle */
  height: 100%;
  background-color: var(--bg-color);
  box-shadow: var(--shadow-lg);
  display: flex;
  flex-direction: column;
  transform: translateX(100%); /* BaÅŸlangÄ±Ã§ta ekran dÄ±ÅŸÄ± */
  transition: transform 0.3s ease-out;
  z-index: 2001; /* Overlay'in Ã¼zerinde olsun */
}

.new-experiment-panel-overlay.open .new-experiment-panel {
  transform: translateX(0); /* AÃ§Ä±ldÄ±ÄŸÄ±nda iÃ§eri kaydÄ±r */
}

.new-experiment-panel-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: 20px 25px;
  border-bottom: 1px solid var(--border-color);
  background-color: var(--content-bg);
  flex-shrink: 0;
}

.new-experiment-panel-header h2 {
  margin: 0;
  font-size: 1.5em;
  color: var(--text-color);
}

.new-experiment-panel-body {
  flex-grow: 1; /* Kalan alanÄ± kapla */
  overflow-y: auto; /* Ä°Ã§erik kaydÄ±rÄ±labilir olsun */
  padding: 25px;
}

.new-experiment-panel-footer {
  padding: 15px 25px;
  border-top: 1px solid var(--border-color);
  background-color: var(--content-bg);
  display: flex;
  justify-content: flex-end;
  flex-shrink: 0;
}

/* =============== Ek CSS Ä°yileÅŸtirmeleri =============== */

/* Daha keskin grafikler iÃ§in canvas rendering */
/* Bu, grafiklerin pixelated gÃ¶rÃ¼nmesini engeller */
.single-chart-container canvas {
    image-rendering: -webkit-optimize-contrast;
    image-rendering: crisp-edges;
    image-rendering: pixelated; /* Safari/macOS iÃ§in */
}

/* Zoom instructions yazÄ±sÄ±nÄ± daha az rahatsÄ±z edici hale getir */
.chart-instructions {
  position: absolute; 
  bottom: 0px; /* Biraz yukarÄ± Ã§ek */
  left: 50%; /* Ortala */
  transform: translateX(-50%); /* Tam ortala */
  padding: 1px 6px; /* Daha kÃ¼Ã§Ã¼k padding */
  font-size: 0.7em; /* Daha kÃ¼Ã§Ã¼k font */
  opacity: 0.9; /* Biraz daha belirgin */
  background-color: var(--bg-color); /* Koyu arkaplan, daha iyi okunurluk */
  border: 1px solid var(--border-color); /* Ã‡erÃ§eve */
  border-radius: 4px;
  white-space: nowrap; /* Tek satÄ±rda tut */
  pointer-events: none; /* Mouse etkileÅŸimini engelle */
  z-index: 10; /* GrafiÄŸin Ã¼zerinde olsun */
}

/* Dikey kartlarÄ±n liste konteyneri iÃ§in daha iyi hizalama */
.experiments-list-container {
  align-items: stretch; /* KartlarÄ±n geniÅŸliÄŸini eÅŸitle */
}

/* Her kartÄ±n iÃ§indeki metrikler ve grafiklerin sÄ±ÄŸmasÄ± iÃ§in */
.experiment-card .card-body {
    justify-content: space-between; /* Ä°Ã§erikleri yay */
    align-items: flex-start;
}

.experiment-card .card-metrics-summary {
    min-width: 200px; /* Bu bÃ¶lÃ¼mÃ¼n minimum geniÅŸliÄŸi */
}

.experiment-card .card-charts-section {
    min-width: 350px; /* iki grafik + gap iÃ§in yeterli minimum */
    gap: 10px; /* Grafiklerin arasÄ± biraz daralsÄ±n */
}

.experiment-card .single-chart-container {
    flex-basis: calc(50% - 5px); /* %50 - yarÄ±m gap */
    max-width: calc(50% - 5px); /* Her grafik iÃ§in max %50 geniÅŸlik */
    height: 100px; /* Grafikleri daha kompakt yap */
}

/* Kart iÃ§indeki baÅŸlÄ±k/ID/Batch alanlarÄ± iÃ§in taÅŸma gÃ¶rÃ¼nÃ¼mÃ¼ */
.experiment-card .card-main-info {
    max-width: calc(100% - 120px); 
}
========== FILE: dashboard/src/App.jsx ==========
// dashboard/src/App.jsx

import { useState, useContext } from 'react';
import { Routes, Route, useNavigate } from 'react-router-dom';
import { ToastContainer } from 'react-toastify';
import 'react-toastify/dist/ReactToastify.css';

import './App.css';
import { ThemeContext } from './context/ThemeContext';

// Yeni Navigasyon ve Panel BileÅŸenleri
import TopNavbar from './components/TopNavbar';
import NewExperimentPanel from './components/NewExperimentPanel';

// Dashboard Overview hala ana iÃ§eriÄŸimiz
import DashboardOverview from './pages/DashboardOverview';

function App() {
  const { theme } = useContext(ThemeContext);
  const navigate = useNavigate();
  const [isNewExperimentPanelOpen, setIsNewExperimentPanelOpen] = useState(false);

  // Deney baÅŸlatÄ±ldÄ±ÄŸÄ±nda panelin kapanmasÄ± ve ana sayfaya yÃ¶nlendirme
  const handleExperimentStarted = () => {
    setIsNewExperimentPanelOpen(false); // Paneli kapat
    navigate('/'); // Ana sayfaya yÃ¶nlendir
  };

  const handleOpenNewExperimentPanel = () => {
    setIsNewExperimentPanelOpen(true);
  };

  const handleCloseNewExperimentPanel = () => {
    setIsNewExperimentPanelOpen(false);
  };

  return (
    <div className="app-layout">
      <ToastContainer position="bottom-right" autoClose={5000} theme={theme} />
      
      {/* Sol menÃ¼ kaldÄ±rÄ±ldÄ±, yerine Ã¼st navigasyon Ã§ubuÄŸu */}
      <TopNavbar onNewExperimentClick={handleOpenNewExperimentPanel} />

      <main className="main-content">
        <Routes>
          <Route path="/" element={<DashboardOverview />} />
          {/* NewExperiment sayfasÄ± artÄ±k bir rota olarak deÄŸil, yan panel olarak aÃ§Ä±lacak */}
        </Routes>
      </main>

      {/* Yeni Deney Yan Paneli */}
      <NewExperimentPanel 
        isOpen={isNewExperimentPanelOpen} 
        onClose={handleCloseNewExperimentPanel} 
        onExperimentStarted={handleExperimentStarted} 
      />
    </div>
  );
}

export default App;
========== FILE: dashboard/src/index.css ==========
/*
  Bu dosya, gelecekte Ã§ok temel, uygulama geneli reset kurallarÄ± iÃ§in kullanÄ±labilir.
  Åimdilik, projenin tÃ¼m Ã¶zel stil mantÄ±ÄŸÄ± App.css dosyasÄ±nda merkezileÅŸtirilmiÅŸtir.
*/
========== FILE: dashboard/src/main.jsx ==========
import React from 'react';
import { createRoot } from 'react-dom/client';
import { BrowserRouter } from 'react-router-dom';
import { ThemeProvider } from './context/ThemeContext';

// Fontsource
import '@fontsource/inter/400.css';
import '@fontsource/inter/500.css';
import '@fontsource/inter/600.css';
import '@fontsource/inter/700.css';

// Stil dosyalarÄ±
import './index.css'; 
import './App.css';
import App from './App.jsx';

createRoot(document.getElementById('root')).render(
  // StrictMode'u bilinÃ§li olarak kapalÄ± tutuyoruz
  // <React.StrictMode>
    <ThemeProvider>
      <BrowserRouter> 
        <App />
      </BrowserRouter>
    </ThemeProvider>
  // </React.StrictMode>
);
========== FILE: dashboard/src/components/ComparisonView.jsx ==========
import PropTypes from 'prop-types';
import { Line } from 'react-chartjs-2';
import { Chart as ChartJS, CategoryScale, LinearScale, PointElement, LineElement, Title, Tooltip, Legend } from 'chart.js';
import zoomPlugin from 'chartjs-plugin-zoom';

ChartJS.register(CategoryScale, LinearScale, PointElement, LineElement, Title, Tooltip, Legend, zoomPlugin);

const chartColors = ['#42b983', '#3b82f6', '#ef4444', '#f59e0b', '#8b5cf6', '#ec4899'];

function ComparisonView({ experiments, onClose }) {
  const chartData = {
    labels: Array.from({ length: Math.max(...experiments.map(e => e.results?.history?.loss?.length || 0)) }, (_, i) => `E${i + 1}`),
    datasets: experiments.map((exp, i) => ({
      label: `${exp.config.data_sourcing.ticker} (${exp.experiment_id.slice(-6)})`,
      data: exp.results?.history?.loss || [], 
      borderColor: chartColors[i % chartColors.length],
      backgroundColor: `${chartColors[i % chartColors.length]}33`,
      tension: 0.1, fill: false, borderWidth: 2, pointRadius: 1, pointHoverRadius: 5,
    })),
  };

  const chartOptions = {
      responsive: true, maintainAspectRatio: false,
      interaction: { mode: 'index', intersect: false, },
      plugins: { 
        legend: { position: 'top', labels: { font: { size: 14 } } },
        tooltip: {
          backgroundColor: 'var(--content-bg)',
          borderColor: 'var(--border-color)',
          borderWidth: 1,
        },
        zoom: {
          pan: { enabled: true, mode: 'xy', modifierKey: 'alt', },
          zoom: { wheel: { enabled: true }, pinch: { enabled: true }, mode: 'xy' }
        }
      },
      scales: {
          y: { title: { display: true, text: 'KayÄ±p DeÄŸeri (Loss)' }, beginAtZero: false, }, 
          x: { title: { display: true, text: 'Epoch' }, grid: { display: false } } 
      }
  };

  return (
    <div className="comparison-modal-overlay" onClick={onClose}>
      <div className="comparison-modal-content" onClick={e => e.stopPropagation()}>
        <div className="comparison-header">
          <h2>Deney KarÅŸÄ±laÅŸtÄ±rmasÄ± ({experiments.length} adet)</h2>
          <button className="close-button" onClick={onClose}>Ã—</button>
        </div>
        <div className="comparison-body">
          <div className="comparison-chart-container">
            <Line data={chartData} options={chartOptions} />
            <p className="chart-instructions">YakÄ±nlaÅŸtÄ±rmak iÃ§in fare tekerleÄŸini kullanÄ±n. SÄ±fÄ±rlamak iÃ§in Ã§ift tÄ±klayÄ±n. KaydÄ±rmak iÃ§in <strong>Alt + SÃ¼rÃ¼kle</strong>.</p>
          </div>
          <h4 className="section-title" style={{ marginTop: 0 }}>Ã–zet Tablosu</h4>
          <div className="table-container">
            <table>
              <thead><tr><th>Deney ID</th><th>Ticker</th><th>Epochs</th><th>LR</th><th>Final KayÄ±p</th></tr></thead>
              <tbody>
                {experiments.map((exp, i) => (
                  <tr key={exp.experiment_id}>
                    <td><span className="color-indicator" style={{backgroundColor: chartColors[i % chartColors.length]}}></span>{exp.experiment_id.slice(0, 18)}...</td>
                    <td>{exp.config?.data_sourcing?.ticker ?? 'N/A'}</td>
                    <td>{Array.isArray(exp.config?.training_params?.epochs) ? exp.config.training_params.epochs[0] : exp.config?.training_params?.epochs ?? 'N/A'}</td>
                    <td>{Array.isArray(exp.config?.training_params?.lr) ? exp.config.training_params.lr[0] : exp.config?.training_params?.lr ?? 'N/A'}</td>
                    <td>{exp.results?.final_loss?.toFixed(6) ?? 'N/A'}</td>
                  </tr>
                ))}
              </tbody>
            </table>
          </div>
        </div>
      </div>
    </div>
  );
}
ComparisonView.propTypes = { experiments: PropTypes.array.isRequired, onClose: PropTypes.func.isRequired, };
export default ComparisonView;
========== FILE: dashboard/src/components/ExperimentCard.jsx ==========
// dashboard/src/components/ExperimentCard.jsx

// DÃœZELTME: React ve tÃ¼m hook'lar doÄŸru ÅŸekilde import edildi.
import React, { useState, useEffect, useRef } from 'react'; 
import PropTypes from 'prop-types';
import { toast } from 'react-toastify';
import SingleExperimentChart from './SingleExperimentChart'; 
import { getCssVar } from '../utils/cssUtils'; 

const Icon = ({ path, className }) => <svg className={className} width="16" height="16" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d={path} /></svg>;
Icon.propTypes = { path: PropTypes.string.isRequired, className: PropTypes.string };

const ICONS = {
  copy: "M16 1H4c-1.1 0-2 .9-2 2v14h2V3h12V1zm3 4H8c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h11c1.1 0 2-.9 2-2V7c0-1.1-.9-2-2-2zm0 16H8V7h11v14z",
  expand: "M19 19H5V5h14v14zm0-16H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2z" 
};

function ExperimentCard({ experiment, isSelected, onSelect }) {
  const [actionsOpen, setActionsOpen] = useState(false);
  const [isDetailsExpanded, setIsDetailsExpanded] = useState(false); // DetaylarÄ±n katlanabilir durumu

  const { 
    experiment_id, status, task_id, pipeline_name,
    created_at, completed_at, failed_at,
    config_summary, results_summary, config: full_config, results: full_results, error: full_error // Tam config ve results
  } = experiment;

  const handleCopyId = () => {
    navigator.clipboard.writeText(experiment_id);
    toast.success('Deney ID panoya kopyalandÄ±!');
    setActionsOpen(false);
  };

  const isRunning = ['STARTED', 'PROGRESS'].includes(status);
  const isFinished = ['SUCCESS', 'FAILURE'].includes(status);

  const finalLoss = results_summary?.final_loss;
  const displayLoss = (finalLoss !== null && finalLoss !== undefined) ? finalLoss.toFixed(6) : 'N/A';
  const startTime = created_at ? new Date(created_at).toLocaleString() : 'N/A';
  const endTime = completed_at || failed_at ? new Date(completed_at || failed_at).toLocaleString() : 'N/A';

  // config_summary'den epochs'u al, eÄŸer liste ise ilk elemanÄ± al (API'ye tekli gÃ¶nderdiÄŸimiz iÃ§in)
  const totalEpochs = Array.isArray(full_config?.training_params?.epochs) ? full_config.training_params.epochs[0] : full_config?.training_params?.epochs;

  // CanlÄ± takip iÃ§in ilerleme yÃ¼zdesi ve metin
  // Bu state artÄ±k sadece bu bileÅŸen iÃ§inde canlÄ± WebSocket verisini tutacak
  const [liveProgress, setLiveProgress] = useState({ epoch: 0, totalEpochs: totalEpochs, text: 'BaÅŸlatÄ±ldÄ±' });

  // WebSocket'ten gelen canlÄ± veriyi yakalamak iÃ§in useEffect
  useEffect(() => {
    // Sadece Ã§alÄ±ÅŸÄ±r durumdaki gÃ¶revler iÃ§in WebSocket baÅŸlat
    if (!isRunning || !task_id) {
        // EÄŸer gÃ¶rev artÄ±k Ã§alÄ±ÅŸmÄ±yorsa veya task_id yoksa, state'i sÄ±fÄ±rla veya varsayÄ±lan yap
        setLiveProgress({ epoch: 0, totalEpochs: totalEpochs, text: 'BaÅŸlatÄ±ldÄ±' });
        return;
    }

    const socket = new WebSocket(`ws://localhost:8000/ws/task_status/${task_id}`);

    socket.onmessage = (event) => {
      const data = JSON.parse(event.data);
      if (data.state === 'PROGRESS' && data.details) {
        setLiveProgress({
          epoch: data.details.epoch,
          totalEpochs: data.details.total_epochs,
          text: data.details.status_text,
        });
      } else if (data.state === 'SUCCESS' || data.state === 'FAILURE') {
        // GÃ¶rev bittiÄŸinde, progress bar'Ä± ve text'i final duruma getir
        setLiveProgress(prev => ({
          ...prev,
          epoch: prev.totalEpochs, // Son epoch'a getir
          text: data.state === 'SUCCESS' ? 'TamamlandÄ±' : `Hata: ${data.result?.error?.message || data.details?.status_text}`,
        }));
      }
    };

    socket.onerror = (err) => {
      console.error(`WebSocket Error for task ${task_id}:`, err);
      setLiveProgress(prev => ({ ...prev, text: 'WebSocket HatasÄ±!' }));
    };

    socket.onclose = () => {
        // Zaten baÅŸarÄ±lÄ± veya baÅŸarÄ±sÄ±z olduysa, state'i deÄŸiÅŸtirmeyelim
        if (!isFinished) { 
            setLiveProgress(prev => ({ ...prev, text: 'BaÄŸlantÄ± kesildi.' }));
        }
    };

    return () => {
      if (socket.readyState === WebSocket.OPEN) {
        socket.close(1000, "Component unmounting or task finished");
      }
    };
  }, [isRunning, task_id, totalEpochs, isFinished]);

  const progressPercent = liveProgress.totalEpochs > 0 ? (liveProgress.epoch / liveProgress.totalEpochs) * 100 : 0;

  return (
    <div className={`experiment-card card ${isSelected ? 'selected-card' : ''} ${isDetailsExpanded ? 'expanded' : ''}`}>
      {/* Kart Ãœst BÃ¶lÃ¼mÃ¼: Checkbox, Durum, Ä°sim, ID, Batch AdÄ±, Aksiyonlar */}
      <div className="card-top-section">
        <div className="card-checkbox-status">
          <input type="checkbox" checked={isSelected} onChange={onSelect} title="KarÅŸÄ±laÅŸtÄ±rmak iÃ§in seÃ§"/>
          <span className={`status-badge status-${status?.toLowerCase() || 'unknown'}`}>{status || 'Bilinmiyor'}</span>
        </div>
        <div className="card-main-info">
            <h3 className="pipeline-name">{pipeline_name || 'N/A'}</h3>
            <span className="experiment-id">ID: {experiment_id.slice(0, 10)}...</span>
            {full_config?.batch_name && <span className="batch-name">Grup: {full_config.batch_name}</span>}
        </div>
        
        {/* Aksiyon MenÃ¼sÃ¼ */}
        <div className="actions-cell">
          <button className="actions-button" onClick={() => setActionsOpen(!actionsOpen)}>â‹®</button>
          {actionsOpen && (
            <div className="actions-menu" onMouseLeave={() => setActionsOpen(false)}>
              <button onClick={() => { setIsDetailsExpanded(!isDetailsExpanded); setActionsOpen(false); }}>
                <Icon path={ICONS.expand} /> {isDetailsExpanded ? 'DetaylarÄ± Gizle' : 'DetaylarÄ± GÃ¶ster'}
              </button>
              <button onClick={handleCopyId}><Icon path={ICONS.copy} /> ID'yi Kopyala</button>
            </div>
          )}
        </div>
      </div>

      {/* Ana GÃ¶vde: Ã–zet Metrikler, Ä°lerleme Ã‡ubuÄŸu ve Grafikler */}
      <div className="card-body">
        <div className="card-metrics-summary">
          <p><strong>Sembol:</strong> {full_config?.data_sourcing?.ticker || 'N/A'}</p> 
          <p><strong>Epochs:</strong> {totalEpochs || 'N/A'}</p>
          <p><strong>LR:</strong> {full_config?.training_params?.lr || 'N/A'}</p> 
          <p><strong>Final KayÄ±p:</strong> {displayLoss}</p>
          <p><strong>BaÅŸlangÄ±Ã§:</strong> {startTime}</p>
          <p><strong>BitiÅŸ:</strong> {endTime}</p>
          {isFinished && full_results?.metrics?.r2_score !== undefined && (
            <p><strong>RÂ² Skoru:</strong> {full_results.metrics.r2_score.toFixed(4)}</p>
          )}
          {isFinished && full_results?.metrics?.mae !== undefined && (
            <p><strong>MAE:</strong> {full_results.metrics.mae.toFixed(4)}</p>
          )}
        </div>

        {/* CanlÄ± Ä°lerleme Ã‡ubuÄŸu (sadece Ã§alÄ±ÅŸÄ±rken) */}
        {isRunning && (
            <div className="live-progress-bar-section">
                <div style={{ display: 'flex', justifyContent: 'space-between', alignItems: 'baseline', marginBottom: '5px' }}>
                    <p style={{ margin: 0, color: getCssVar('--text-color-darker'), fontSize: '0.8em' }}>
                        CanlÄ± Takip: {liveProgress.text}
                    </p>
                    {liveProgress.totalEpochs > 0 && (
                        <span style={{ fontWeight: 'bold', fontFamily: 'var(--font-mono)', fontSize: '0.8em' }}>
                            {liveProgress.epoch} / {liveProgress.totalEpochs}
                        </span>
                    )}
                </div>
                <progress value={progressPercent} max="100"></progress>
            </div>
        )}

        {/* Grafik BÃ¶lÃ¼mÃ¼ (Her zaman gÃ¶rÃ¼nÃ¼r) */}
        <div className="card-charts-section">
          {/* KayÄ±p GrafiÄŸi */}
          <SingleExperimentChart 
            taskId={task_id} 
            mode={isRunning ? 'live' : 'report'} 
            chartType="loss" 
            reportData={full_results} 
          />
          {/* Tahmin GrafiÄŸi */}
          <SingleExperimentChart 
            taskId={task_id} 
            mode={isRunning ? 'live' : 'report'} 
            chartType="prediction" 
            reportData={full_results} 
          />
        </div>
      </div>

      {/* Detaylar BÃ¶lÃ¼mÃ¼ (Katlanabilir) */}
      <div className="card-expanded-details">
        {status === 'FAILURE' && full_error && (
            <div style={{marginBottom: '15px'}}>
                <h4 style={{marginTop: 0, color: getCssVar('--error-color')}}>Hata DetayÄ±</h4>
                <pre style={{backgroundColor: getCssVar('--bg-color'), padding: '10px', borderRadius: '8px', whiteSpace: 'pre-wrap', maxHeight: '150px', overflowY: 'auto', fontSize: '0.8em', color: getCssVar('--error-color')}}>
                    <code>{JSON.stringify(full_error, null, 2)}</code>
                </pre>
            </div>
        )}
        <h4 style={{marginTop: 0}}>DetaylÄ± KonfigÃ¼rasyon</h4>
        <pre style={{backgroundColor: getCssVar('--bg-color'), padding: '10px', borderRadius: '8px', whiteSpace: 'pre-wrap', maxHeight: '200px', overflowY: 'auto', fontSize: '0.8em'}}>
          <code>{JSON.stringify(full_config, null, 2)}</code>
        </pre>
      </div>
    </div>
  );
}

ExperimentCard.propTypes = {
  experiment: PropTypes.object.isRequired,
  isSelected: PropTypes.bool.isRequired,
  onSelect: PropTypes.func.isRequired,
};

export default React.memo(ExperimentCard); // React.memo ile sarmalayÄ±n
========== FILE: dashboard/src/components/Logo.jsx ==========
import PropTypes from 'prop-types';

function Logo({ size = 36 }) {
  return (
    <div className="logo-container">
      <svg width={size} height={size} viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" style={{ flexShrink: 0 }}>
        <defs>
          <linearGradient id="azura-stream-gradient-component" x1="15" y1="50" x2="85" y2="50" gradientUnits="userSpaceOnUse">
            <stop offset="0%" stopColor="var(--secondary-color)"/>
            <stop offset="100%" stopColor="var(--primary-color)"/>
          </linearGradient>
        </defs>
        <path d="M50 10 L10 90 H32 L42 70 H58 L68 90 H90 Z" fill="var(--content-bg)" stroke="var(--text-color-darker)" strokeWidth="4" strokeLinejoin="round"/>
        <path d="M15 55 C 35 45, 65 45, 85 55" stroke="url(#azura-stream-gradient-component)" strokeWidth="8" strokeLinecap="round" fill="none"/>
      </svg>
      <h1>AzuraForge</h1>
    </div>
  );
}
Logo.propTypes = { size: PropTypes.number, };
export default Logo;
========== FILE: dashboard/src/components/NewExperimentPanel.jsx ==========
// dashboard/src/components/NewExperimentPanel.jsx

import React from 'react';
import PropTypes from 'prop-types';
// NewExperiment artÄ±k sadece form iÃ§eriÄŸi, sayfa baÅŸlÄ±ÄŸÄ± kaldÄ±rÄ±ldÄ±
import NewExperimentFormContent from '../pages/NewExperiment'; 

function CloseIcon() {
  return (
    <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round">
      <line x1="18" y1="6" x2="6" y2="18"></line>
      <line x1="6" y1="6" x2="18" y2="18"></line>
    </svg>
  );
}

function NewExperimentPanel({ isOpen, onClose, onExperimentStarted }) {
  const panelRef = React.useRef(null);

  React.useEffect(() => {
    if (isOpen && panelRef.current) {
      // Panelin iÃ§eriÄŸine veya ilk inputa focus vermek iÃ§in
      panelRef.current.focus(); 
    }
  }, [isOpen]);

  // Overlay'e tÄ±klanÄ±nca paneli kapatma (event propagation'Ä± durdurmak iÃ§in)
  const handleOverlayClick = (e) => {
    if (e.target === e.currentTarget) {
      onClose();
    }
  };

  return (
    <div className={`new-experiment-panel-overlay ${isOpen ? 'open' : ''}`} onClick={handleOverlayClick}>
      <div 
        ref={panelRef} 
        className={`new-experiment-panel ${isOpen ? 'open' : ''}`}
        onKeyDown={(e) => e.key === 'Escape' && onClose()}
        tabIndex="-1" 
      >
        <div className="new-experiment-panel-header">
          <h2>Yeni Deney BaÅŸlat</h2>
          <button className="close-button" onClick={onClose} title="Kapat">
            <CloseIcon />
          </button>
        </div>
        <div className="new-experiment-panel-body">
          <NewExperimentFormContent 
            onExperimentStarted={onExperimentStarted} 
            onClosePanel={onClose} 
          />
        </div>
      </div>
    </div>
  );
}

NewExperimentPanel.propTypes = {
  isOpen: PropTypes.bool.isRequired,
  onClose: PropTypes.func.isRequired,
  onExperimentStarted: PropTypes.func.isRequired,
};

export default NewExperimentPanel;
========== FILE: dashboard/src/components/SingleExperimentChart.jsx ==========
// dashboard/src/components/SingleExperimentChart.jsx

import React, { useState, useEffect, useMemo, useRef } from 'react'; 
import PropTypes from 'prop-types';
import { Line } from 'react-chartjs-2';
import { 
  Chart as ChartJS, CategoryScale, LinearScale, PointElement, LineElement, 
  Title, Tooltip, Legend, Filler, TimeScale 
} from 'chart.js';
import 'chartjs-adapter-date-fns';
import zoomPlugin from 'chartjs-plugin-zoom'; 
import { getCssVar } from '../utils/cssUtils';

ChartJS.register(CategoryScale, LinearScale, PointElement, LineElement, Title, Tooltip, Legend, Filler, TimeScale, zoomPlugin);

const getChartOptions = (title, chartColors, isTimeScale = false, enableZoom = false, compactMode = false) => {
  const options = {
    responsive: true, 
    maintainAspectRatio: false,
    animation: { duration: 300, easing: 'linear' },
    plugins: { 
      legend: { display: compactMode ? false : true, position: 'top', labels: { font: { size: compactMode ? 8 : 10, color: chartColors.textColor } } },
      title: { display: compactMode ? false : true, text: title, font: { size: compactMode ? 10 : 14, color: chartColors.textColor } },
      tooltip: {
        enabled: true, backgroundColor: chartColors.contentBg,
        titleColor: chartColors.textColor, bodyColor: chartColors.textColor,
        borderColor: chartColors.border, borderWidth: 1, padding: 5, 
        displayColors: true,
        bodyFont: { size: compactMode ? 9 : 12 }, 
        titleFont: { size: compactMode ? 9 : 12 },
        callbacks: {
            label: (ctx) => `${ctx.dataset.label}: ${typeof ctx.parsed.y === 'number' ? ctx.parsed.y.toFixed(4) : ctx.parsed.y}`,
        }
      },
    },
    scales: { 
      y: { 
        grid: { color: chartColors.border, borderDash: [2, 4], drawTicks: false },
        ticks: { display: compactMode ? false : true, padding: 5, maxTicksLimit: compactMode ? 2 : 5, font: { size: compactMode ? 8 : 10, color: chartColors.textColorDarker } },
        beginAtZero: true, 
      }, 
      x: {
        grid: { display: false },
        ticks: { display: compactMode ? false : true, padding: 5, maxRotation: 0, autoSkip: true, maxTicksLimit: compactMode ? 3 : 7, font: { size: compactMode ? 8 : 10, color: chartColors.textColorDarker } },
      } 
    },
    layout: {
      padding: {
        left: compactMode ? 5 : 10, right: compactMode ? 5 : 10, top: compactMode ? 0 : 5, bottom: compactMode ? 0 : 5
      }
    }
  };

  if (isTimeScale) {
    options.scales.x = { 
      type: 'time', 
      time: { unit: 'day', tooltipFormat: 'yyyy-MM-dd' },
      ticks: { font: { size: compactMode ? 8 : 10, color: chartColors.textColorDarker } } 
    };
  }

  if (enableZoom) {
    options.plugins.zoom = {
      pan: { enabled: true, mode: 'x', modifierKey: 'alt', },
      zoom: { wheel: { enabled: true }, pinch: { enabled: true }, mode: 'x' }
    };
  }

  return options;
};


function SingleExperimentChart({ 
  taskId, 
  mode, 
  chartType, 
  reportData, 
}) {
  const [liveData, setLiveData] = useState({
    loss: [],
    prediction: { x_axis: [], y_true: [], y_pred: [] }
  });

  const chartRef = useRef(null); 

  const chartColors = useMemo(() => ({
    primary: getCssVar('--primary-color'),
    info: getCssVar('--info-color'), 
    error: getCssVar('--error-color'), 
    border: getCssVar('--border-color'),
    textColor: getCssVar('--text-color'),
    textColorDarker: getCssVar('--text-color-darker'),
    contentBg: getCssVar('--content-bg'),
    textInverse: getCssVar('--text-inverse'),
  }), []);

  // CanlÄ± Takip (WebSocket) MantÄ±ÄŸÄ±
  useEffect(() => {
    let socket;
    if (mode === 'live' && taskId) {
        socket = new WebSocket(`ws://localhost:8000/ws/task_status/${taskId}`);
        
        // HÄ±z sorununu gidermek iÃ§in bir throttle mekanizmasÄ± dÃ¼ÅŸÃ¼nebiliriz.
        // Ancak bu, grafiklerin anlÄ±k akÄ±ÅŸÄ±nÄ± bozabilir. Åimdilik bu kÄ±smÄ± optimize etmiyoruz.
        // EÄŸer yavaÅŸlÄ±k devam ederse, buraya bir throttle ekleyebiliriz.

        socket.onmessage = (event) => {
            const data = JSON.parse(event.data);
            if (data.state === 'PROGRESS' && data.details) {
                setLiveData(prev => {
                    let updatedLoss = [...prev.loss];
                    let updatedPrediction = { ...prev.prediction };

                    if (data.details.loss !== undefined) {
                        const newLoss = data.details.loss;
                        const newEpoch = data.details.epoch;
                        const lastEpochIndex = updatedLoss.length - 1;
                        if (newEpoch > updatedLoss.length) { 
                            updatedLoss.push(newLoss);
                        } else if (lastEpochIndex >= 0) { 
                            updatedLoss[lastEpochIndex] = newLoss;
                        } else { 
                            updatedLoss.push(newLoss);
                        }
                    }
                    // BURADAKÄ° GÃœNCELLEME: validation_data varsa prediction'Ä± gÃ¼ncelle
                    if (data.details.validation_data) {
                        updatedPrediction = data.details.validation_data;
                    }
                    return { loss: updatedLoss, prediction: updatedPrediction };
                });
            } else if (data.state === 'SUCCESS' && data.result?.results) {
                setLiveData({
                    loss: data.result.results.history?.loss || [],
                    prediction: {
                        x_axis: data.result.results.time_index || [],
                        y_true: data.result.results.y_true || [],
                        y_pred: data.result.results.y_pred || [],
                    }
                });
            }
        };
        
        socket.onerror = (err) => { console.error(`WebSocket Error for task ${taskId}:`, err); };
        socket.onclose = () => {}; 
    }
    
    return () => { 
      if (socket && socket.readyState === WebSocket.OPEN) {
        socket.close(1000, "Component unmounting or task finished"); 
      }
    };
  }, [mode, taskId]); 

  const currentLossHistory = mode === 'live' ? liveData.loss : reportData?.history?.loss || [];
  const currentPredictionXAxis = mode === 'live' ? liveData.prediction.x_axis : reportData?.time_index || [];
  const currentPredictionYTrue = mode === 'live' ? liveData.prediction.y_true : reportData?.y_true || [];
  const currentPredictionYPred = mode === 'live' ? liveData.prediction.y_pred : reportData?.y_pred || [];


  const chartData = useMemo(() => {
    if (chartType === 'loss') {
      return {
        labels: currentLossHistory.map((_, i) => `E${i + 1}`),
        datasets: [{
          label: 'EÄŸitim KaybÄ±',
          data: currentLossHistory,
          borderColor: chartColors.primary,
          backgroundColor: `color-mix(in srgb, ${chartColors.primary} 20%, transparent)`,
          tension: 0.4,
          borderWidth: 2,
          pointRadius: 0,
          fill: 'origin',
        }]
      };
    } else if (chartType === 'prediction') {
      return {
        labels: currentPredictionXAxis,
        datasets: [
          {
            label: 'GerÃ§ek', 
            data: currentPredictionYTrue,
            borderColor: chartColors.info,
            backgroundColor: `color-mix(in srgb, ${chartColors.info} 20%, transparent)`,
            pointRadius: 0,
            fill: 'origin'
          },
          {
            label: 'Tahmin', 
            data: currentPredictionYPred,
            borderColor: chartColors.error,
            borderDash: [5, 5],
            pointRadius: 0,
            fill: false
          }
        ].map(dataset => ({
            ...dataset,
            data: dataset.data.map((val, i) => ({ x: currentPredictionXAxis[i], y: val }))
        }))
      };
    }
    return { labels: [], datasets: [] };
  }, [chartType, currentLossHistory, currentPredictionXAxis, currentPredictionYTrue, currentPredictionYPred, chartColors]);

  const chartTitle = chartType === 'loss' ? 'KayÄ±p' : 'Tahmin'; 

  const hasData = chartType === 'loss' ? currentLossHistory.length > 0 : currentPredictionYTrue.length > 0;

  return (
    <div className="single-chart-container">
      {hasData ? (
        <Line 
          ref={chartRef} 
          data={chartData} 
          options={getChartOptions(
            chartTitle, 
            chartColors, 
            chartType === 'prediction', 
            chartType === 'prediction' && mode === 'report', 
            true 
          )} 
        />
      ) : (
        <div className="no-chart-data-message">
          {mode === 'live' ? 'CanlÄ± veri bekleniyor...' : 'Veri mevcut deÄŸil.'}
        </div>
      )}
       {(chartType === 'prediction' && mode === 'report') && ( 
            <p className="chart-instructions">YakÄ±nlaÅŸtÄ±rmak iÃ§in fare tekerleÄŸi, kaydÄ±rmak iÃ§in Alt + SÃ¼rÃ¼kle.</p>
        )}
    </div>
  );
}

export default React.memo(SingleExperimentChart);
========== FILE: dashboard/src/components/ThemeToggle.jsx ==========
import { useContext } from 'react';
import { ThemeContext } from '../context/ThemeContext';

const SunIcon = () => <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round"><circle cx="12" cy="12" r="5"></circle><line x1="12" y1="1" x2="12" y2="3"></line><line x1="12" y1="21" x2="12" y2="23"></line><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line><line x1="1" y1="12" x2="3" y2="12"></line><line x1="21" y1="12" x2="23" y2="12"></line><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line></svg>;
const MoonIcon = () => <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path></svg>;

function ThemeToggle() {
  const { theme, toggleTheme } = useContext(ThemeContext);

  return (
    <button onClick={toggleTheme} className="theme-toggle-button" title="TemayÄ± DeÄŸiÅŸtir">
      {theme === 'light' ? <MoonIcon /> : <SunIcon />}
    </button>
  );
}

export default ThemeToggle;
========== FILE: dashboard/src/components/TopNavbar.jsx ==========
// dashboard/src/components/TopNavbar.jsx

import React from 'react';
import PropTypes from 'prop-types';
import Logo from './Logo';
import ThemeToggle from './ThemeToggle';

function TopNavbar({ onNewExperimentClick }) {
  return (
    <nav className="top-navbar">
      <Logo />
      <div style={{ display: 'flex', alignItems: 'center', gap: '15px' }}>
        <button className="button-primary" onClick={onNewExperimentClick}>
          <span role="img" aria-label="rocket">ğŸš€</span> Yeni Deney BaÅŸlat
        </button>
        <ThemeToggle />
      </div>
    </nav>
  );
}

TopNavbar.propTypes = {
  onNewExperimentClick: PropTypes.func.isRequired,
};

export default TopNavbar;
========== FILE: dashboard/src/context/ThemeContext.jsx ==========
import { createContext, useState, useEffect, useMemo } from 'react';
import PropTypes from 'prop-types';

export const ThemeContext = createContext();

export function ThemeProvider({ children }) {
  const [theme, setTheme] = useState(() => {
    const savedTheme = localStorage.getItem('theme');
    const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
    return savedTheme || (prefersDark ? 'dark' : 'light');
  });

  useEffect(() => {
    const body = document.body;
    // body'nin class'Ä±nÄ± mevcut temaya gÃ¶re ayarla
    body.classList.toggle('light-theme', theme === 'light');
    // SeÃ§imi tarayÄ±cÄ± hafÄ±zasÄ±na kaydet
    localStorage.setItem('theme', theme);
  }, [theme]);

  const toggleTheme = () => {
    setTheme(prevTheme => (prevTheme === 'light' ? 'dark' : 'light'));
  };

  // Sadece theme ve toggleTheme'i dÄ±ÅŸarÄ±ya veriyoruz.
  const value = useMemo(() => ({ theme, toggleTheme }), [theme]);

  return (
    <ThemeContext.Provider value={value}>
      {children}
    </ThemeContext.Provider>
  );
}

ThemeProvider.propTypes = {
  children: PropTypes.node.isRequired,
};
========== FILE: dashboard/src/pages/DashboardOverview.jsx ==========
import { useState, useEffect, useMemo } from 'react';
import ExperimentCard from '../components/ExperimentCard'; 
import ComparisonView from '../components/ComparisonView';
import { fetchExperiments } from '../services/api'; // fetchExperimentDetails kaldÄ±rÄ±ldÄ±, tek Ã§aÄŸrÄ± yeterli
import { toast } from 'react-toastify'; 

function DashboardOverview() {
  const [experiments, setExperiments] = useState([]);
  const [loading, setLoading] = useState(true);
  const [error, setError] = useState(null);
  
  const [searchTerm, setSearchTerm] = useState('');
  const [filterStatus, setFilterStatus] = useState('ALL');
  
  const [selectedForComparison, setSelectedForComparison] = useState(new Set());
  const [comparisonData, setComparisonData] = useState(null);

  // API'den tÃ¼m deney verilerini tek Ã§aÄŸrÄ±da Ã§ekiyoruz
  const getExperiments = async (showLoadingIndicator = false) => {
    if (showLoadingIndicator) setLoading(true);
    try {
      // ArtÄ±k API'nin kendisi tÃ¼m detaylarÄ± dÃ¶ndÃ¼rÃ¼yor olmalÄ±
      const response = await fetchExperiments();
      setExperiments(response.data); // Data zaten detailed olarak geliyor
      setError(null);
    } catch (err) {
      setError('API sunucusuna baÄŸlanÄ±lamadÄ± veya veri Ã§ekilemedi. Servislerin Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun.');
      console.error(err);
    } finally {
      if (showLoadingIndicator) setLoading(false);
    }
  };

  useEffect(() => {
    getExperiments(true);
    // PerformansÄ± iyileÅŸtirmek iÃ§in, bu intervalÄ± daha uzun tutabiliriz (Ã¶rn: 10-15 saniye)
    // Veya sadece active gÃ¶revler iÃ§in daha sÄ±k, diÄŸerleri iÃ§in daha az sÄ±k gÃ¼ncelleme yapÄ±labilir.
    const intervalId = setInterval(() => getExperiments(false), 5000); 
    return () => clearInterval(intervalId);
  }, []);

  const allStatuses = useMemo(() => {
    const statuses = new Set(experiments.map(exp => exp.status));
    return ['ALL', ...Array.from(statuses).sort()];
  }, [experiments]);

  const filteredExperiments = useMemo(() => {
    return experiments.filter(exp => {
      const statusMatch = filterStatus === 'ALL' || exp.status === filterStatus;
      if (!statusMatch) return false;

      if (searchTerm) {
        const lowerCaseSearchTerm = searchTerm.toLowerCase();
        const searchFields = [
          exp.experiment_id,
          exp.pipeline_name,
          exp.config_summary?.ticker, // config_summary kullanÄ±labilir
          exp.batch_name,
          // exp.config?.data_sourcing?.ticker, // Zaten config_summary'de var, gereksiz tekrar
        ];
        // Ek olarak, full_config'ten diÄŸer alanlarda da arama yapÄ±labilir (Ã¶rn. model_params iÃ§inde bir deÄŸer)
        // Ancak bu, searchFields array'ini daha karmaÅŸÄ±k hale getirir.
        return searchFields.some(field => typeof field === 'string' && field.toLowerCase().includes(lowerCaseSearchTerm));
      }

      return true;
    });
  }, [experiments, filterStatus, searchTerm]);
  
  const handleComparisonSelect = (experimentId) => {
    setSelectedForComparison(prev => {
      const newSelection = new Set(prev);
      if (newSelection.has(experimentId)) {
        newSelection.delete(experimentId);
      } else {
        newSelection.add(experimentId);
      }
      return newSelection;
    });
  };

  const handleStartComparison = async () => {
    const idsToCompare = Array.from(selectedForComparison);
    if (idsToCompare.length < 2) {
        toast.warn('KarÅŸÄ±laÅŸtÄ±rma iÃ§in en az 2 deney seÃ§melisiniz.');
        return;
    }
    
    // API'den zaten tÃ¼m detaylar geldiÄŸi iÃ§in burada tekrar fetch yapmaya gerek yok.
    const dataToCompare = idsToCompare.map(id => 
        experiments.find(exp => exp.experiment_id === id)
    ).filter(Boolean); 

    const validDataForComparison = dataToCompare.filter(exp => 
        exp.status === 'SUCCESS' && exp.results?.history?.loss && exp.results.history.loss.length > 0
    );

    if (validDataForComparison.length < 2) {
        toast.warn('KarÅŸÄ±laÅŸtÄ±rma iÃ§in en az 2 adet, baÅŸarÄ±lÄ± ve kayÄ±p geÃ§miÅŸi olan deney seÃ§melisiniz.');
        setComparisonData(null); 
        return;
    }

    setComparisonData(validDataForComparison);
  };

  if (loading) return <p style={{textAlign: 'center', padding: '40px'}}>Deney verileri yÃ¼kleniyor...</p>;
  if (error) return <p style={{textAlign: 'center', padding: '40px', color: 'var(--error-color)'}}>{error}</p>;

  return (
    <div className="dashboard-overview">
      {comparisonData && <ComparisonView experiments={comparisonData} onClose={() => setComparisonData(null)}/>}
      
      <div className="page-header">
        <h1>Genel BakÄ±ÅŸ</h1>
        <p>TÃ¼m deneylerinizi tek bir yerden yÃ¶netin, takip edin ve karÅŸÄ±laÅŸtÄ±rÄ±n.</p>
      </div>
      
      <div className="card" style={{ marginBottom: '25px' }}>
        <div style={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center', flexWrap: 'wrap', gap: '20px' }}>
          <div style={{ display: 'flex', gap: '20px', flexWrap: 'wrap', alignItems: 'flex-end' }}>
            <div className="form-group" style={{ marginBottom: 0 }}>
              <label htmlFor="search-term">Arama</label>
              <input type="text" id="search-term" placeholder="ID, Pipeline, Sembol ara..." value={searchTerm} onChange={(e) => setSearchTerm(e.target.value)} />
            </div>
            <div className="form-group" style={{ marginBottom: 0 }}>
              <label htmlFor="filter-status">Durum</label>
              <select id="filter-status" value={filterStatus} onChange={(e) => setFilterStatus(e.target.value)}>
                {allStatuses.map(s => <option key={s} value={s}>{s === 'ALL' ? 'TÃ¼mÃ¼' : s}</option>)}
              </select>
            </div>
          </div>
          <button 
            className="button-primary" 
            onClick={handleStartComparison} 
            disabled={selectedForComparison.size < 2} 
            title={selectedForComparison.size < 2 ? 'KarÅŸÄ±laÅŸtÄ±rmak iÃ§in en az 2 deney seÃ§in' : ''}
          >
            <span role="img" aria-label="scales">âš–ï¸</span> SeÃ§ilenleri KarÅŸÄ±laÅŸtÄ±r ({selectedForComparison.size})
          </button>
        </div>
      </div>
      
      {/* KartlarÄ± dikey olarak sÄ±ralamak iÃ§in flex column kullanÄ±yoruz */}
      <div className="experiments-list-container"> 
        {filteredExperiments.length === 0 ? (
          <p style={{textAlign: 'center', padding: '20px'}}>Filtrelerinize uyan bir deney bulunamadÄ±.</p>
        ) : (
          filteredExperiments.map((exp) => (
            <ExperimentCard 
              key={exp.experiment_id} 
              experiment={exp} 
              isSelected={selectedForComparison.has(exp.experiment_id)}
              onSelect={() => handleComparisonSelect(exp.experiment_id)}
            />
          ))
        )}
      </div>
    </div>
  );
}

export default DashboardOverview;
========== FILE: dashboard/src/pages/NewExperiment.jsx ==========
// dashboard/src/pages/NewExperiment.jsx
// Bu dosya artÄ±k bir React sayfasÄ± deÄŸil, sadece form iÃ§eriÄŸi saÄŸlayan bir bileÅŸen.

import { useState, useEffect, useCallback } from 'react';
import PropTypes from 'prop-types';
import { toast } from 'react-toastify';
import { fetchAvailablePipelines, fetchPipelineDefaultConfig, startNewExperiment } from '../services/api';

// Helper: AÅŸaÄŸÄ± ok ikonu (Katlanabilir bÃ¶lÃ¼mler iÃ§in)
const ChevronDownIcon = ({ className = '' }) => (
  <svg className={className} width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round">
    <polyline points="6 9 12 15 18 9"></polyline>
  </svg>
);
ChevronDownIcon.propTypes = { className: PropTypes.string };


// YardÄ±mcÄ± fonksiyon: Bir deÄŸeri (string, number, array) sayÄ± dizisine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
const parseNumberListInput = (value) => {
  if (value === null || value === undefined || value === '') return []; 
  
  if (Array.isArray(value)) { 
    return value.map(v => typeof v === 'string' ? parseFloat(v.replace(',', '.')) : v); 
  }
  
  if (typeof value === 'number') { 
    return [value]; 
  }

  if (typeof value === 'string') {
    const parts = value.split(',').map(s => s.trim()).filter(s => s !== '');
    return parts.map(s => {
      const decimalReadyString = s.replace(',', '.'); 
      const num = parseFloat(decimalReadyString);
      return isNaN(num) ? NaN : num; 
    });
  }
  
  return [NaN]; 
};

// YardÄ±mcÄ± fonksiyon: Input deÄŸerini string'e dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r (UI'da gÃ¶rÃ¼ntÃ¼lemek iÃ§in).
const formatListInput = (value) => {
  if (Array.isArray(value)) {
    return value.join(', ');
  }
  return value?.toString() ?? ''; 
};


// --- Her Pipeline Ä°Ã§in Ã–zel Form BileÅŸeni ---

function StockPredictorConfigForm({ config, onConfigChange, errors }) {
  const localConfig = JSON.parse(JSON.stringify(config || {}));
  
  const [expandedSections, setExpandedSections] = useState({
    data_sourcing: true, 
    feature_engineering: false, 
    model_params: false,
    training_params: false,
    system: false,
  });

  useEffect(() => {
    setExpandedSections(prev => ({ ...prev, data_sourcing: true }));
  }, [config]);

  const toggleSection = (sectionName) => {
    setExpandedSections(prev => ({ ...prev, [sectionName]: !prev[sectionName] }));
  };

  const handleChange = (e, path) => {
    const { value, type } = e.target;
    let newValue = value;

    if (type === 'select-one' && (value === 'true' || value === 'false')) {
        newValue = (value === 'true');
    }

    let updatedConfig = JSON.parse(JSON.stringify(localConfig));
    let current = updatedConfig;
    const pathParts = path.split('.'); 

    for (let i = 0; i < pathParts.length - 1; i++) {
      current = current[pathParts[i]] = current[pathParts[i]] || {};
    }
    current[pathParts[pathParts.length - 1]] = newValue; 
    onConfigChange(updatedConfig);
  };

  if (!config) return <p>KonfigÃ¼rasyon yÃ¼klenemedi.</p>;

  const renderFieldset = (sectionName, legendText, content) => (
    <fieldset className={`form-fieldset collapsible-fieldset ${expandedSections[sectionName] ? 'expanded' : ''}`}>
      <div className={`collapsible-header ${!expandedSections[sectionName] ? 'collapsed' : ''}`} onClick={() => toggleSection(sectionName)}>
        <span>{legendText}</span>
        <ChevronDownIcon className="icon" />
      </div>
      <div className="collapsible-content">
        {content}
      </div>
    </fieldset>
  );


  return (
    <>
      {renderFieldset('data_sourcing', 'Veri KaynaÄŸÄ±', (
        <div className="form-group">
          <label htmlFor="ticker">Ticker SembolÃ¼:</label>
          <input
            type="text"
            id="ticker"
            value={formatListInput(localConfig.data_sourcing?.ticker)}
            onChange={(e) => handleChange(e, 'data_sourcing.ticker')}
            className={errors['data_sourcing-ticker'] ? 'input-error' : ''}
            placeholder="Ã–rn: MSFT, AAPL, GOOG"
          />
          <small>Tek bir deÄŸer veya virgÃ¼lle ayrÄ±lmÄ±ÅŸ birden fazla deÄŸer (Ã¶rn: MSFT, AAPL, GOOG)</small>
          {errors['data_sourcing-ticker'] && <span className="form-error-message">{errors['data_sourcing-ticker']}</span>}
        </div>
      ))}

      {renderFieldset('feature_engineering', 'Ã–zellik MÃ¼hendisliÄŸi', (
        <div className="form-group">
          <label htmlFor="target_col_transform">Hedef SÃ¼tun DÃ¶nÃ¼ÅŸÃ¼mÃ¼:</label>
          <select
            id="target_col_transform"
            value={localConfig.feature_engineering?.target_col_transform || 'none'}
            onChange={(e) => handleChange(e, 'feature_engineering.target_col_transform')}
            className={errors['feature_engineering-target_col_transform'] ? 'input-error' : ''}
          >
            <option value="none">Yok</option>
            <option value="log">Log (log1p)</option>
          </select>
          <small>Modelin hedef sÃ¼tunu Ã¼zerinde uygulanacak dÃ¶nÃ¼ÅŸÃ¼m.</small>
          {errors['feature_engineering-target_col_transform'] && <span className="form-error-message">{errors['feature_engineering-target_col_transform']}</span>}
        </div>
      ))}

      {renderFieldset('model_params', 'Model Parametreleri', (
        <>
          <div className="form-group">
            <label htmlFor="sequence_length">Sekans UzunluÄŸu:</label>
            <input
              type="text" 
              id="sequence_length"
              value={formatListInput(localConfig.model_params?.sequence_length)}
              onChange={(e) => handleChange(e, 'model_params.sequence_length')}
              className={errors['model_params-sequence_length'] ? 'input-error' : ''}
              placeholder="Ã–rn: 30, 60, 90"
            />
            <small>GeÃ§miÅŸ kaÃ§ gÃ¼nÃ¼n verisinin girdi olarak kullanÄ±lacaÄŸÄ±.</small>
            {errors['model_params-sequence_length'] && <span className="form-error-message">{errors['model_params-sequence_length']}</span>}
          </div>
          <div className="form-group">
            <label htmlFor="hidden_size">Gizli Katman Boyutu:</label>
            <input
              type="text" 
              id="hidden_size"
              value={formatListInput(localConfig.model_params?.hidden_size)}
              onChange={(e) => handleChange(e, 'model_params.hidden_size')}
              className={errors['model_params-hidden_size'] ? 'input-error' : ''}
              placeholder="Ã–rn: 50, 100, 150"
            />
            <small>LSTM katmanÄ±ndaki gizli birim sayÄ±sÄ±.</small>
            {errors['model_params-hidden_size'] && <span className="form-error-message">{errors['model_params-hidden_size']}</span>}
          </div>
        </>
      ))}

      {renderFieldset('training_params', 'EÄŸitim Parametreleri', (
        <>
          <div className="form-group">
            <label htmlFor="epochs">Epoch SayÄ±sÄ±:</label>
            <input
              type="text" 
              id="epochs"
              value={formatListInput(localConfig.training_params?.epochs)}
              onChange={(e) => handleChange(e, 'training_params.epochs')}
              className={errors['training_params-epochs'] ? 'input-error' : ''}
              placeholder="Ã–rn: 50, 100"
            />
            <small>Modelin eÄŸitim veri seti Ã¼zerinden kaÃ§ kez geÃ§eceÄŸi.</small>
            {errors['training_params-epochs'] && <span className="form-error-message">{errors['training_params-epochs']}</span>}
          </div>
          <div className="form-group">
            <label htmlFor="lr">Ã–ÄŸrenme OranÄ± (LR):</label>
            <input
              type="text" 
              id="lr"
              value={formatListInput(localConfig.training_params?.lr)}
              onChange={(e) => handleChange(e, 'training_params.lr')}
              className={errors['training_params-lr'] ? 'input-error' : ''}
              placeholder="Ã–rn: 0.001, 0.0001"
            />
            <small>Optimizer'Ä±n aÄŸÄ±rlÄ±klarÄ± ne kadar hÄ±zlÄ± gÃ¼ncelleyeceÄŸi.</small>
            {errors['training_params-lr'] && <span className="form-error-message">{errors['training_params-lr']}</span>}
          </div>
          <div className="form-group">
            <label htmlFor="optimizer">Optimizer:</label>
            <select
              id="optimizer"
              value={localConfig.training_params?.optimizer || 'adam'}
              onChange={(e) => handleChange(e, 'training_params.optimizer')}
              className={errors['training_params-optimizer'] ? 'input-error' : ''}
            >
              <option value="adam">Adam</option>
              <option value="sgd">SGD</option>
            </select>
            <small>Modelin aÄŸÄ±rlÄ±klarÄ±nÄ± gÃ¼ncellemek iÃ§in kullanÄ±lan algoritma.</small>
            {errors['training_params-optimizer'] && <span className="form-error-message">{errors['training_params-optimizer']}</span>}
          </div>
          <div className="form-group">
            <label htmlFor="test_size">Test Seti Boyutu (0.0 - 1.0):</label>
            <input
              type="text" 
              id="test_size"
              value={formatListInput(localConfig.training_params?.test_size)}
              onChange={(e) => handleChange(e, 'training_params.test_size')}
              className={errors['training_params-test_size'] ? 'input-error' : ''}
              placeholder="Ã–rn: 0.1, 0.2"
            />
            <small>Veri setinin test iÃ§in ayrÄ±lacak oranÄ±.</small>
            {errors['training_params-test_size'] && <span className="form-error-message">{errors['training_params-test_size']}</span>}
          </div>
          <div className="form-group">
            <label htmlFor="validate_every">Her KaÃ§ Epoch'ta Bir DoÄŸrula:</label>
            <input
              type="text" 
              id="validate_every"
              value={formatListInput(localConfig.training_params?.validate_every)}
              onChange={(e) => handleChange(e, 'training_params.validate_every')}
              className={errors['training_params-validate_every'] ? 'input-error' : ''}
              placeholder="Ã–rn: 5, 10"
            />
            <small>CanlÄ± takip panelindeki tahmin grafiÄŸinin kaÃ§ epoch'ta bir gÃ¼ncelleneceÄŸi.</small>
            {errors['training_params-validate_every'] && <span className="form-error-message">{errors['training_params-validate_every']}</span>}
          </div>
        </>
      ))}
      
      {renderFieldset('system', 'Sistem AyarlarÄ±', (
        <>
          <div className="form-group">
            <label htmlFor="caching_enabled">Ã–nbellek Etkin mi?</label>
            <select
              id="caching_enabled"
              value={localConfig.system?.caching_enabled ? 'true' : 'false'}
              onChange={(e) => handleChange(e, 'system.caching_enabled')} 
              className={errors['system-caching_enabled'] ? 'input-error' : ''}
            >
              <option value="true">Evet</option>
              <option value="false">HayÄ±r</option>
            </select>
            <small>Veri Ã§ekme iÅŸleminin Ã¶nbelleÄŸe alÄ±nÄ±p alÄ±nmayacaÄŸÄ±.</small>
            {errors['system-caching_enabled'] && <span className="form-error-message">{errors['system-caching_enabled']}</span>}
          </div>
          <div className="form-group">
            <label htmlFor="cache_max_age_hours">Ã–nbellek YaÅŸam SÃ¼resi (saat):</label>
            <input
              type="text" 
              id="cache_max_age_hours"
              value={formatListInput(localConfig.system?.cache_max_age_hours)}
              onChange={(e) => handleChange(e, 'system.cache_max_age_hours')}
              className={errors['system-cache_max_age_hours'] ? 'input-error' : ''}
              placeholder="Ã–rn: 12, 24"
            />
            <small>Ã–nbellekteki verinin kaÃ§ saat sonra geÃ§ersiz sayÄ±lacaÄŸÄ±.</small>
            {errors['system-cache_max_age_hours'] && <span className="form-error-message">{errors['system-cache_max_age_hours']}</span>}
          </div>
        </>
      ))}
    </>
  );
}

StockPredictorConfigForm.propTypes = {
  config: PropTypes.object,
  onConfigChange: PropTypes.func.isRequired,
  errors: PropTypes.object.isRequired,
};

// --- Ana NewExperiment BileÅŸeni (Formu ve ButonlarÄ± YÃ¶neten DÄ±ÅŸ BileÅŸen) ---
// Bu bileÅŸen artÄ±k bir React sayfasÄ± deÄŸil, NewExperimentPanel iÃ§inde kullanÄ±lacak.

function NewExperiment({ onExperimentStarted, onClosePanel }) { 
  const [pipelines, setPipelines] = useState([]);
  const [selectedPipelineId, setSelectedPipelineId] = useState('');
  const [currentConfig, setCurrentConfig] = useState(null);
  const [defaultConfig, setDefaultConfig] = useState(null); 
  const [batchName, setBatchName] = useState('');
  const [isLoading, setIsLoading] = useState(true);
  const [isSubmitting, setIsSubmitting] = useState(false);
  const [formErrors, setFormErrors] = useState({});

  useEffect(() => {
    const loadPipelines = async () => {
      setIsLoading(true);
      try {
        const response = await fetchAvailablePipelines();
        if (response.data && response.data.length > 0) {
          setPipelines(response.data);
          if (!selectedPipelineId) {
            setSelectedPipelineId(response.data[0].id);
          }
        }
      } catch (error) { 
        toast.error('Pipeline listesi yÃ¼klenemedi. API servisinin Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun.'); 
        console.error("Pipeline yÃ¼kleme hatasÄ±:", error);
      } finally { setIsLoading(false); }
    };
    loadPipelines();
  }, [selectedPipelineId]); 

  useEffect(() => {
    const loadPipelineConfig = async (pipelineId) => {
      if (!pipelineId) {
        setCurrentConfig(null);
        setDefaultConfig(null);
        setIsLoading(false);
        return;
      }
      setIsLoading(true);
      setCurrentConfig(null); 
      setDefaultConfig(null);
      setFormErrors({});
      try {
        const { data } = await fetchPipelineDefaultConfig(pipelineId);
        const formattedData = formatConfigForUI(data); 
        setCurrentConfig(formattedData); 
        setDefaultConfig(JSON.parse(JSON.stringify(data))); 
      } catch (error) { 
        toast.error(`KonfigÃ¼rasyon yÃ¼klenemedi: ${error.response?.data?.detail || error.message}`); 
        console.error("KonfigÃ¼rasyon yÃ¼kleme hatasÄ±:", error);
        setCurrentConfig({}); 
      } finally { 
        setIsLoading(false); 
      }
    };
    loadPipelineConfig(selectedPipelineId);
  }, [selectedPipelineId]);

  const formatConfigForUI = (config) => {
    const formatted = {};
    for (const key in config) {
        if (typeof config[key] === 'object' && config[key] !== null && !Array.isArray(config[key])) {
            formatted[key] = formatConfigForUI(config[key]); 
        } 
        else if (Array.isArray(config[key]) || typeof config[key] === 'number') {
            formatted[key] = formatListInput(config[key]); 
        }
        else {
            formatted[key] = config[key];
        }
    }
    return formatted;
  };

  const handleResetToDefault = useCallback(() => {
    if (defaultConfig) {
      setCurrentConfig(formatConfigForUI(defaultConfig));
      setFormErrors({});
      toast.info('KonfigÃ¼rasyon varsayÄ±lan ayarlara sÄ±fÄ±rlandÄ±.');
    }
  }, [defaultConfig]);

  const validateConfig = (config) => {
    const errors = {};

    const validateNumberField = (value, path, min = -Infinity, max = Infinity) => {
      const parsedValues = parseNumberListInput(value); 
      
      if (parsedValues.length === 0) {
          errors[path.join('-')] = 'Bu alan boÅŸ bÄ±rakÄ±lamaz.';
          return;
      }
      if (parsedValues.some(v => typeof v !== 'number' || isNaN(v) || v < min || v > max)) {
          errors[path.join('-')] = `${path[path.length - 1]} iÃ§in tÃ¼m deÄŸerler sayÄ±sal olmalÄ± ve ${min} ile ${max} arasÄ±nda olmalÄ±dÄ±r.`;
      }
    };

    const validateStringField = (value, path) => {
        if (Array.isArray(value)) { 
            if (value.length === 0 || value.some(v => typeof v !== 'string' || v.trim() === '')) {
                errors[path.join('-')] = `${path[path.length - 1]} boÅŸ bÄ±rakÄ±lamaz veya boÅŸ Ã¶ÄŸe iÃ§eremez.`;
            }
        } else if (typeof value !== 'string' || value.trim() === '') { 
            errors[path.join('-')] = `${path[path.length - 1]} boÅŸ bÄ±rakÄ±lamaz.`;
        }
    };

    if (config.data_sourcing) {
        validateStringField(config.data_sourcing.ticker, ['data_sourcing', 'ticker']);
    }

    if (config.feature_engineering) {
        if (typeof config.feature_engineering.target_col_transform !== 'string' || !['none', 'log'].includes(config.feature_engineering.target_col_transform.toLowerCase())) {
            errors['feature_engineering-target_col_transform'] = 'GeÃ§ersiz dÃ¶nÃ¼ÅŸÃ¼m seÃ§imi.';
        }
    }

    if (config.model_params) {
      validateNumberField(config.model_params.sequence_length, ['model_params', 'sequence_length'], 1); 
      validateNumberField(config.model_params.hidden_size, ['model_params', 'hidden_size'], 1);     
    }

    if (config.training_params) {
      validateNumberField(config.training_params.epochs, ['training_params', 'epochs'], 1);         
      validateNumberField(config.training_params.lr, ['training_params', 'lr'], 0);                 
      validateNumberField(config.training_params.test_size, ['training_params', 'test_size'], 0, 1); 
      validateNumberField(config.training_params.validate_every, ['training_params', 'validate_every'], 1); 
      
      if (typeof config.training_params.optimizer !== 'string' || !['adam', 'sgd'].includes(config.training_params.optimizer.toLowerCase())) {
          errors['training_params-optimizer'] = 'GeÃ§ersiz optimizer seÃ§imi.';
      }
    }

    if (config.system) {
        if (typeof config.system.caching_enabled !== 'boolean') { 
            errors['system-caching_enabled'] = 'Ã–nbellek etkinleÅŸtirme seÃ§imi yapÄ±lmalÄ±.';
        }
        validateNumberField(config.system.cache_max_age_hours, ['system', 'cache_max_age_hours'], 0); 
    }
    
    return errors;
  };

  const handleSubmit = async (e) => {
    e.preventDefault();
    if (!currentConfig) {
        toast.error('KonfigÃ¼rasyon henÃ¼z yÃ¼klenmedi.');
        return;
    }

    const configToSendToApi = JSON.parse(JSON.stringify(currentConfig));
    
    const numericFieldsPaths = [
        ['model_params', 'sequence_length'],
        ['model_params', 'hidden_size'],
        ['training_params', 'epochs'],
        ['training_params', 'lr'],
        ['training_params', 'test_size'],
        ['training_params', 'validate_every'],
        ['system', 'cache_max_age_hours']
    ];

    numericFieldsPaths.forEach(path => {
        let current = configToSendToApi;
        for (let i = 0; i < path.length - 1; i++) {
            current = current[path[i]];
            if (current === undefined || current === null) return; 
        }
        const fieldName = path[path.length - 1];
        
        if (current && current[fieldName] !== undefined && current[fieldName] !== null) {
            const parsed = parseNumberListInput(current[fieldName]);
            if (parsed.length === 1 && !isNaN(parsed[0])) {
                current[fieldName] = parsed[0];
            } else {
                current[fieldName] = parsed; 
            }
        }
    });

    if (configToSendToApi.data_sourcing?.ticker && Array.isArray(configToSendToApi.data_sourcing.ticker) && configToSendToApi.data_sourcing.ticker.length === 1) {
        configToSendToApi.data_sourcing.ticker = configToSendToApi.data_sourcing.ticker[0];
    }
    
    const errors = validateConfig(configToSendToApi); 
    if (Object.keys(errors).length > 0) {
      setFormErrors(errors); 
      toast.error('LÃ¼tfen formdaki hatalarÄ± dÃ¼zeltin.');
      return;
    }
    setFormErrors({}); 

    setIsSubmitting(true);
    configToSendToApi.pipeline_name = selectedPipelineId;

    if (batchName.trim()) { 
      configToSendToApi.batch_name = batchName.trim();
    } else {
        delete configToSendToApi.batch_name;
    }

    try {
      const { data } = await startNewExperiment(configToSendToApi); 
      if (data.batch_id) {
        toast.success(`ğŸ‰ Batch gÃ¶revi (${data.task_ids.length} deney) baÅŸarÄ±yla gÃ¶nderildi! Batch ID: ${data.batch_id.slice(0, 8)}...`);
      } else {
        toast.success(`ğŸš€ GÃ¶rev baÅŸarÄ±yla gÃ¶nderildi! Deney ID: ${data.task_id.slice(0, 8)}...`);
      }
      if (onExperimentStarted) onExperimentStarted(data.task_id); // Paneli kapatÄ±p ana sayfaya yÃ¶nlendirir
    } catch (err) {
      toast.error('Deney baÅŸlatÄ±lamadÄ±. API/Worker loglarÄ±nÄ± veya tarayÄ±cÄ± konsolunu kontrol edin.');
      console.error("Deney baÅŸlatma hatasÄ±:", err.response?.data?.detail || err.message || err);
    } finally {
      setIsSubmitting(false);
    }
  };
  
  const selectedPipelineDetails = pipelines.find(p => p.id === selectedPipelineId);

  const renderSelectedPipelineForm = () => {
    if (isLoading) {
        return <p>Parametreler yÃ¼kleniyor...</p>;
    }
    if (!currentConfig) { 
        return <p>Bu pipeline iÃ§in dÃ¼zenlenebilir konfigÃ¼rasyon bulunamadÄ±.</p>;
    }
    
    switch (selectedPipelineId) {
      case 'stock_predictor':
        return <StockPredictorConfigForm config={currentConfig} onConfigChange={setCurrentConfig} errors={formErrors} />;
      default:
        return <p>LÃ¼tfen bir pipeline seÃ§in veya bu pipeline iÃ§in yapÄ±landÄ±rma formu bulunamadÄ±.</p>;
    }
  };

  return (
    <form onSubmit={handleSubmit} className="new-experiment-panel-form"> 
        <div className="new-experiment-panel-form-content"> 
          <div className="card">
            <div className="form-group">
              <label htmlFor="pipeline-select">Ã‡alÄ±ÅŸtÄ±rÄ±lacak Pipeline Eklentisi</label>
              <select id="pipeline-select" value={selectedPipelineId} onChange={(e) => setSelectedPipelineId(e.target.value)} disabled={isLoading || isSubmitting}>
                {pipelines.map(p => <option key={p.id} value={p.id}>{p.name} ({p.id})</option>)}
              </select>
            </div>
            <div className="form-group">
              <label htmlFor="batch-name">Deney Grubu AdÄ± (Ä°steÄŸe BaÄŸlÄ±)</label>
              <input 
                type="text" 
                id="batch-name" 
                value={batchName} 
                onChange={(e) => setBatchName(e.target.value)} 
                placeholder="Ã–rn: LR ve Epoch Optimizasyonu"
                disabled={isLoading || isSubmitting}
              />
              <small style={{ color: 'var(--text-color-darker)', fontSize: '0.85em' }}>
                Birden fazla parametre kombinasyonu gÃ¶nderirken grubu isimlendirmek iÃ§in kullanÄ±lÄ±r.
              </small>
            </div>
          </div>
          
          <div className="card" style={{padding: 0}}> 
            <div className="collapsible-header" onClick={handleResetToDefault} style={{borderBottom: 'none', borderRadius: '8px', marginBottom: '15px', justifyContent: 'center'}}>
                <span role="img" aria-label="reset">ğŸ”„</span> VarsayÄ±lan KonfigÃ¼rasyona DÃ¶n
            </div>
            <h3>Deney Parametreleri</h3>
            {renderSelectedPipelineForm()}
          </div>
        </div>

        <div className="form-action-bar">
          <div className="pipeline-info">
            <strong>Pipeline:</strong>
            <span>{selectedPipelineDetails?.name || '...'}</span>
          </div>
          <button type="submit" disabled={isLoading || isSubmitting} className="button-primary">
            {isSubmitting ? 'BaÅŸlatÄ±lÄ±yor...' : 'EÄŸitimi BaÅŸlat'}
          </button>
        </div>
    </form>
  );
}

NewExperiment.propTypes = { 
    onExperimentStarted: PropTypes.func.isRequired,
    onClosePanel: PropTypes.func.isRequired, 
};
export default NewExperiment;
========== FILE: dashboard/src/services/api.js ==========
// dashboard/src/services/api.js

import axios from 'axios';

export const API_BASE_URL = 'http://localhost:8000/api/v1';

const apiClient = axios.create({
  baseURL: API_BASE_URL,
  headers: { 'Content-Type': 'application/json' },
});

// fetchExperiments artÄ±k tÃ¼m detaylarÄ± getiriyor
export const fetchExperiments = () => apiClient.get('/experiments');
export const startNewExperiment = (config) => apiClient.post('/experiments', config);
export const fetchAvailablePipelines = () => apiClient.get('/pipelines'); 
export const fetchPipelineDefaultConfig = (pipelineId) => apiClient.get(`/pipelines/${pipelineId}/config`);

// fetchExperimentDetails artÄ±k UI'da doÄŸrudan kullanÄ±lmayacak, ancak API'de kalabilir.
export const fetchExperimentDetails = (experimentId) => {
  return apiClient.get(`/experiments/${experimentId}/details`);
};
========== FILE: dashboard/src/utils/cssUtils.js ==========
/**
 * Belirtilen CSS deÄŸiÅŸkeninin hesaplanmÄ±ÅŸ deÄŸerini dÃ¶ndÃ¼rÃ¼r.
 * @param {string} varName - '--primary-color' gibi CSS deÄŸiÅŸken adÄ±.
 * @returns {string} - DeÄŸiÅŸkenin renk deÄŸeri (Ã¶rn. '#42b983').
 */
export const getCssVar = (varName) => {
  if (typeof window === 'undefined') return '';
  return getComputedStyle(document.documentElement).getPropertyValue(varName).trim();
};
========== FILE: docs/ARCHITECTURE.md ==========
# ğŸ—ï¸ AzuraForge Mimarisi

Bu belge, AzuraForge platformunu oluÅŸturan servislerin ve bileÅŸenlerin birbirleriyle nasÄ±l etkileÅŸime girdiÄŸini, Ã¶zellikle de **asenkron ve olay gÃ¼dÃ¼mlÃ¼ yapÄ±nÄ±n** nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± detaylandÄ±rmaktadÄ±r.

## 1. Temel BileÅŸenler ve SorumluluklarÄ±

Platform, her biri belirli bir gÃ¶reve odaklanmÄ±ÅŸ baÄŸÄ±msÄ±z servislerden oluÅŸur:

*   **Dashboard (ArayÃ¼z KatmanÄ±):**
    *   KullanÄ±cÄ±nÄ±n etkileÅŸime girdiÄŸi React tabanlÄ± web uygulamasÄ±.
    *   Deneyleri baÅŸlatÄ±r, canlÄ± ilerlemeyi gÃ¶sterir, raporlarÄ± gÃ¶rÃ¼ntÃ¼ler.
    *   Sadece `API` servisi ile konuÅŸur.

*   **API (Ä°letiÅŸim ve AÄŸ GeÃ§idi KatmanÄ±):**
    *   Platformun dÄ±ÅŸ dÃ¼nyaya aÃ§Ä±lan kapÄ±sÄ±dÄ±r.
    *   Gelen istekleri doÄŸrular ve gÃ¶revleri `Celery` kuyruÄŸuna (Redis) iletir.
    *   `Dashboard`'dan gelen canlÄ± takip istekleri iÃ§in `WebSocket` baÄŸlantÄ±larÄ±nÄ± yÃ¶netir.
    *   Redis Pub/Sub kanallarÄ±na **abone (subscribe)** olarak `Worker`'dan gelen olaylarÄ± dinler.

*   **Worker (Ä°ÅŸleme KatmanÄ±):**
    *   AÄŸÄ±r hesaplama yÃ¼kÃ¼nÃ¼ Ã¼stlenir (model eÄŸitimi, rapor oluÅŸturma vb.).
    *   `Celery` kuyruÄŸundan gÃ¶revleri alÄ±r ve iÅŸler.
    *   EÄŸitim sÄ±rasÄ±nda ilerleme bilgilerini (`loss`, `epoch` vb.) Redis Pub/Sub kanallarÄ±na **yayÄ±nlar (publish)**.
    *   `Learner` ve `Core` kÃ¼tÃ¼phanelerini kullanarak AI modellerini Ã§alÄ±ÅŸtÄ±rÄ±r.

*   **Redis (MesajlaÅŸma ve Ã–nbellek KatmanÄ±):**
    *   Platformun merkezi sinir sistemidir.
    *   **Celery Broker & Backend:** `API` ve `Worker` arasÄ±ndaki gÃ¶rev kuyruÄŸu ve sonuÃ§ deposu olarak hizmet eder.
    *   **Pub/Sub Sunucusu:** `Worker` ile `API` arasÄ±nda gerÃ§ek zamanlÄ±, bloklamayan iletiÅŸim iÃ§in kullanÄ±lÄ±r.

## 2. Bir Deneyin YaÅŸam DÃ¶ngÃ¼sÃ¼: Olay GÃ¼dÃ¼mlÃ¼ AkÄ±ÅŸ

AÅŸaÄŸÄ±daki ÅŸema, kullanÄ±cÄ± bir deneyi baÅŸlattÄ±ÄŸÄ± andan itibaren sistemde gerÃ§ekleÅŸen olaylar zincirini gÃ¶stermektedir.

```mermaid
sequenceDiagram
    participant D as Dashboard
    participant A as API Server
    participant R as Redis
    participant W as Worker

    D->>A: 1. POST /experiments (config ile)
    A->>R: 2. send_task('start_training', config)
    A-->>D: 3. { task_id: 'xyz' }

    D->>A: 4. WebSocket /ws/task_status/xyz
    Note over A: WebSocket baÄŸlantÄ±sÄ± aÃ§Ä±lÄ±r.
    A->>R: 5. SUBSCRIBE 'task-progress:xyz'
    Note over A: API artÄ±k bu kanalÄ± dinliyor.

    R->>W: 6. GÃ¶revi (task_id: 'xyz') teslim eder.
    Note over W: Worker, Pipeline'Ä± baÅŸlatÄ±r ve<br/>RedisProgressCallback'i ekler.
    
    loop EÄŸitim DÃ¶ngÃ¼sÃ¼ (Her Epoch Sonu)
        W->>W: Learner, on_epoch_end olayÄ±nÄ± tetikler.
        W->>R: 7. PUBLISH 'task-progress:xyz'<br/>{ epoch: n, loss: 0.123, ... }
    end

    R-->>A: 8. Kanalda yeni mesaj var!
    A-->>D: 9. WebSocket ile veriyi anÄ±nda iletir.
    Note over D: LiveTrackerPane gÃ¼ncellenir.
    
    Note over W: EÄŸitim biter.
    W->>R: 10. GÃ¶rev sonucunu (results.json) yazar.
    W-->>A: (Opsiyonel) GÃ¶rev durumu 'SUCCESS' olur.
```

### AkÄ±ÅŸÄ±n AdÄ±m AdÄ±m AÃ§Ä±klamasÄ±:

1.  **Deney BaÅŸlatma:** `Dashboard`, `API`'ye deney konfigÃ¼rasyonunu iÃ§eren bir HTTP POST isteÄŸi gÃ¶nderir.
2.  **GÃ¶rev KuyruÄŸa Atma:** `API`, bu isteÄŸi alÄ±r ve `Celery`'nin `send_task` metoduyla gÃ¶revi Redis'teki kuyruÄŸa bÄ±rakÄ±r.
3.  **AnÄ±nda Geri DÃ¶nÃ¼ÅŸ:** `API`, gÃ¶revin iÅŸlenmesini beklemeden, `Dashboard`'a anÄ±nda bir `task_id` dÃ¶ndÃ¼rÃ¼r. ArayÃ¼z "donmaz".
4.  **CanlÄ± Takip BaÄŸlantÄ±sÄ±:** `Dashboard`, aldÄ±ÄŸÄ± `task_id` ile `API`'nin WebSocket endpoint'ine baÄŸlanÄ±r.
5.  **Kanala Abone Olma:** `API`, bu `task_id`'ye Ã¶zel bir Redis Pub/Sub kanalÄ±na (`task-progress:xyz`) abone olur ve sessizce beklemeye baÅŸlar.
6.  **GÃ¶revi Alma:** `Worker`, Redis kuyruÄŸundaki gÃ¶revi alÄ±r ve `start_training_pipeline` gÃ¶revini Ã§alÄ±ÅŸtÄ±rmaya baÅŸlar.
7.  **Ä°lerleme YayÄ±nlama:** EÄŸitim sÄ±rasÄ±nda, `Learner`'daki `RedisProgressCallback`, her epoch sonunda ilerleme verisini (kayÄ±p, epoch vb.) ilgili Redis kanalÄ±na yayÄ±nlar.
8.  **MesajÄ± Yakalama:** `API`, abone olduÄŸu kanalda bir mesaj belirdiÄŸini anÄ±nda fark eder.
9.  **AnÄ±nda Ä°letim:** `API`, bu mesajÄ± alÄ±r ve olduÄŸu gibi WebSocket Ã¼zerinden `Dashboard`'a iletir. `Dashboard`'daki ilgili bileÅŸen (grafik, ilerleme Ã§ubuÄŸu) kendini gÃ¼nceller.
10. **GÃ¶revin TamamlanmasÄ±:** EÄŸitim bittiÄŸinde, `Worker` nihai sonuÃ§larÄ± (`results.json`) yazar ve Celery gÃ¶revini `SUCCESS` olarak iÅŸaretler.

Bu mimari, hesaplama (`Worker`) ve iletiÅŸim (`API`) katmanlarÄ±nÄ± birbirinden tamamen ayÄ±rarak platforma **saÄŸlamlÄ±k, Ã¶lÃ§eklenebilirlik ve gerÃ§ek zamanlÄ±lÄ±k** kazandÄ±rÄ±r.


========== FILE: docs/CONTRIBUTING.md ==========
========== FILE: docs/CONTRIBUTING.md ==========
# ğŸ¤ AzuraForge Platformuna KatkÄ±da Bulunma Rehberi

AzuraForge projesine gÃ¶sterdiÄŸiniz ilgi ve katkÄ±larÄ±nÄ±z iÃ§in teÅŸekkÃ¼r ederiz! Bu proje, modern yapay zeka sistemlerinin nasÄ±l inÅŸa edilmesi gerektiÄŸine dair bir vizyonu hayata geÃ§irmeyi amaÃ§lamaktadÄ±r. Bu rehber, kod tabanÄ±nÄ±n tutarlÄ±, okunabilir, sÃ¼rdÃ¼rÃ¼lebilir ve yÃ¼ksek kalitede kalmasÄ±nÄ± saÄŸlamak iÃ§in benimsediÄŸimiz Ã§alÄ±ÅŸma prensiplerini ve standartlarÄ±nÄ± aÃ§Ä±klamaktadÄ±r.

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

EÄŸer henÃ¼z geliÅŸtirme ortamÄ±nÄ±zÄ± kurmadÄ±ysanÄ±z, lÃ¼tfen platformun ana [GeliÅŸtirme Rehberi](./DEVELOPMENT_GUIDE.md) belgesindeki "GeliÅŸtirme OrtamÄ± Kurulumu" bÃ¶lÃ¼mÃ¼nÃ¼ takip edin.

## ğŸ› ï¸ Kodlama StandartlarÄ±

Projeye eklenen her kodun aÅŸaÄŸÄ±daki standartlarÄ± karÅŸÄ±lamasÄ± beklenmektedir. Bu standartlarÄ±n birÃ§oÄŸu, ilgili reponun kÃ¶k dizinindeki `pre-commit` hook'larÄ± ile otomatik olarak kontrol edilir.

1.  **Stil KÄ±lavuzu (PEP8 & Black):**
    *   TÃ¼m Python kodlarÄ±, PEP8 stil kurallarÄ±na uymalÄ±dÄ±r.
    *   Kodunuzu `black` ile otomatik formatlayÄ±n.
    *   **Kontrol:** `black .` komutunu Ã§alÄ±ÅŸtÄ±rÄ±n.
    *   **Otomasyon:** `pre-commit` hook'larÄ± bu formatlamayÄ± zorunlu kÄ±lar.

2.  **Linting (`flake8`):**
    *   TÃ¼m Python kodlarÄ±, `flake8` denetiminden hatasÄ±z geÃ§melidir.
    *   **Kontrol:** Ä°lgili repo iÃ§inde `flake8 src` komutunu Ã§alÄ±ÅŸtÄ±rÄ±n.
    *   **Otomasyon:** `pre-commit` hook'larÄ± linting'i zorunlu kÄ±lar.

3.  **Tip Ä°puÃ§larÄ± (Type Hinting & Mypy):**
    *   TÃ¼m fonksiyon ve metod imzalarÄ±, parametreler ve dÃ¶nÃ¼ÅŸ deÄŸerleri iÃ§in tip ipuÃ§larÄ± (`typing` modÃ¼lÃ¼ kullanÄ±larak) iÃ§ermelidir.
    *   **Kontrol:** Ä°lgili repo iÃ§inde `mypy src` komutunu Ã§alÄ±ÅŸtÄ±rÄ±n.
    *   **Otomasyon:** `pre-commit` hook'larÄ± statik tip denetimini zorunlu kÄ±lar.

4.  **DokÃ¼mantasyon (Docstrings):**
    *   TÃ¼m public modÃ¼ller, sÄ±nÄ±flar ve fonksiyonlar, ne iÅŸe yaradÄ±klarÄ±nÄ±, aldÄ±klarÄ± argÃ¼manlarÄ± (`Args:`) ve ne dÃ¶ndÃ¼rdÃ¼klerini (`Returns:`) aÃ§Ä±klayan Google-style docstring'ler iÃ§ermelidir.

5.  **Testler (`pytest`):**
    *   Eklenen her yeni Ã¶zellik veya fonksiyon iÃ§in ilgili birim testleri (`unit tests`) `tests/` klasÃ¶rÃ¼ne eklenmelidir.
    *   YapÄ±lan bir hata dÃ¼zeltmesi (bug fix) iÃ§in, o hatanÄ±n tekrar oluÅŸmasÄ±nÄ± engelleyecek bir regresyon testi yazÄ±lmalÄ±dÄ±r.
    *   **Kontrol:** Ä°lgili reponun kÃ¶k dizinindeyken `pytest` komutunu Ã§alÄ±ÅŸtÄ±rÄ±n.

## ğŸ“ Commit MesajlarÄ±

Commit mesajlarÄ±, yapÄ±lan deÄŸiÅŸikliÄŸi net bir ÅŸekilde aÃ§Ä±klamalÄ±dÄ±r ve [Conventional Commits](https://www.conventionalcommits.org/en/v1.0.0/) standardÄ±na uymalÄ±dÄ±r. Bu, otomatik versiyonlama ve deÄŸiÅŸiklik gÃ¼nlÃ¼ÄŸÃ¼ oluÅŸturmak iÃ§in hayati Ã¶nem taÅŸÄ±r.

**Format:**
```
<tip>(<kapsam>): <aÃ§Ä±klama>

[opsiyonel gÃ¶vde]
```

**Ã–rnek Tipler:**
*   `feat`: Yeni bir Ã¶zellik ekler (Minor versiyon artÄ±rÄ±mÄ±).
*   `fix`: Bir hata dÃ¼zeltmesi (Patch versiyon artÄ±rÄ±mÄ±).
*   `docs`: Sadece dokÃ¼mantasyon deÄŸiÅŸiklikleri.
*   `style`: Kod formatÄ±, eksik noktalÄ± virgÃ¼l gibi stil dÃ¼zeltmeleri.
*   `refactor`: Kodu yeniden yapÄ±landÄ±rma, davranÄ±ÅŸ deÄŸiÅŸikliÄŸi yok.
*   `perf`: Performans iyileÅŸtirmesi yapan kod deÄŸiÅŸikliÄŸi.
*   `test`: Eksik testlerin eklenmesi veya mevcut testlerin dÃ¼zeltilmesi.
*   `build`: Build sistemi veya dÄ±ÅŸ baÄŸÄ±mlÄ±lÄ±k deÄŸiÅŸiklikleri.
*   `ci`: CI/CD yapÄ±landÄ±rma dosyalarÄ± ve script'leri deÄŸiÅŸiklikleri.

**Ã–rnekler:**
*   `feat(learner): Add LSTM backward pass implementation`
*   `fix(api): Handle null values from experiment results`
*   `docs(platform): Update development guide with Pub/Sub architecture`
*   `refactor(worker): Extract Redis publishing logic into a callback`

## ğŸ”„ Pull Request (PR) SÃ¼reci

1.  **Branch OluÅŸturma:** `main` branch'inden kendi feature branch'inizi (`feat/yeni-ozellik` veya `fix/hata-adi` gibi) oluÅŸturun.
2.  **DeÄŸiÅŸikliklerinizi YapÄ±n:** YukarÄ±daki standartlara uyduÄŸunuzdan emin olun.
3.  **Test Edin:** Yerel testlerinizi (`pytest`) Ã§alÄ±ÅŸtÄ±rÄ±n ve geÃ§tiÄŸinden emin olun.
4.  **Commit ve Push:** DeÄŸiÅŸikliklerinizi anlamlÄ± commit mesajlarÄ±yla branch'inize `push` edin.
5.  **Pull Request AÃ§Ä±n:** GitHub Ã¼zerinden `main` branch'ine bir "Pull Request" (PR) aÃ§Ä±n.
6.  **CI Kontrolleri:** PR'Ä±nÄ±zÄ±n CI kontrollerinden (testler, linting) baÅŸarÄ±yla geÃ§tiÄŸinden emin olun.
7.  **Kod Ä°ncelemesi:** Kodunuz incelenecek ve gerekli geri bildirimler saÄŸlanacaktÄ±r.

Bu standartlara uyarak, AzuraForge platformunun uzun vadede saÄŸlÄ±klÄ±, sÃ¼rdÃ¼rÃ¼lebilir ve yÃ¼ksek kalitede kalmasÄ±na yardÄ±mcÄ± olursunuz.


## ğŸ“¦ Versiyonlama ve BaÄŸÄ±mlÄ±lÄ±k YÃ¶netimi

Platformun kararlÄ±lÄ±ÄŸÄ±nÄ± saÄŸlamak iÃ§in tÃ¼m Python paketlerimiz Anlamsal Versiyonlama (Semantic Versioning) ve Git etiketlerini kullanÄ±r. BaÄŸÄ±mlÄ±lÄ±klar asla `@main` branch'ine iÅŸaret etmemelidir.

### Bir KÃ¼tÃ¼phanede DeÄŸiÅŸiklik YapÄ±ldÄ±ÄŸÄ±nda Ä°zlenecek AdÄ±mlar:

Bir kÃ¼tÃ¼phanede (Ã¶rn: `learner`) bir hata dÃ¼zeltmesi veya yeni bir Ã¶zellik eklendiÄŸinde, aÅŸaÄŸÄ±daki adÄ±mlar izlenmelidir:

1.  **DeÄŸiÅŸiklikleri TamamlayÄ±n:** Gerekli kod deÄŸiÅŸikliklerini yapÄ±n, testleri gÃ¼ncelleyin ve `main` branch'ine birleÅŸtirin.

2.  **Versiyonu YÃ¼kseltin:** `pyproject.toml` dosyasÄ±ndaki `version` numarasÄ±nÄ± anlamsal versiyonlama kurallarÄ±na gÃ¶re artÄ±rÄ±n.
    *   `fix` (hata dÃ¼zeltmesi): `0.1.3` -> `0.1.4` (Patch artÄ±ÅŸÄ±)
    *   `feat` (yeni Ã¶zellik): `0.1.3` -> `0.2.0` (Minor artÄ±ÅŸ)

3.  **Yeni Versiyonu Etiketleyin:** Yeni versiyon numarasÄ±nÄ± bir Git etiketi olarak oluÅŸturun ve GitHub'a gÃ¶nderin.
    ```bash
    # learner/ dizinindeyken
    git tag v0.2.0
    git push origin v0.2.0
    ```

4.  **BaÄŸÄ±mlÄ± RepolarÄ± GÃ¼ncelleyin:** `learner` kÃ¼tÃ¼phanesini kullanan tÃ¼m diÄŸer repolarÄ±n (`api`, `app-stock-predictor` vb.) `pyproject.toml` dosyalarÄ±ndaki ilgili satÄ±rÄ± yeni versiyon etiketiyle (`...@v0.2.0`) gÃ¼ncelleyin.
========== FILE: docs/DEVELOPMENT_GUIDE.md ==========
========== FILE: docs/DEVELOPMENT_GUIDE.md ==========
# ğŸ› ï¸ AzuraForge Platform GeliÅŸtirme Rehberi

Bu belge, AzuraForge platformunda geliÅŸtirme yapmak isteyenler iÃ§in adÄ±m adÄ±m kurulum, Ã§alÄ±ÅŸma prensipleri ve katkÄ±da bulunma yÃ¶nergelerini iÃ§erir.

## ğŸ¯ Temel Felsefemiz

AzuraForge'da geliÅŸtirme yaparken, iki temel prensibi aklÄ±mÄ±zda tutarÄ±z:

1.  **BaÄŸÄ±msÄ±z Paketler:** Her repo (`core`, `learner`, `api` vb.), kendi baÅŸÄ±na yaÅŸayan, kurulabilir ve test edilebilir baÄŸÄ±msÄ±z bir Python/JavaScript paketidir.
2.  **DÃ¼zenlenebilir Kurulum:** Repolar arasÄ± baÄŸÄ±mlÄ±lÄ±klar, yerel geliÅŸtirmeyi hÄ±zlandÄ±rmak iÃ§in `pip install -e` (editable) komutuyla kurulur. Bu sayede bir kÃ¼tÃ¼phanede yaptÄ±ÄŸÄ±nÄ±z deÄŸiÅŸiklik, diÄŸerlerine anÄ±nda yansÄ±r.


Her repomuz, kendi baÅŸÄ±na yaÅŸayan, kurulabilir ve test edilebilir baÄŸÄ±msÄ±z bir Python/JavaScript paketidir. Repolar arasÄ± baÄŸÄ±mlÄ±lÄ±klar, Git adresleri (`@git+https://...`) Ã¼zerinden kurulur.

## ğŸ“¦ Proje RepolarÄ±na Genel BakÄ±ÅŸ

AzuraForge platformu, aÅŸaÄŸÄ±daki baÄŸÄ±msÄ±z GitHub depolarÄ±ndan oluÅŸur. GeliÅŸtirme yaparken bu repolarÄ±n bir kÄ±smÄ±nÄ± veya tamamÄ±nÄ± yerel makinenizde klonlamanÄ±z gerekecektir.

*   **`core`**: Temel otomatik tÃ¼rev motoru.
*   **`learner`**: `core` Ã¼zerinde yÃ¼ksek seviyeli Ã¶ÄŸrenme kÃ¼tÃ¼phanesi.
*   **`app-stock-predictor`**: Bir uygulama eklentisi Ã¶rneÄŸi.
*   **`applications`**: Resmi uygulama katalogu.
*   **`api`**: RESTful API ve WebSocket sunucusu (Redis Pub/Sub dinleyicisi).
*   **`worker`**: Arka plan gÃ¶revlerini iÅŸleyen Celery worker (Redis Pub/Sub yayÄ±ncÄ±sÄ±).
*   **`dashboard`**: React tabanlÄ± web kullanÄ±cÄ± arayÃ¼zÃ¼.
*   **`platform`**: TÃ¼m servisleri bir araya getiren ana orkestrasyon deposu (bu repo).

## âš™ï¸ GeliÅŸtirme OrtamÄ± Kurulumu

Bu adÄ±mlar, platformun tÃ¼m parÃ§alarÄ±nÄ± yerel geliÅŸtirme iÃ§in hazÄ±r hale getirir.

1.  **Gerekli AraÃ§lar:** Git, Python 3.8+, Node.js & npm, Docker Desktop.

2.  **RepolarÄ± Klonlama:**
    TÃ¼m ilgili repolarÄ± aynÄ± seviyede bir klasÃ¶re klonlayÄ±n:
    ```bash
    mkdir azuraforge-dev
    cd azuraforge-dev

    git clone https://github.com/AzuraForge/platform.git
    git clone https://github.com/AzuraForge/core.git
    git clone https://github.com/AzuraForge/learner.git
    git clone https://github.com/AzuraForge/applications.git
    git clone https://github.com/AzuraForge/app-stock-predictor.git
    git clone https://github.com/AzuraForge/api.git
    git clone https://github.com/AzuraForge/worker.git
    git clone https://github.com/AzuraForge/dashboard.git
    ```

3.  **Sanal Ortam ve BaÄŸÄ±mlÄ±lÄ±klar (Python):**

    **`.env` DosyasÄ±nÄ± OluÅŸturma:**
    Platformu Ã§alÄ±ÅŸtÄ±rmadan Ã¶nce, ana `platform` dizininde bir `.env` dosyasÄ± oluÅŸturun. Bu dosya, servislerin ortak dizinlere eriÅŸimini saÄŸlar.
    ```
    # .env
    REDIS_URL=redis://redis:6379/0
    REPORTS_DIR=./reports
    CACHE_DIR=./.cache
    ```

    Yerel geliÅŸtirme iÃ§in, **`platform` projesinin** kÃ¶k dizininde tek bir sanal ortam oluÅŸturup tÃ¼m Python baÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± oraya kurmak en pratik yoldur.

    ```bash
    cd platform # Ana `platform` reposunun iÃ§ine gir
    python -m venv .venv
    # Windows: .\.venv\Scripts\activate | Linux/macOS: source ./.venv/bin/activate
    ```
    # TÃ¼m Python repolarÄ±nÄ± "dÃ¼zenlenebilir" modda kur
    ```bash
    pip install -e ../core 
    pip install -e ../learner
    pip install -e ../applications
    pip install -e ../app-stock-predictor
    pip install -e ../api
    pip install -e ../worker
    ```

4.  **JavaScript BaÄŸÄ±mlÄ±lÄ±klarÄ± (Dashboard):**
    ```bash
    cd ../dashboard # `dashboard` reposunun iÃ§ine gir
    npm install
    ```

5.  **Redis Kurulumu (Docker ile):**
    ```bash
    docker run -d -p 6379:6379 --name azuraforge_redis redis
    ```

## â–¶ï¸ Servisleri Ã‡alÄ±ÅŸtÄ±rma (Yerel GeliÅŸtirme)

Sanal ortamÄ±nÄ±z aktifken ve Redis Ã§alÄ±ÅŸÄ±rken, her servisi ayrÄ± bir terminalde baÅŸlatÄ±n.

1.  **API Sunucusu (`api` reposundan):**
    ```bash
    cd ../api # veya bulunduÄŸunuz yere gÃ¶re ayarlayÄ±n
    # Gerekirse sanal ortamÄ± aktive et
    start-api
    ```

2.  **Worker Servisi (`worker` reposundan):**
    ```bash
    cd ../worker
    # Gerekirse sanal ortamÄ± aktive et
    start-worker
    ```

3.  **Dashboard (`dashboard` reposundan):**
    ```bash
    cd ../dashboard
    npm run dev
    ```

##  ğŸ”„ Ä°teratif GeliÅŸtirme AkÄ±ÅŸÄ±

Ã‡oÄŸu zaman, kodda kÃ¼Ã§Ã¼k deÄŸiÅŸiklikler yapÄ±p bunlarÄ± hÄ±zla test etmek istersiniz.

1.  **KÃ¼tÃ¼phanede DeÄŸiÅŸiklik (Ã¶rn: `core/src/azuraforge_core/tensor.py`):**
    *   DeÄŸiÅŸikliÄŸi yapÄ±n ve kaydedin.
    *   Bu deÄŸiÅŸikliÄŸin diÄŸer kÃ¼tÃ¼phanelerde anÄ±nda etkili olmasÄ± iÃ§in **ekstra bir `pip install` komutuna GEREK YOKTUR**, Ã§Ã¼nkÃ¼ `-e` ile kurulduklarÄ± iÃ§in doÄŸrudan kaynak dosyayÄ± kullanÄ±rlar.
    *   `core` projesine geri dÃ¶nÃ¼p birim testlerini (`pytest`) koÅŸarak deÄŸiÅŸikliÄŸi doÄŸrulayÄ±n.
    *   DeÄŸiÅŸikliÄŸi `commit`'leyin ve `push`'layÄ±n.

2.  **Uygulama/Servis DeÄŸiÅŸikliÄŸi (Ã¶rn: `app-stock-predictor/src/azuraforge_stockapp/pipeline.py`):**
    *   DeÄŸiÅŸikliÄŸi yapÄ±n ve kaydedin.
    *   `api` veya `worker` servisleri otomatik olarak `reload` (yeniden yÃ¼kleme) yapacaktÄ±r (eÄŸer `uvicorn --reload` ile Ã§alÄ±ÅŸÄ±yorlarsa). DeÄŸiÅŸikliÄŸin etkisini gÃ¶rmek iÃ§in genellikle ilgili servisi (API veya Worker) yeniden baÅŸlatmak yeterlidir.
    *   DeÄŸiÅŸikliÄŸi `commit`'leyin ve `push`'layÄ±n.

3.  **Yeni Bir BaÄŸÄ±mlÄ±lÄ±k EklendiÄŸinde (`pyproject.toml` deÄŸiÅŸtiÄŸinde):**
    *   Bir reponun (Ã¶rn: `learner`) `pyproject.toml` dosyasÄ±na yeni bir baÄŸÄ±mlÄ±lÄ±k (Ã¶rn: `pandas`) eklediyseniz, bu deÄŸiÅŸikliÄŸin diÄŸer repolar tarafÄ±ndan tanÄ±nmasÄ± iÃ§in **baÄŸÄ±mlÄ±lÄ±k zincirini yeniden kurmanÄ±z gerekir.**
    *   `platform` klasÃ¶rÃ¼ndeki ana sanal ortamÄ±nÄ±zÄ± aktive edin.
    *   `pip install -e ../learner` komutunu tekrar Ã§alÄ±ÅŸtÄ±rÄ±n. `pip`, sadece eksik olan yeni baÄŸÄ±mlÄ±lÄ±klarÄ± (`pandas`) ekleyecektir.

##  CanlÄ± Takip Mimarisi NasÄ±l Ã‡alÄ±ÅŸÄ±r?

1.  `Dashboard`, `API`'ye bir `/experiments` POST isteÄŸi atar.
2.  `API`, gÃ¶revi `Celery` kuyruÄŸuna bÄ±rakÄ±r ve `Dashboard`'a bir `task_id` dÃ¶ner.
3.  `Dashboard`, bu `task_id` ile `API`'nin `/ws/task_status/{task_id}` WebSocket endpoint'ine baÄŸlanÄ±r.
4.  `API`, bu baÄŸlantÄ± iÃ§in bir Redis istemcisi oluÅŸturur ve `task-progress:{task_id}` kanalÄ±na **abone (subscribe)** olur.
5.  `Worker`, gÃ¶revi kuyruktan alÄ±r ve `Learner`'Ä±, iÃ§ine `RedisProgressCallback` enjekte edilmiÅŸ ÅŸekilde Ã§alÄ±ÅŸtÄ±rÄ±r.
6.  `Learner`, her epoch sonunda `on_epoch_end` olayÄ±nÄ± yayÄ±nlar.
7.  `RedisProgressCallback`, bu olayÄ± yakalar ve ilerleme verisini (epoch, loss) Redis'teki `task-progress:{task_id}` kanalÄ±na **yayÄ±nlar (publish)**.
8.  `API`, abone olduÄŸu kanalda yeni bir mesaj duyar, onu alÄ±r ve WebSocket Ã¼zerinden anÄ±nda `Dashboard`'a iletir.
9.  `Dashboard`'daki `LiveTrackerPane` bileÅŸeni, gelen bu veriyle kendini gÃ¼nceller.

Bu yapÄ±, `Worker`'Ä±n CPU kullanÄ±mÄ± ne kadar yoÄŸun olursa olsun, raporlama ve arayÃ¼z gÃ¼ncellemesinin bloklanmadan, anlÄ±k olarak gerÃ§ekleÅŸmesini saÄŸlar.

##  Platform Mimarisi NasÄ±l Ã‡alÄ±ÅŸÄ±r?

### Standart Bir Deney AkÄ±ÅŸÄ±

1.  **BaÅŸlatma (`Dashboard` -> `API` -> `Worker`):**
    *   `Dashboard`, kullanÄ±cÄ±dan aldÄ±ÄŸÄ± konfigÃ¼rasyon ile `API`'nin `/experiments` endpoint'ine bir `POST` isteÄŸi atar.
    *   `API`, gÃ¶revi `Celery` kuyruÄŸuna bÄ±rakÄ±r ve bir `task_id` dÃ¶ner.
    *   `Worker`, gÃ¶revi kuyruktan alÄ±r ve ilgili `Pipeline` eklentisinin bir Ã¶rneÄŸini oluÅŸturur.

2.  **EÄŸitim ve CanlÄ± Takip (`Worker` -> `Learner` -> `API` -> `Dashboard`):**
    *   `Worker`, eklentinin standart `run` metodunu Ã§aÄŸÄ±rÄ±r. Bu metot, `azuraforge-learner` iÃ§indeki `TimeSeriesPipeline`'den gelir.
    *   `run` metodu, `LivePredictionCallback` ve `RedisProgressCallback` gibi Ã¶zel `Callback`'ler oluÅŸturur.
    *   `Learner`'Ä±n `fit` metodu Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r. Her epoch sonunda:
        *   `LivePredictionCallback`, doÄŸrulama seti Ã¼zerinde tahmin yapar.
        *   `RedisProgressCallback`, hem kayÄ±p bilgisini hem de canlÄ± tahmin verisini birleÅŸtirerek Redis Pub/Sub kanalÄ±na yayÄ±nlar.
    *   `API`, bu kanala abone olduÄŸu iÃ§in mesajÄ± anÄ±nda alÄ±r ve `WebSocket` Ã¼zerinden `Dashboard`'a iletir.
    *   `LiveTrackerPane`, gelen bu zengin veriyle kendini (kayÄ±p grafiÄŸi, tahmin grafiÄŸi, ilerleme Ã§ubuÄŸu) gÃ¼nceller.

3.  **Tamamlama ve Raporlama (`Worker` -> `Learner`):**
    *   EÄŸitim bittiÄŸinde, `TimeSeriesPipeline`'in `run` metodu, son deÄŸerlendirmeyi yapar ve `azuraforge_learner.reporting` iÃ§indeki `generate_regression_report` fonksiyonunu Ã§aÄŸÄ±rÄ±r.
    *   Bu fonksiyon, `/reports` dizini altÄ±na, grafikleri iÃ§eren bir `report.md` dosyasÄ± oluÅŸturur.
    *   `run` metodu, deneyin tÃ¼m sonuÃ§larÄ±nÄ± (`history`, `metrics`, ham veriler) iÃ§eren bir JSON objesini `worker` gÃ¶revine dÃ¶ndÃ¼rÃ¼r.
    *   `Worker`, bu sonucu `results.json` dosyasÄ±na yazar ve gÃ¶revi `SUCCESS` olarak iÅŸaretler.

## ğŸ¤ KatkÄ±da Bulunma

Bu proje bir aÃ§Ä±k kaynak projesi olarak geliÅŸtirilmektedir. KatkÄ±da bulunmak iÃ§in lÃ¼tfen `platform/docs/CONTRIBUTING.md` dosyasÄ±nÄ± inceleyin.
========== FILE: docs/PROJECT_JOURNEY.md ==========
# ğŸ—ºï¸ Proje YolculuÄŸu: AzuraForge'un GeliÅŸim Hikayesi ve Gelecek Vizyonu

Bu belge, AzuraForge platformunun baÅŸlangÄ±cÄ±ndan mevcut durumuna kadar olan geliÅŸim sÃ¼recini, karÅŸÄ±laÅŸÄ±lan zorluklarÄ±, bulunan Ã§Ã¶zÃ¼mleri ve projenin **kendi kendini anlayan, sÄ±fÄ±rdan inÅŸa edilmiÅŸ, eklenti tabanlÄ± ve evrensel bir yapay zeka geliÅŸtirme platformuna** dÃ¶nÃ¼ÅŸme vizyonunu Ã¶zetlemektedir.

## ğŸš€ Proje Vizyonu ve Felsefesi

AzuraForge, basit bir araÃ§ seti olmanÄ±n Ã¶tesinde, bir felsefeyi temsil eder: **Ferrari motorunu (kanÄ±tlanmÄ±ÅŸ, sÄ±fÄ±rdan inÅŸa edilmiÅŸ AI gÃ¼cÃ¼) alÄ±p, modÃ¼ler ve geniÅŸletilebilir bir uzay gemisi ÅŸasisine (daÄŸÄ±tÄ±k MLOps mimarisi) monte etmek.** Bu uzay gemisi, yeni eklentilerle (`app-xx`) sÃ¼rekli geliÅŸtirilerek farklÄ± gÃ¶revleri yerine getirebilen, akÄ±llÄ± ve kendi kendini yÃ¶netebilen bir yapÄ±ya dÃ¶nÃ¼ÅŸÃ¼r.

Bu vizyonu gerÃ§ekleÅŸtirmek iÃ§in iki temel anayasal prensibe baÄŸlÄ±yÄ±z:

1.  **Ã‡ekirdek BaÄŸÄ±msÄ±zlÄ±ÄŸÄ± ve Derin AnlayÄ±ÅŸ ("Smart Learner" Ruhu):**
    Platformun kalbindeki (`azuraforge-core`, `azuraforge-learner`) algoritmalar ve yapÄ±lar, dÄ±ÅŸ kÃ¼tÃ¼phanelere minimal baÄŸÄ±mlÄ±lÄ±kla, temel prensipleri anlaÅŸÄ±larak sÄ±fÄ±rdan inÅŸa edilir. Bu bize tam kontrol, esneklik, ÅŸeffaflÄ±k ve derinlemesine bir "know-how" saÄŸlar.

2.  **ModÃ¼ler ve Ã–lÃ§eklenebilir Ekosistem ("AzuraForge" Mimarisi):**
    Ã‡ekirdeÄŸin saf gÃ¼cÃ¼; daÄŸÄ±tÄ±k, asenkron, olay gÃ¼dÃ¼mlÃ¼ ve eklenti tabanlÄ± bir mimariyle sunulur. Bu sayede platform; saÄŸlam, esnek, bÃ¼yÃ¼meye aÃ§Ä±k ve modern mÃ¼hendislik standartlarÄ±na uygun kalÄ±r.

---

## âœ… Tamamlanan Fazlar ve Elde Edilen BaÅŸarÄ±lar

### Faz 0: Fikir ve Prototip ("Smart Learner" Projesi)
- **DÃ¼ÅŸÃ¼nce:** Mevcut ML araÃ§larÄ±nÄ±n karmaÅŸÄ±klÄ±ÄŸÄ±na, "kara kutu" yapÄ±sÄ±na ve aÅŸÄ±rÄ± baÄŸÄ±mlÄ±lÄ±klarÄ±na bir tepki olarak, sÄ±fÄ±rdan bir derin Ã¶ÄŸrenme motoru (`mininn`) inÅŸa etme fikri doÄŸdu.
- **KanÄ±t:** Monolitik bir prototip olan "Smart Learner" projesi geliÅŸtirildi. SÄ±fÄ±rdan yazÄ±lan `LSTM` mimarisi, hava durumu tahmininde **RÂ² > 0.98**, hisse senedi fiyat tahmininde ise **RÂ² â‰ˆ 0.73** gibi somut ve etkileyici baÅŸarÄ±lar elde etti.
- **Ã–ÄŸrenilen Ders:** Monolitik yapÄ± hÄ±zlÄ± prototipleme saÄŸlasa da, Ã¶lÃ§eklenebilirlik, canlÄ± takip ve modÃ¼ler geniÅŸleme iÃ§in yetersizdi. Daha bÃ¼yÃ¼k bir vizyon iÃ§in paradigma deÄŸiÅŸimi gerekiyordu.

### Faz 1-3: Mikroservis Mimarisine GeÃ§iÅŸ ve Temel YapÄ±nÄ±n Ä°nÅŸasÄ±
- **Karar:** Uzun vadeli sÃ¼rdÃ¼rÃ¼lebilirlik iÃ§in platform, baÄŸÄ±msÄ±z repolara (`azuraforge-core`, `learner`, `api`, `worker`, `dashboard` vb.) sahip bir mikroservis mimarisine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼.
- **YapÄ±:** `docker-compose` ile orkestrasyon, `pip install -e` ile yerel geliÅŸtirme ve `git+https` ile repo'lar arasÄ± baÄŸÄ±mlÄ±lÄ±klar kuruldu.
- **Ä°lk BaÅŸarÄ±:** `Dashboard` -> `API` -> `Worker` -> `Uygulama` -> `Learner` -> `Core` ÅŸeklindeki temel gÃ¶rev akÄ±ÅŸÄ± baÅŸarÄ±yla Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±.

### Faz 4-8: GerÃ§ek ZamanlÄ± AkÄ±ÅŸÄ±n SaÄŸlanmasÄ± (Pub/Sub Mimarisi - DÃ¶nÃ¼m NoktasÄ±)
- **Sorun:** CPU-yoÄŸun eÄŸitim gÃ¶revleri, Worker'Ä±n durum gÃ¼ncelleme mesajlarÄ±nÄ± gÃ¶ndermesini engelleyerek arayÃ¼zÃ¼n "donmasÄ±na" neden oluyordu.
- **Ã‡Ã¶zÃ¼m:** Hesaplama ve raporlama gÃ¶revlerini tamamen ayÄ±rmak iÃ§in Redis Pub/Sub modeline geÃ§ildi.
    - `Learner` sadece olay (`on_epoch_end`) yayÄ±nlayan saf bir bileÅŸen haline getirildi.
    - `Worker`, `RedisProgressCallback` aracÄ±lÄ±ÄŸÄ±yla bu olaylarÄ± dinleyip bir Redis kanalÄ±na yayÄ±nlar hale geldi.
    - `API`, bu kanala abone olup, gelen her mesajÄ± WebSocket Ã¼zerinden anÄ±nda `Dashboard`'a iletir hale geldi.
- **BAÅARI:** Bu mimari deÄŸiÅŸiklik sayesinde, anlÄ±k ve akÄ±cÄ± bir canlÄ± takip deneyimi mÃ¼mkÃ¼n oldu.

### Faz 9-17: Mimari Olgunluk ve GeliÅŸmiÅŸ Yetenekler ("Checkpoint Echo")
- **Standardizasyon (`BasePipeline`):** Uygulama eklentisi geliÅŸtirmeyi standartlaÅŸtÄ±ran `TimeSeriesPipeline` soyut sÄ±nÄ±fÄ± oluÅŸturuldu.
- **Verimlilik (Caching):** Harici API Ã§aÄŸrÄ±larÄ±nÄ± Ã¶nbelleÄŸe alan merkezi bir caching mekanizmasÄ± eklendi.
- **AkÄ±llÄ± Ã–n Ä°ÅŸleme:** `TimeSeriesPipeline`'a, hedef deÄŸiÅŸkene otomatik logaritmik dÃ¶nÃ¼ÅŸÃ¼m ve ters dÃ¶nÃ¼ÅŸÃ¼m uygulama yeteneÄŸi kazandÄ±rÄ±ldÄ±.
- **Dinamik Raporlama:** Raporlama, statik Markdown dosyalarÄ±ndan, `results.json` verisini kullanan, `Chart.js` ile Ã§izilmiÅŸ **interaktif ve canlÄ± grafikler** sunan bir yapÄ±ya dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼.
- **BAÅARI:** Platform, kararlÄ±, canlÄ± takip yetenekli, dinamik raporlama sunan, verimli bir Ã¶nbellekleme mekanizmasÄ±na ve geliÅŸmiÅŸ Ã¶n iÅŸleme yeteneklerine sahip "Checkpoint Echo" kilometre taÅŸÄ±na ulaÅŸtÄ±.

---

## ğŸ—ºï¸ Gelecek Vizyonu ve Stratejik Yol HaritasÄ±

Bu saÄŸlam temel Ã¼zerine inÅŸa edilecek adÄ±mlar, AzuraForge'u daha da zenginleÅŸtirmeyi ve kapsamÄ±nÄ± geniÅŸletmeyi hedefleyecektir.

### **Faz 0: BÃ¼yÃ¼k BirleÅŸme (The Grand Unification) - Mevcut GÃ¼Ã§leri Konsolide Etme**
*Bu faz, "Smart Learner" prototipinin kanÄ±tlanmÄ±ÅŸ baÅŸarÄ±larÄ±nÄ± ve olgunlaÅŸmÄ±ÅŸ kodunu AzuraForge ekosistemine tam olarak entegre etmeyi hedefler.*

-   **1. KanÄ±tlanmÄ±ÅŸ Pipeline'larÄ± Eklenti Haline Getirme:**
    -   `weather_forecaster` (RÂ² > 0.98) ve `stock_predictor` (RÂ² â‰ˆ 0.73) pipeline'larÄ±nÄ±, AzuraForge standartlarÄ±na uygun, baÄŸÄ±msÄ±z `app-weather-forecaster` ve `app-stock-predictor` eklentileri olarak hayata geÃ§irmek.
-   **2. Motor ve Ã–ÄŸreniciyi YÃ¼kseltme:**
    -   "Smart Learner"daki daha olgun `Tensor` ve `LSTM` implementasyonlarÄ±nÄ±, birim testleriyle birlikte `azuraforge-core` ve `azuraforge-learner` paketlerine taÅŸÄ±mak.
-   **3. Hikayeyi BirleÅŸtirme:**
    -   Projenin tÃ¼m evrim hikayesini, bu belgede olduÄŸu gibi tek ve tutarlÄ± bir anlatÄ±da birleÅŸtirmek.

### **Faz 1: Deneyimi DerinleÅŸtirme ve ZenginleÅŸtirme**
*Bu faz, platformu daha profesyonel ve gÃ¼Ã§lÃ¼ kÄ±lacak temel MLOps yeteneklerini eklemeyi hedefler.*

-   **1. GeliÅŸmiÅŸ Model YÃ¶netimi ve Sunumu:**
    -   `ModelCheckpoint` callback'ini entegre ederek en iyi modelleri kalÄ±cÄ± olarak kaydetmek.
    -   `API`'ye `/models` ve `/models/{model_id}/predict` gibi yeni endpoint'ler ekleyerek, kaydedilmiÅŸ modellere eriÅŸim ve onlar Ã¼zerinden tahmin yapma imkanÄ± saÄŸlamak.
    -   `Dashboard`'a bir "Model KayÄ±t Defteri" sayfasÄ± eklemek.
-   **2. "Hiper SÃ¼rÃ¼cÃ¼" - Otomatik Hiperparametre Optimizasyonu:**
    -   `Dashboard` Ã¼zerinden parametre aralÄ±klarÄ± tanÄ±mlayarak hiperparametre optimizasyon gÃ¶revleri baÅŸlatma arayÃ¼zÃ¼ geliÅŸtirmek.
    -   `Worker`'Ä±n bu gÃ¶revleri yÃ¼zlerce alt gÃ¶reve bÃ¶lerek paralel Ã§alÄ±ÅŸtÄ±rmasÄ±nÄ± saÄŸlamak.
    -   SonuÃ§larÄ± `Dashboard`'da interaktif bir tablo veya Ä±sÄ± haritasÄ± ile gÃ¶rselleÅŸtirmek.
-   **3. GPU DesteÄŸinin Aktivasyonu (`CuPy`):**
    -   `docker-compose.yml` dosyasÄ±na GPU desteÄŸi eklemek ve platformun uyumlu donanÄ±mlarda `CuPy` ile hÄ±zlandÄ±rÄ±lmasÄ±nÄ± saÄŸlamak.

### **Faz 2: Evreni GeniÅŸletme (Yeni Veri Modaliteleri ve Yetenekler)**
*Bu faz, platformun farklÄ± veri tÃ¼rleriyle Ã§alÄ±ÅŸabilme yeteneÄŸini kanÄ±tlamayÄ± hedefler.*

-   **1. GÃ¶rÃ¼ntÃ¼ Ä°ÅŸleme ModÃ¼lÃ¼ (`app-image-classifier`):**
    -   `azuraforge-core`'a `Conv2D`, `MaxPool2D`, `Flatten` katmanlarÄ±nÄ± sÄ±fÄ±rdan eklemek.
    -   `azuraforge-learner`'a `BaseImagePipeline` soyut sÄ±nÄ±fÄ±nÄ± ve sÄ±nÄ±flandÄ±rma raporlamasÄ± (`Confusion Matrix` vb.) eklemek.
    -   `app-image-classifier` eklentisini geliÅŸtirmek.
-   **2. Ãœretken Yapay Zeka ModÃ¼lÃ¼ (`app-gan-generator`):**
    -   `azuraforge-core` ile basit bir GAN veya VAE mimarisi implemente etmek ve temel rakamlar Ã¼reten bir eklenti geliÅŸtirmek.
-   **3. DoÄŸal Dil Ä°ÅŸleme / Ses ModÃ¼lÃ¼ (Projenin KÃ¶kenine DÃ¶nÃ¼ÅŸ):**
    -   `azuraforge-core`'a `Embedding` katmanÄ± ve temel bir `Attention` mekanizmasÄ± eklemek.
    -   Metin sÄ±nÄ±flandÄ±rma veya ses tanÄ±ma gibi gÃ¶revler iÃ§in temel pipeline'larÄ± ve eklentileri oluÅŸturarak daha karmaÅŸÄ±k hedeflere (Ã¶rn: TTS) zemin hazÄ±rlamak.

### **Faz 3: GeleceÄŸi Kucaklamak (2025 ve Ã–tesi Trendleri)**
*Bu faz, platformu endÃ¼stri standardÄ± ve gerÃ§ekten "akÄ±llÄ±" bir sisteme dÃ¶nÃ¼ÅŸtÃ¼rmeyi hedefler.*

-   **1. Model BirleÅŸtirme ve Transfer Ã–ÄŸrenmesi:**
    -   Platforma, sÄ±fÄ±rdan eÄŸitmek yerine, daha Ã¶nce eÄŸitilmiÅŸ bir modeli (model kayÄ±t defterinden) yÃ¼kleyip yeni bir veri setiyle **ince ayar (fine-tuning)** yapabilme yeteneÄŸi kazandÄ±rmak.
-   **2. AÃ§Ä±klanabilir Yapay Zeka (XAI) Entegrasyonu:**
    -   Raporlara, modelin bir tahmini "neden" yaptÄ±ÄŸÄ±nÄ± basitÃ§e aÃ§Ä±klayan (Ã¶rn: SHAP, LIME entegrasyonu ile) bir bÃ¶lÃ¼m eklemek.
-   **3. Otomatik MLOps (AutoML-light):**
    -   Hiperparametre optimizasyonu sonuÃ§larÄ±nÄ± analiz edip, bir sonraki deney iÃ§in en olasÄ± parametre setini kullanÄ±cÄ±ya Ã¶neren bir "AkÄ±llÄ± Asistan" Ã¶zelliÄŸi geliÅŸtirmek.
-   **4. Ã‡oklu-Modalite (Multi-Modality):**
    -   Tek bir pipeline'Ä±n hem metin hem de gÃ¶rÃ¼ntÃ¼ gibi farklÄ± tÃ¼rde girdileri aynÄ± anda alarak tahmin yapabildiÄŸi mimarileri desteklemek.
========== FILE: docs/ROADMAP.md ==========
# ğŸ—ºï¸ AzuraForge Stratejik Yol HaritasÄ±

Bu belge, AzuraForge platformunun stratejik hedeflerini, tamamlanan kilometre taÅŸlarÄ±nÄ± ve gelecekteki geliÅŸtirme fazlarÄ±nÄ± Ã¶zetlemektedir. Bu yol haritasÄ±, projenin nereye gittiÄŸini gÃ¶steren canlÄ± bir dokÃ¼mandÄ±r.

**Daha detaylÄ± proje geÃ§miÅŸi ve evrimi iÃ§in [Proje YolculuÄŸu](./PROJECT_JOURNEY.md) belgesini inceleyebilirsiniz.**

---

### **ğŸ“ MEVCUT DURUM: Faz 0 - "Checkpoint Echo" (Fonksiyonel MVP)**

Platform, temel MLOps yeteneklerine sahip, Ã§alÄ±ÅŸan ve kararlÄ± bir MVP (Minimum Viable Product) aÅŸamasÄ±ndadÄ±r.

*   **Tamamlananlar:**
    *   SÄ±fÄ±rdan inÅŸa edilmiÅŸ `core` ve `learner` motorlarÄ±.
    *   Olay gÃ¼dÃ¼mlÃ¼ mimari ile asenkron gÃ¶rev iÅŸleme.
    *   `docker-compose` ile tam orkestrasyon.
    *   CanlÄ± deney takibi (WebSocket & Redis Pub/Sub).
    *   Dinamik ve interaktif raporlama arayÃ¼zÃ¼.
    *   GeniÅŸletilebilir eklenti sistemi (`entry_points`).

---

### **â¡ï¸ FAZ 1: TEMELÄ° SAÄLAMLAÅTIRMA (Foundation Hardening)**

*   **Hedef:** Projeyi Ã¼retim kalitesine, endÃ¼stri standardÄ± geliÅŸtirme pratiklerine ve yÃ¼ksek gÃ¼venilirliÄŸe taÅŸÄ±mak.
*   **Durum:** `ğŸŸ¢ Aktif`
*   **Ana BaÅŸlÄ±klar:**
    *   `[âœ”ï¸]` KapsamlÄ± DokÃ¼mantasyon (`VISION.md`, `ROADMAP.md`, `ARCHITECTURE.md`).
    *   `[â³]` Anlamsal Versiyonlama ve Git Etiketleme Stratejisi.
    *   `[â³]` Test KapsamÄ±nÄ±n ArtÄ±rÄ±lmasÄ± (Unit & Integration Tests).
    *   `[â³]` SÃ¼rekli Entegrasyon (CI) Pipeline'larÄ± Kurulumu (GitHub Actions).
    *   `[â¬œ]` Deney Verilerinin Dosya Sisteminden VeritabanÄ±na (PostgreSQL) TaÅŸÄ±nmasÄ±.

---

### **â¡ï¸ FAZ 2: DENEYÄ°MÄ° DERÄ°NLEÅTÄ°RME (MLOps Capability Expansion)**

*   **Hedef:** Platformu, temel bir araÃ§tan daha profesyonel ve gÃ¼Ã§lÃ¼ bir MLOps Ã§Ã¶zÃ¼mÃ¼ne dÃ¶nÃ¼ÅŸtÃ¼rmek.
*   **Durum:** `â¬œ PlanlanÄ±yor`
*   **Ana BaÅŸlÄ±klar:**
    *   `[â¬œ]` **Model KayÄ±t Defteri (Model Registry):** EÄŸitilen en iyi modellerin kalÄ±cÄ± olarak saklanmasÄ± ve yÃ¶netilmesi.
    *   `[â¬œ]` **Model Sunumu (Model Serving):** KayÄ±tlÄ± modeller Ã¼zerinden tahmin yapmak iÃ§in API endpoint'leri (`/models/{id}/predict`).
    *   `[â¬œ]` **Hiperparametre Optimizasyonu:** `Dashboard` Ã¼zerinden optimizasyon gÃ¶revleri baÅŸlatma ve sonuÃ§larÄ± gÃ¶rselleÅŸtirme.
    *   `[â¬œ]` **Kimlik DoÄŸrulama ve Yetkilendirme:** Ã‡ok kullanÄ±cÄ±lÄ± ortamlar iÃ§in gÃ¼venlik katmanÄ± (JWT).
    *   `[â¬œ]` **GPU DesteÄŸi:** Uyumlu donanÄ±mlarda `CuPy` ile eÄŸitimi hÄ±zlandÄ±rma.

---

### **â¡ï¸ FAZ 3: EVRENÄ° GENÄ°ÅLETME (New Data Modalities)**

*   **Hedef:** Platformun yeteneklerini zaman serilerinin Ã¶tesine taÅŸÄ±yarak farklÄ± veri tÃ¼rleri ile Ã§alÄ±ÅŸabildiÄŸini kanÄ±tlamak.
*   **Durum:** `â¬œ PlanlanÄ±yor`
*   **Ana BaÅŸlÄ±klar:**
    *   `[â¬œ]` **GÃ¶rÃ¼ntÃ¼ Ä°ÅŸleme DesteÄŸi:** `Conv2D`, `MaxPool2D` katmanlarÄ± ve `ImageClassificationPipeline` eklentisi.
    *   `[â¬œ]` **DoÄŸal Dil Ä°ÅŸleme Temelleri:** `Embedding`, `Attention` katmanlarÄ± ve metin sÄ±nÄ±flandÄ±rma pipeline'Ä±.
    *   `[â¬œ]` **AÃ§Ä±klanabilir Yapay Zeka (XAI):** Raporlara, modelin tahminlerini "neden" yaptÄ±ÄŸÄ±nÄ± aÃ§Ä±klayan SHAP/LIME gibi gÃ¶rseller eklemek.

---
`[âœ”ï¸] TamamlandÄ±` `[ğŸŸ¢ Aktif]` `[â³ Devam Ediyor]` `[â¬œ PlanlanÄ±yor]`

========== FILE: docs/VISION.md ==========
# ğŸ“œ AzuraForge Vizyonu ve Felsefesi

AzuraForge, sadece bir yazÄ±lÄ±m projesi deÄŸil, modern yapay zeka sistemlerinin nasÄ±l inÅŸa edilmesi ve anlaÅŸÄ±lmasÄ± gerektiÄŸine dair bir manifestodur. Vizyonumuz, **ÅŸeffaf, modÃ¼ler ve derinlemesine anlaÅŸÄ±lmÄ±ÅŸ bir AI motorunu, saÄŸlam ve Ã¶lÃ§eklenebilir bir MLOps ÅŸasisi Ã¼zerine yerleÅŸtirerek**, hem Ã¶ÄŸrenme aracÄ± hem de gÃ¼Ã§lÃ¼ bir Ã¼retim platformu olarak hizmet edebilen bir ekosistem yaratmaktÄ±r.

## ğŸ¯ Temel Felsefe: Ferrari Motoru ve Uzay Gemisi Åasisi

Bu vizyonu, basit bir metaforla Ã¶zetliyoruz:

*   **Ferrari Motoru:** Platformun kalbindeki (`core`, `learner`) AI motoru, kanÄ±tlanmÄ±ÅŸ temel algoritmalardan oluÅŸur. Bu motor, dÄ±ÅŸ kÃ¼tÃ¼phanelere minimal baÄŸÄ±mlÄ±lÄ±kla, temel prensipleri anlaÅŸÄ±larak sÄ±fÄ±rdan inÅŸa edilmiÅŸtir. Bu bize tam kontrol, esneklik, ÅŸeffaflÄ±k ve en Ã¶nemlisi, bir "kara kutu" ile Ã§alÄ±ÅŸmak yerine sistemin ruhunu anlama imkanÄ± verir.

*   **Uzay Gemisi Åasisi:** Bu saf gÃ¼Ã§, modern mÃ¼hendislik pratikleriyle tasarlanmÄ±ÅŸ, daÄŸÄ±tÄ±k ve Ã¶lÃ§eklenebilir bir MLOps mimarisiyle sunulur. Bu ÅŸasi, Ferrari motorunun gÃ¼cÃ¼nÃ¼ gÃ¼venli, verimli ve yÃ¶netilebilir bir ÅŸekilde kullanÄ±labilir hale getirir.

## â­ "The AzuraForge Way": DÃ¶rt Anayasal Prensip

Platforma yapÄ±lan her katkÄ± ve alÄ±nan her karar, bu dÃ¶rt temel prensibe uygun olmalÄ±dÄ±r.

1.  **Ã–nce Kalite, Sonra HÄ±z (Quality First, Velocity Second):**
    HÄ±zlÄ± prototipleme dÃ¶nemi baÅŸarÄ±yla tamamlanmÄ±ÅŸtÄ±r. ArtÄ±k her yeni Ã¶zellik, testlerle gÃ¼vence altÄ±na alÄ±nmÄ±ÅŸ, dokÃ¼mante edilmiÅŸ ve standartlara uygun olmalÄ±dÄ±r. SÃ¼rdÃ¼rÃ¼lebilir hÄ±z, ancak saÄŸlam bir temel Ã¼zerine inÅŸa edilebilir.

2.  **ÅeffaflÄ±k ve Sahiplenme (Transparency and Ownership):**
    Kod "kara kutu" olamaz. AlÄ±nan Ã¶nemli mimari kararlarÄ±n "nedenleri" (`ARCHITECTURE.md` gibi belgelerle) aÃ§Ä±kÃ§a belgelenir. Her bileÅŸenin (`api`, `worker` vb.) net bir sorumluluÄŸu ve amacÄ± vardÄ±r.

3.  **Pragmatik MÃ¼kemmeliyetÃ§ilik (Pragmatic Perfectionism):**
    En iyi mÃ¼hendislik pratiklerini (olay gÃ¼dÃ¼mlÃ¼ mimari, eklenti sistemi, CI/CD) hedefleriz, ancak bunlarÄ± projenin mevcut ihtiyaÃ§larÄ±na ve hedeflerine hizmet edecek ÅŸekilde pragmatik bir yaklaÅŸÄ±mla uygularÄ±z. MÃ¼kemmellik, karmaÅŸÄ±klÄ±k demek deÄŸildir.

4.  **KullanÄ±cÄ± OdaklÄ± DeÄŸer (User-Centric Value):**
    GeliÅŸtirdiÄŸimiz her Ã¶zellik, son kullanÄ±cÄ±ya (bu durumda platformu kullanan AI geliÅŸtiricisi/araÅŸtÄ±rmacÄ±sÄ±) somut bir deÄŸer katmalÄ±dÄ±r. CanlÄ± deney takibi, interaktif raporlama ve deney karÅŸÄ±laÅŸtÄ±rma gibi Ã¶zellikler bu prensibin en gÃ¼zel Ã¶rnekleridir.

Bu vizyon ve prensipler, AzuraForge'un gelecekteki geliÅŸimine rehberlik edecek olan kutup yÄ±ldÄ±zÄ±mÄ±zdÄ±r.

========== FILE: learner/Dockerfile ==========
# ========== GÃœNCELLEME: learner/Dockerfile ==========
# Stage 1: Builder
FROM python:3.10-slim-bullseye AS builder

RUN apt-get update && apt-get install -y git --no-install-recommends && rm -rf /var/lib/apt/lists/*

# KRÄ°TÄ°K DÃœZELTME: Ã‡alÄ±ÅŸma dizinini doÄŸrudan 'src' klasÃ¶rÃ¼nÃ¼n iÃ§ine ayarla
WORKDIR /app/src 

# src klasÃ¶rÃ¼nÃ¼n iÃ§eriÄŸini (yani azuraforge_learner klasÃ¶rÃ¼nÃ¼) mevcut WORKDIR'e kopyala
COPY src ./src

# pyproject.toml ve setup.py'Ä± bir Ã¼st dizine (/app) kopyala, 
# Ã§Ã¼nkÃ¼ pip install oradan Ã§alÄ±ÅŸacak.
COPY pyproject.toml /app/
COPY setup.py /app/

# pip install komutunu ana paketin kÃ¶k dizininden (/app) Ã§alÄ±ÅŸtÄ±r.
# Bu, setup.py'Ä±n package_dir={"": "src"} ayarÄ±nÄ± doÄŸru algÄ±lamasÄ±nÄ± saÄŸlar.
RUN --mount=type=cache,target=/root/.cache/pip pip install --no-cache-dir /app

# Stage 2: Runtime
FROM python:3.10-slim-bullseye AS runtime

# Runtime'da da aynÄ± Ã§alÄ±ÅŸma dizinini koru
WORKDIR /app/src

# Builder aÅŸamasÄ±ndan kurulu paketi kopyala
COPY --from=builder /usr/local/lib/python3.10/site-packages /usr/local/lib/python3.10/site-packages
# src klasÃ¶rÃ¼nÃ¼n iÃ§eriÄŸini kopyala
COPY --from=builder /app/src ./src

CMD ["python", "-c", "print('AzuraForge Learner library image built successfully!')"]
========== FILE: learner/pyproject.toml ==========
# learner/pyproject.toml

[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "azuraforge-learner"
version = "0.1.5"
authors = [{ name = "Azmi Sahin" }]
description = "High-level deep learning library for model training and management, using the AzuraForge Core engine."
readme = "README.md"
requires-python = ">=3.8"
license = { text = "MIT" }
dependencies = [
    "azuraforge-core @ git+https://github.com/AzuraForge/core.git@v0.1.3",
    "scikit-learn",
    "numpy",
    "redis",
    "matplotlib",
    "pandas",
    "pyarrow"
]

[project.optional-dependencies]
dev = ["pytest"]
========== FILE: learner/README.md ==========
# AzuraForge Learner ğŸ§ 

**AzuraForge Learner**, `azuraforge-core` motorunu kullanarak modelleri kolayca oluÅŸturmak, eÄŸitmek ve yÃ¶netmek iÃ§in tasarlanmÄ±ÅŸ yÃ¼ksek seviyeli bir kÃ¼tÃ¼phanedir.

## Kurulum

```bash
pip install azuraforge-learner@git+https://github.com/AzuraForge/learner.git
```

========== FILE: learner/setup.py ==========
from setuptools import setup, find_packages
setup(
    package_dir={"": "src"},
    packages=find_packages(where="src"),
)

========== FILE: learner/src/azuraforge_learner/caching.py ==========
# learner/src/azuraforge_learner/caching.py

import logging
import os
import hashlib
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, Optional
import pandas as pd

def get_cache_filepath(cache_dir: str, context: str, params: Dict[str, Any]) -> str:
    """
    Verilen parametrelere gÃ¶re deterministik bir Ã¶nbellek dosya yolu oluÅŸturur.
    Dosya adÄ±, parametrelerin sÄ±ralÄ± bir karmasÄ±ndan (hash) tÃ¼retilir.
    
    Args:
        cache_dir (str): Ã–nbellek dosyalarÄ±nÄ±n saklanacaÄŸÄ± ana dizin.
        context (str): Ã–nbelleÄŸin ait olduÄŸu baÄŸlam (Ã¶rn: 'stock_predictor').
        params (Dict[str, Any]): Dosya adÄ±nÄ± oluÅŸturmak iÃ§in kullanÄ±lacak parametreler.
        
    Returns:
        str: OluÅŸturulan tam dosya yolu.
    """
    # Parametreleri anahtarlarÄ±na gÃ¶re sÄ±ralayarak tutarlÄ± bir string oluÅŸtur
    param_str = str(sorted(params.items()))
    # Bu string'in hash'ini alarak benzersiz ve dosya sistemi iÃ§in gÃ¼venli bir kimlik oluÅŸtur
    param_hash = hashlib.md5(param_str.encode()).hexdigest()
    filename = f"{context}_{param_hash}.parquet"
    
    # BaÄŸlama Ã¶zel bir alt klasÃ¶r oluÅŸturarak karÄ±ÅŸÄ±klÄ±ÄŸÄ± Ã¶nle
    full_cache_dir = os.path.join(cache_dir, context)
    os.makedirs(full_cache_dir, exist_ok=True)
    
    return os.path.join(full_cache_dir, filename)

def load_from_cache(filepath: str, max_age_hours: int) -> Optional[pd.DataFrame]:
    """
    Veriyi Ã¶nbellekten yÃ¼kler. EÄŸer dosya yoksa veya belirtilen sÃ¼reden eskiyse
    None dÃ¶ner.
    
    Args:
        filepath (str): Ã–nbellek dosyasÄ±nÄ±n yolu.
        max_age_hours (int): Ã–nbelleÄŸin saat cinsinden maksimum geÃ§erlilik sÃ¼resi.
        
    Returns:
        Optional[pd.DataFrame]: GeÃ§erli Ã¶nbellek verisi varsa DataFrame, yoksa None.
    """
    if not os.path.exists(filepath):
        return None
        
    try:
        # DosyanÄ±n son deÄŸiÅŸtirilme zamanÄ±nÄ± al (UTC olarak)
        mod_time = datetime.fromtimestamp(os.path.getmtime(filepath), tz=timezone.utc)
        # EÄŸer dosyanÄ±n yaÅŸÄ±, izin verilen maksimum yaÅŸtan kÃ¼Ã§Ã¼kse, geÃ§erlidir
        if (datetime.now(timezone.utc) - mod_time) < timedelta(hours=max_age_hours):
            logging.info(f"GeÃ§erli Ã¶nbellek bulundu, buradan okunuyor: {filepath}")
            return pd.read_parquet(filepath)
        else:
            logging.info(f"Ã–nbellek sÃ¼resi dolmuÅŸ: {filepath}")
            os.remove(filepath) # SÃ¼resi dolmuÅŸ dosyayÄ± temizle
            return None
    except Exception as e:
        logging.error(f"Ã–nbellekten okuma hatasÄ± {filepath}: {e}")
        return None

def save_to_cache(df: pd.DataFrame, filepath: str) -> None:
    """
    Verilen DataFrame'i belirtilen yola Parquet formatÄ±nda kaydeder.
    
    Args:
        df (pd.DataFrame): Kaydedilecek veri.
        filepath (str): Kaydedilecek dosyanÄ±n tam yolu.
    """
    try:
        # DosyanÄ±n kaydedileceÄŸi dizinin var olduÄŸundan emin ol
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        df.to_parquet(filepath)
        logging.info(f"Veri Ã¶nbelleÄŸe kaydedildi: {filepath}")
    except Exception as e:
        logging.error(f"Ã–nbelleÄŸe yazma hatasÄ± {filepath}: {e}")
========== FILE: learner/src/azuraforge_learner/callbacks.py ==========
# learner/src/azuraforge_learner/callbacks.py

import os
import numpy as np
from typing import TYPE_CHECKING, Optional, Any
from .events import Event # Event'i de import edelim

# DÃ¶ngÃ¼sel importu Ã¶nlemek iÃ§in, sadece tip kontrolÃ¼ sÄ±rasÄ±nda Learner'Ä± import et
if TYPE_CHECKING:
    from .learner import Learner

class Callback:
    """
    TÃ¼m callback'lerin temel sÄ±nÄ±fÄ±.
    Kendisini Ã§alÄ±ÅŸtÄ±ran Learner'a bir referans tutar.
    """
    def __init__(self):
        self.learner: Optional['Learner'] = None

    def set_learner(self, learner: 'Learner'):
        """Bu metod, Learner tarafÄ±ndan Ã§aÄŸrÄ±larak referansÄ± ayarlar."""
        self.learner = learner

    def __call__(self, event: Event):
        """
        Gelen olaya gÃ¶re ilgili metodu (Ã¶rn: on_epoch_end) Ã§aÄŸÄ±rÄ±r.
        """
        method = getattr(self, f"on_{event.name}", None)
        if method:
            method(event)

    # Olay metotlarÄ±
    def on_train_begin(self, event: Event) -> None: pass
    def on_train_end(self, event: Event) -> None: pass
    def on_epoch_begin(self, event: Event) -> None: pass
    def on_epoch_end(self, event: Event) -> None: pass
    def on_batch_begin(self, event: Event) -> None: pass
    def on_batch_end(self, event: Event) -> None: pass


# Ã–NEMLÄ°: ModelCheckpoint ve EarlyStopping sÄ±nÄ±flarÄ±nÄ± koruyoruz ve
# yeni temel sÄ±nÄ±ftan miras almalarÄ±nÄ± saÄŸlÄ±yoruz.
class ModelCheckpoint(Callback):
    """Her epoch sonunda performansÄ± izler ve sadece en iyi modeli kaydeder."""
    def __init__(self, filepath: str, monitor: str = "val_loss", mode: str = "min", verbose: int = 1):
        super().__init__() # Temel sÄ±nÄ±fÄ±n init'ini Ã§aÄŸÄ±r
        self.filepath = filepath
        self.monitor = monitor
        self.mode = mode
        self.verbose = verbose
        self.best = np.inf if mode == "min" else -np.inf

        dir_path = os.path.dirname(self.filepath)
        if dir_path:
            os.makedirs(dir_path, exist_ok=True)

    def on_epoch_end(self, event: Event):
        current_val = event.payload.get(self.monitor)
        if current_val is None:
            if event.payload.get("epoch") == 0 and self.verbose > 0:
                print(f"ModelCheckpoint Warning: Can't find metric '{self.monitor}' to save model.")
            return

        is_better = (self.mode == "min" and current_val < self.best) or \
                    (self.mode == "max" and current_val > self.best)

        if is_better:
            if self.verbose > 0:
                print(f"ModelCheckpoint: {self.monitor} improved from {self.best:.6f} to {current_val:.6f}. Saving model...")
            self.best = current_val
            if self.learner and hasattr(self.learner, 'save_model'): # Learner'da save_model metodu varsa
                 self.learner.save_model(self.filepath)


class EarlyStopping(Callback):
    """Performans belirli bir epoch sayÄ±sÄ± boyunca iyileÅŸmediÄŸinde eÄŸitimi durdurur."""
    def __init__(self, monitor: str = "val_loss", patience: int = 10, mode: str = "min", verbose: int = 1):
        super().__init__() # Temel sÄ±nÄ±fÄ±n init'ini Ã§aÄŸÄ±r
        self.monitor = monitor
        self.patience = patience
        self.mode = mode
        self.verbose = verbose
        self.wait = 0
        self.best = np.inf if mode == "min" else -np.inf

    def on_train_begin(self, event: Event):
        self.wait = 0
        self.best = np.inf if self.mode == "min" else -np.inf

    def on_epoch_end(self, event: Event):
        current_val = event.payload.get(self.monitor)
        if current_val is None:
            return
            
        is_better = (self.mode == "min" and current_val < self.best) or \
                    (self.mode == "max" and current_val > self.best)

        if is_better:
            self.best = current_val
            self.wait = 0
        else:
            self.wait += 1
            if self.wait >= self.patience:
                if self.verbose > 0:
                    print(f"EarlyStopping: Stopping training. {self.monitor} did not improve for {self.patience} epochs.")
                if self.learner:
                    self.learner.stop_training = True
========== FILE: learner/src/azuraforge_learner/events.py ==========
from dataclasses import dataclass, field
from typing import Dict, Any, Literal, TYPE_CHECKING

if TYPE_CHECKING:
    from .learner import Learner

EventName = Literal["train_begin", "train_end", "epoch_begin", "epoch_end"]

@dataclass
class Event:
    name: EventName
    learner: 'Learner'
    payload: Dict[str, Any] = field(default_factory=dict)

========== FILE: learner/src/azuraforge_learner/layers.py ==========
from typing import List, Tuple, Optional
import numpy as np
from azuraforge_core import Tensor, xp, ArrayType

class Layer:
    def forward(self, x: Tensor) -> Tensor: raise NotImplementedError
    def parameters(self) -> List[Tensor]: return []
    def __call__(self, x: Tensor) -> Tensor: return self.forward(x)

class Linear(Layer):
    def __init__(self, input_dim: int, output_dim: int):
        limit = np.sqrt(2.0 / input_dim)
        self.weights = Tensor(xp.random.randn(input_dim, output_dim) * limit, requires_grad=True)
        self.bias = Tensor(xp.zeros(output_dim), requires_grad=True)
    def forward(self, x: Tensor) -> Tensor:
        return x.dot(self.weights) + self.bias
    def parameters(self) -> List[Tensor]:
        return [self.weights, self.bias]

class ReLU(Layer):
    def forward(self, x: Tensor) -> Tensor:
        return x.relu()

class Sigmoid(Layer):
    def forward(self, x: Tensor) -> Tensor:
        return x.sigmoid()

# DÃœZELTME: LSTM katmanÄ± tam backward pass ile yeniden yazÄ±ldÄ±.
class LSTM(Layer):
    def __init__(self, input_size: int, hidden_size: int):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        H = hidden_size
        D = input_size
        
        limit = np.sqrt(1.0 / H)
        self.W_x = Tensor(xp.random.randn(D, H * 4) * limit, requires_grad=True)
        self.W_h = Tensor(xp.random.randn(H, H * 4) * limit, requires_grad=True)
        self.b = Tensor(xp.zeros(H * 4), requires_grad=True)
        
        self.cache: Optional[Tuple] = None

    def parameters(self) -> List[Tensor]:
        return [self.W_x, self.W_h, self.b]

    def forward(self, x: Tensor) -> Tensor:
        N, T, D = x.data.shape
        H = self.hidden_size

        h_prev = xp.zeros((N, H))
        c_prev = xp.zeros((N, H))
        
        h_all = xp.zeros((N, T, H))
        c_all = xp.zeros((N, T, H))
        gates_all = xp.zeros((N, T, 4 * H))
        i_all = xp.zeros((N, T, H))
        f_all = xp.zeros((N, T, H))
        o_all = xp.zeros((N, T, H))
        g_all = xp.zeros((N, T, H))

        for t in range(T):
            x_t = x.data[:, t, :]
            gates = x_t @ self.W_x.data + h_prev @ self.W_h.data + self.b.data
            
            i = 1 / (1 + xp.exp(-gates[:, :H]))
            f = 1 / (1 + xp.exp(-gates[:, H:2*H]))
            o = 1 / (1 + xp.exp(-gates[:, 2*H:3*H]))
            g = xp.tanh(gates[:, 3*H:])
            
            c_next = f * c_prev + i * g
            h_next = o * xp.tanh(c_next)

            h_prev, c_prev = h_next, c_next
            
            h_all[:, t, :] = h_next
            c_all[:, t, :] = c_next
            gates_all[:, t, :] = gates
            i_all[:, t, :] = i
            f_all[:, t, :] = f
            o_all[:, t, :] = o
            g_all[:, t, :] = g

        # Ã‡Ä±ktÄ± olarak tÃ¼m zaman adÄ±mlarÄ±ndaki gizli durumlarÄ± dÃ¶ndÃ¼r
        out = Tensor(h_all, _children=(x, self.W_x, self.W_h, self.b), _op="lstm", requires_grad=x.requires_grad)
        
        # Geriye yayÄ±lÄ±m iÃ§in gerekli tÃ¼m ara deÄŸerleri sakla
        self.cache = (x.data, h_all, c_all, i_all, f_all, o_all, g_all)

        def _backward():
            if not out.requires_grad or out.grad is None: return
            assert self.cache is not None, "Cache is not set"
            
            x_data, h_data, c_data, i_data, f_data, o_data, g_data = self.cache
            _N, _T, _D = x_data.shape
            _H = self.hidden_size
            
            # BaÅŸlangÄ±Ã§ gradyanlarÄ±
            dx = xp.zeros_like(x_data)
            dW_x = xp.zeros_like(self.W_x.data)
            dW_h = xp.zeros_like(self.W_h.data)
            db = xp.zeros_like(self.b.data)
            
            dh_next = xp.zeros((_N, _H))
            dc_next = xp.zeros((_N, _H))

            for t in reversed(range(_T)):
                dh = out.grad[:, t, :] + dh_next
                
                # Geriye yayÄ±lÄ±m adÄ±mlarÄ±
                dc = dc_next + dh * o_data[:, t, :] * (1 - xp.tanh(c_data[:, t, :])**2)
                
                di = dc * g_data[:, t, :]
                df = dc * (c_data[:, t-1, :] if t > 0 else 0)
                do = dh * xp.tanh(c_data[:, t, :])
                dg = dc * i_data[:, t, :]
                
                d_gates_i = di * i_data[:, t, :] * (1 - i_data[:, t, :])
                d_gates_f = df * f_data[:, t, :] * (1 - f_data[:, t, :])
                d_gates_o = do * o_data[:, t, :] * (1 - o_data[:, t, :])
                d_gates_g = dg * (1 - g_data[:, t, :]**2)
                
                dgates = xp.concatenate((d_gates_i, d_gates_f, d_gates_o, d_gates_g), axis=1)

                # GradyanlarÄ± biriktir
                x_t = x_data[:, t, :]
                h_prev = h_data[:, t-1, :] if t > 0 else xp.zeros((_N, _H))
                
                dx[:, t, :] = dgates @ self.W_x.data.T
                dh_next = dgates @ self.W_h.data.T
                dc_next = dc * f_data[:, t, :]
                
                dW_x += x_t.T @ dgates
                dW_h += h_prev.T @ dgates
                db += xp.sum(dgates, axis=0)

            # Hesaplanan gradyanlarÄ± tensÃ¶rlere ata
            if x.requires_grad and x.grad is not None: x.grad += dx
            if self.W_x.requires_grad and self.W_x.grad is not None: self.W_x.grad += dW_x
            if self.W_h.requires_grad and self.W_h.grad is not None: self.W_h.grad += dW_h
            if self.b.requires_grad and self.b.grad is not None: self.b.grad += db

        out._backward = _backward
        # Sadece son gizli durumu dÃ¶ndÃ¼rerek uyumluluÄŸu koru
        return Tensor(h_all[:, -1, :], _children=(out,), _op="lstm_last_step")


    def forward_old(self, x: Tensor) -> Tensor:
        # Eski forward metodu referans iÃ§in burada bÄ±rakÄ±labilir
        # ...
        pass
========== FILE: learner/src/azuraforge_learner/learner.py ==========
# learner/src/azuraforge_learner/learner.py

import time
from typing import Any, Dict, List, Optional
import numpy as np

from azuraforge_core import Tensor
from .events import Event
from .models import Sequential
from .losses import Loss
from .optimizers import Optimizer
from .callbacks import Callback

class Learner:
    def __init__(self, model: Sequential, criterion: Loss, optimizer: Optimizer, callbacks: Optional[List[Callback]] = None):
        self.model = model
        self.criterion = criterion
        self.optimizer = optimizer
        self.callbacks = callbacks or []
        
        # KRÄ°TÄ°K DÃœZELTME: TÃ¼m callback'lere bu learner Ã¶rneÄŸini tanÄ±t.
        # Bu, callback'lerin `self.learner` Ã¼zerinden `predict` gibi metotlara eriÅŸmesini saÄŸlar.
        for cb in self.callbacks:
            cb.set_learner(self)
                 
        self.history: Dict[str, List[float]] = {}
        self.stop_training: bool = False

    def _publish(self, event_name: str, payload: Optional[Dict[str, Any]] = None):
        """OlayÄ± tÃ¼m callback'lere yayÄ±nlar."""
        event = Event(name=event_name, learner=self, payload=payload or {})
        for cb in self.callbacks:
            cb(event)

    def fit(self, X_train: np.ndarray, y_train: np.ndarray, epochs: int, pipeline_name: str = "Bilinmiyor"):
        self.history = {"loss": []}
        X_train_t, y_train_t = Tensor(X_train), Tensor(y_train)
        
        self._publish("train_begin", payload={"total_epochs": epochs, "status_text": "EÄŸitim baÅŸlÄ±yor...", "pipeline_name": pipeline_name})
        
        for epoch in range(epochs):
            if self.stop_training:
                break
            
            self._publish("epoch_begin", payload={"epoch": epoch, "total_epochs": epochs, "pipeline_name": pipeline_name})
            
            y_pred = self.model(X_train_t)
            loss = self.criterion(y_pred, y_train_t)
            
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
            
            current_loss = loss.to_cpu().item() if hasattr(loss, 'to_cpu') else float(loss.data)
            
            epoch_logs = {
                "epoch": epoch + 1, "total_epochs": epochs, "loss": current_loss,
                "status_text": f"Epoch {epoch + 1}/{epochs} tamamlandÄ±, KayÄ±p: {current_loss:.6f}",
                "pipeline_name": pipeline_name
            }
            
            self.history["loss"].append(current_loss)
            self._publish("epoch_end", payload=epoch_logs)
            
        self._publish("train_end", payload={"status_text": "EÄŸitim tamamlandÄ±.", "pipeline_name": pipeline_name})
        return self.history
        
    def predict(self, X_test: np.ndarray) -> np.ndarray:
        if not isinstance(X_test, np.ndarray):
            raise TypeError("Girdi (X_test) bir NumPy dizisi olmalÄ±dÄ±r.")
        
        input_tensor = Tensor(X_test)
        predictions_tensor = self.model(input_tensor)
        return predictions_tensor.to_cpu()

    def evaluate(self, X_val: np.ndarray, y_val: np.ndarray) -> Dict[str, float]:
        from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
        
        y_val_t = Tensor(y_val)
        y_pred_t = self.model(Tensor(X_val))
        
        val_loss = self.criterion(y_pred_t, y_val_t).to_cpu().item()
        y_pred_np = y_pred_t.to_cpu()
        
        y_val_np = y_val if isinstance(y_val, np.ndarray) else np.array(y_val)
        
        val_r2 = r2_score(y_val_np, y_pred_np)
        val_mae = mean_absolute_error(y_val_np, y_pred_np)
        val_rmse = np.sqrt(mean_squared_error(y_val_np, y_pred_np))

        return {"val_loss": val_loss, "val_r2": val_r2, "val_mae": val_mae, "val_rmse": val_rmse}
========== FILE: learner/src/azuraforge_learner/losses.py ==========
from azuraforge_core import Tensor

class Loss:
    def __call__(self, y_pred: Tensor, y_true: Tensor) -> Tensor: raise NotImplementedError

class MSELoss(Loss):
    def __call__(self, y_pred: Tensor, y_true: Tensor) -> Tensor:
        return ((y_pred - y_true) ** 2).mean()

========== FILE: learner/src/azuraforge_learner/models.py ==========
from typing import List
from .layers import Layer
from azuraforge_core import Tensor

class Sequential(Layer):
    def __init__(self, *layers: Layer):
        self.layers = list(layers)
    def forward(self, x: Tensor) -> Tensor:
        for layer in self.layers:
            x = layer(x)
        return x
    def parameters(self) -> List[Tensor]:
        return [p for layer in self.layers for p in layer.parameters()]

========== FILE: learner/src/azuraforge_learner/optimizers.py ==========
from typing import List
from azuraforge_core import Tensor

class Optimizer:
    def __init__(self, params: List[Tensor], lr: float):
        self.params = [p for p in params if p.requires_grad]
        self.lr = lr
    def step(self) -> None: raise NotImplementedError
    def zero_grad(self) -> None:
        for p in self.params:
            if p.grad is not None: p.grad.fill(0.0)

class SGD(Optimizer):
    def step(self) -> None:
        for p in self.params:
            if p.grad is not None: p.data -= self.lr * p.grad

# YENÄ°: Adam Optimizer eklendi
class Adam(Optimizer):
    def __init__(self, params: List[Tensor], lr: float = 0.001, beta1: float = 0.9, beta2: float = 0.999, epsilon: float = 1e-8):
        super().__init__(params, lr)
        from azuraforge_core import xp # xp'yi burada import ediyoruz
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.m = {id(p): xp.zeros_like(p.data) for p in self.params}
        self.v = {id(p): xp.zeros_like(p.data) for p in self.params}
        self.t = 0

    def step(self) -> None:
        self.t += 1
        from azuraforge_core import xp # xp'yi burada import ediyoruz
        for p in self.params:
            if p.grad is not None:
                param_id = id(p)
                self.m[param_id] = self.beta1 * self.m[param_id] + (1 - self.beta1) * p.grad
                self.v[param_id] = self.beta2 * self.v[param_id] + (1 - self.beta2) * (p.grad**2)

                m_hat = self.m[param_id] / (1 - self.beta1**self.t)
                v_hat = self.v[param_id] / (1 - self.beta2**self.t)

                update_val = self.lr * m_hat / (xp.sqrt(v_hat) + self.epsilon)
                p.data -= update_val
========== FILE: learner/src/azuraforge_learner/pipelines.py ==========
# learner/src/azuraforge_learner/pipelines.py

import logging
import os
from abc import ABC, abstractmethod
from typing import Dict, Any, Tuple, Optional, List

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

from .learner import Learner, Callback
from .models import Sequential
from .reporting import generate_regression_report
from .optimizers import Adam, SGD
from .losses import MSELoss
from .events import Event
from .caching import get_cache_filepath, load_from_cache, save_to_cache

def _create_sequences(data: np.ndarray, seq_length: int) -> Tuple[np.ndarray, np.ndarray]:
    xs, ys = [], []
    for i in range(len(data) - seq_length):
        x = data[i:(i + seq_length)]
        y = data[i + seq_length]
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys).reshape(-1, data.shape[1])

class BasePipeline(ABC):
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)

    @abstractmethod
    def run(self, callbacks: Optional[List[Callback]] = None) -> Dict[str, Any]:
        pass

class LivePredictionCallback(Callback):
    def __init__(self, pipeline: 'TimeSeriesPipeline', X_val: np.ndarray, y_val: np.ndarray, time_index_val: pd.Index):
        super().__init__()
        self.pipeline = pipeline
        self.X_val = X_val
        self.y_val = y_val
        self.time_index_val = time_index_val
        # DÃ¼zeltme: validate_every config'den gelmeli, yoksa varsayÄ±lan 5.
        self.validate_every = self.pipeline.config.get("training_params", {}).get("validate_every", 5)
        self.last_results: Dict[str, Any] = {}

    def on_epoch_end(self, event: Event) -> None:
        epoch = event.payload.get("epoch", 0)
        total_epochs = event.payload.get("total_epochs", 1)

        # BURADAKÄ° DÃœZELTME: validate_every kontrolÃ¼.
        # EÄŸer validate_every > 0 ise, sadece o aralÄ±klarda ve son epoch'ta gÃ¶nder.
        # EÄŸer validate_every <= 0 ise, her epoch'ta gÃ¶nder (devre dÄ±ÅŸÄ± bÄ±rakma gibi).
        should_validate_and_send = False
        if self.validate_every > 0:
            if (epoch % self.validate_every == 0 and epoch > 0) or \
               (epoch == total_epochs and total_epochs > 0): # Son epoch ise her zaman gÃ¶nder
                should_validate_and_send = True
        else: # validate_every 0 veya negatifse, her epoch'ta gÃ¶nder
            should_validate_and_send = True

        if should_validate_and_send:
            if not self.learner: return

            y_pred_scaled = self.learner.predict(self.X_val)
            
            y_test_unscaled, y_pred_unscaled = self.pipeline._inverse_transform_all(
                self.y_val, y_pred_scaled
            )
            
            # validation_data'yÄ± payload'a ekliyoruz.
            event.payload['validation_data'] = {
                "x_axis": [d.isoformat() for d in self.time_index_val],
                "y_true": y_test_unscaled.tolist(), 
                "y_pred": y_pred_unscaled.tolist(), 
                "x_label": "Tarih", 
                "y_label": self.pipeline._get_target_and_feature_cols()[0]
            }
            
            # Metrikleri hesapla ve sakla (API'ye nihai results olarak gitmek iÃ§in)
            from sklearn.metrics import r2_score, mean_absolute_error
            self.last_results = {
                "history": self.learner.history,
                "metrics": {
                    'r2_score': float(r2_score(y_test_unscaled, y_pred_unscaled)), 
                    'mae': float(mean_absolute_error(y_test_unscaled, y_pred_unscaled))
                },
                "final_loss": event.payload.get("loss"), 
                "y_true": y_test_unscaled.tolist(),
                "y_pred": y_pred_unscaled.tolist(),
                "time_index": [d.isoformat() for d in self.time_index_val],
                "y_label": self.pipeline._get_target_and_feature_cols()[0]
            }

class TimeSeriesPipeline(BasePipeline):
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.scaler = MinMaxScaler(feature_range=(-1, 1))
        self.feature_scaler = MinMaxScaler(feature_range=(-1, 1))
        self.learner: Optional[Learner] = None
        self.X_train: Optional[np.ndarray] = None
        self.y_train: Optional[np.ndarray] = None
        self.X_test: Optional[np.ndarray] = None
        self.y_test: Optional[np.ndarray] = None
        self.time_index_test: Optional[pd.Index] = None

    @abstractmethod
    def _load_data_from_source(self) -> pd.DataFrame:
        pass
        
    def get_caching_params(self) -> Dict[str, Any]:
        return self.config.get("data_sourcing", {})

    @abstractmethod
    def _get_target_and_feature_cols(self) -> Tuple[str, List[str]]:
        pass
    
    @abstractmethod
    def _create_model(self, input_shape: Tuple) -> Sequential:
        pass

    def _create_learner(self, model: Sequential, callbacks: Optional[List[Callback]]) -> Learner:
        training_params = self.config.get("training_params", {})
        lr = float(training_params.get("lr", 0.001))
        optimizer_type = str(training_params.get("optimizer", "adam")).lower()
        optimizer = Adam(model.parameters(), lr=lr) if optimizer_type == "adam" else SGD(model.parameters(), lr=lr)
        return Learner(model, MSELoss(), optimizer, callbacks=callbacks)

    def _inverse_transform_all(self, y_true_scaled, y_pred_scaled):
        y_true_unscaled_transformed = self.scaler.inverse_transform(y_true_scaled)
        y_pred_unscaled_transformed = self.scaler.inverse_transform(y_pred_scaled)

        target_transform = self.config.get("feature_engineering", {}).get("target_col_transform")
        if target_transform == 'log':
            self.logger.info(f"Target column will be exponentiated from log-transformed data.")
            y_true_final = np.expm1(y_true_unscaled_transformed)
            y_pred_final = np.expm1(y_pred_unscaled_transformed)
        else:
            y_true_final = y_true_unscaled_transformed
            y_pred_final = y_pred_unscaled_transformed
            
        return y_true_final.flatten(), y_pred_final.flatten()

    def run(self, callbacks: Optional[List[Callback]] = None) -> Dict[str, Any]:
        self.logger.info(f"'{self.config.get('pipeline_name')}' pipeline baÅŸlatÄ±lÄ±yor...")
        
        system_config = self.config.get("system", {})
        cache_enabled = system_config.get("caching_enabled", True)
        cache_dir = os.getenv("CACHE_DIR", ".cache")
        cache_max_age = system_config.get("cache_max_age_hours", 24)
        
        cache_params = self.get_caching_params()
        cache_filepath = get_cache_filepath(cache_dir, self.config.get('pipeline_name', 'default_context'), cache_params)

        raw_data = None
        if cache_enabled: raw_data = load_from_cache(cache_filepath, cache_max_age)
        if raw_data is None:
            self.logger.info("Ã–nbellek boÅŸ veya geÃ§ersiz. Veri kaynaktan Ã§ekiliyor...")
            raw_data = self._load_data_from_source()
            if cache_enabled and isinstance(raw_data, pd.DataFrame) and not raw_data.empty:
                save_to_cache(raw_data, cache_filepath)

        target_col, feature_cols = self._get_target_and_feature_cols()
        
        features_df = raw_data[feature_cols].copy()
        target_series = raw_data[target_col].copy()

        target_transform = self.config.get("feature_engineering", {}).get("target_col_transform")
        if target_transform == 'log':
            self.logger.info(f"'{target_col}' sÃ¼tununa log(1+x) dÃ¶nÃ¼ÅŸÃ¼mÃ¼ uygulanÄ±yor.")
            target_series = np.log1p(target_series)
        
        scaled_features = self.feature_scaler.fit_transform(features_df)
        scaled_target = self.scaler.fit_transform(target_series.values.reshape(-1, 1))
        
        scaled_data = np.concatenate([scaled_features, scaled_target], axis=1)

        sequence_length = self.config.get("model_params", {}).get("sequence_length", 60)
        if len(scaled_data) <= sequence_length:
            return {"status": "failed", "message": "Sekans oluÅŸturmak iÃ§in yeterli veri yok."}
        
        X, y_unsequenced = _create_sequences(scaled_data, sequence_length)
        
        target_idx = feature_cols.index(target_col)
        y = y_unsequenced[:, target_idx].reshape(-1, 1)
        
        test_size = self.config.get("training_params", {}).get("test_size", 0.2)
        split_idx = int(len(X) * (1 - test_size))
        self.X_train, self.X_test = X[:split_idx], X[split_idx:]
        self.y_train, self.y_test = y[:split_idx], y[split_idx:]
        self.time_index_test = raw_data.index[split_idx + sequence_length:]

        model = self._create_model(self.X_train.shape)
        
        live_predict_cb = LivePredictionCallback(pipeline=self, X_val=self.X_test, y_val=self.y_test, time_index_val=self.time_index_test)
        all_callbacks = (callbacks or []) + [live_predict_cb]
        
        self.learner = self._create_learner(model, all_callbacks)

        epochs = int(self.config.get("training_params", {}).get("epochs", 50))
        self.logger.info(f"{epochs} epoch iÃ§in model eÄŸitimi baÅŸlÄ±yor...")
        history = self.learner.fit(self.X_train, self.y_train, epochs=epochs, pipeline_name=self.config.get("pipeline_name"))
        
        final_results = live_predict_cb.last_results
        if not final_results:
            self.logger.warning("EÄŸitim tamamlandÄ± ancak LivePredictionCallback'den sonuÃ§ alÄ±namadÄ±. Manuel olarak toplanÄ±yor.")
            final_results = {
                "history": history,
                "final_loss": history['loss'][-1] if history.get('loss') else None,
                "metrics": {},
                "y_true": [], "y_pred": [], "time_index": []
            }


        self.logger.info("Rapor oluÅŸturuluyor...")
        # RaporlamayÄ± sadece gerekli verilerle Ã§aÄŸÄ±r
        generate_regression_report(
            {
                "metrics": final_results.get('metrics', {}),
                "history": final_results.get('history', {}),
                "y_true": final_results.get('y_true', []),
                "y_pred": final_results.get('y_pred', []),
                "time_index": final_results.get('time_index', []),
                "y_label": final_results.get('y_label', self._get_target_and_feature_cols()[0])
            },
            self.config
        )
        
        return {
            "final_loss": final_results.get('final_loss'),
            "metrics": final_results.get('metrics', {}),
            "history": final_results.get('history', {}), 
            "y_true": final_results.get('y_true', []),
            "y_pred": final_results.get('y_pred', []),
            "time_index": final_results.get('time_index', [])
        }
========== FILE: learner/src/azuraforge_learner/reporting.py ==========
# learner/src/azuraforge_learner/reporting.py

import os
import logging
import json # EKSÄ°K OLAN IMPORT EKLENDÄ°
from datetime import datetime
from typing import Any, Dict, List, Optional
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

def set_professional_style():
    """Matplotlib iÃ§in profesyonel bir stil ayarlar."""
    try:
        plt.style.use('seaborn-v0_8-whitegrid')
        plt.rcParams.update({
            'font.family': 'sans-serif', 'font.sans-serif': 'DejaVu Sans',
            'figure.figsize': (12, 7), 'axes.labelweight': 'bold',
            'axes.titleweight': 'bold', 'grid.color': '#dddddd'
        })
    except Exception as e:
        logging.warning(f"Matplotlib stili yÃ¼klenemedi: {e}. VarsayÄ±lan kullanÄ±lacak.")

def plot_loss_history(history: Dict[str, List[float]], save_path: str):
    set_professional_style()
    fig, ax = plt.subplots()
    ax.plot(history.get('loss', []), label='EÄŸitim KaybÄ±')
    if 'val_loss' in history:
        ax.plot(history['val_loss'], label='DoÄŸrulama KaybÄ±')
    ax.set_title('Model Ã–ÄŸrenme EÄŸrisi')
    ax.set_xlabel('Epoch')
    ax.set_ylabel('KayÄ±p (Loss)')
    ax.legend()
    ax.grid(True)
    fig.savefig(save_path)
    plt.close(fig)

def plot_prediction_comparison(y_true: np.ndarray, y_pred: np.ndarray, time_index: pd.Index, save_path: str, y_label: str):
    set_professional_style()
    fig, ax = plt.subplots()
    ax.plot(time_index, y_true, label='GerÃ§ek DeÄŸerler', marker='.', markersize=4, linestyle='-')
    ax.plot(time_index, y_pred, label='Tahmin Edilen DeÄŸerler', linestyle='--')
    ax.set_title('Tahmin vs GerÃ§ek DeÄŸerler')
    ax.set_xlabel('Tarih')
    ax.set_ylabel(y_label)
    ax.legend()
    ax.grid(True)
    plt.xticks(rotation=45)
    fig.tight_layout()
    fig.savefig(save_path)
    plt.close(fig)

def generate_regression_report(results: Dict[str, Any], config: Dict[str, Any]):
    experiment_dir = config.get('experiment_dir')
    if not experiment_dir:
        logging.error("Rapor oluÅŸturmak iÃ§in 'experiment_dir' konfigÃ¼rasyonda bulunamadÄ±.")
        return
        
    report_name = config.get('pipeline_name', 'Bilinmeyen Deney')
    
    img_dir = os.path.join(experiment_dir, "images")
    os.makedirs(img_dir, exist_ok=True)
    report_path = os.path.join(experiment_dir, "report.md")
    logging.info(f"Regresyon raporu oluÅŸturuluyor: {report_path}")

    loss_img_path = os.path.join(img_dir, "loss_history.png")
    if 'history' in results and results['history'].get('loss'):
        plot_loss_history(results['history'], save_path=loss_img_path)

    comparison_img_path = os.path.join(img_dir, "prediction_comparison.png")
    if 'y_true' in results and 'y_pred' in results and 'time_index' in results:
        plot_prediction_comparison(
            y_true=np.asarray(results['y_true']), y_pred=np.asarray(results['y_pred']),
            time_index=results['time_index'], save_path=comparison_img_path,
            y_label=results.get('y_label', 'DeÄŸer')
        )

    metrics = results.get('metrics', {})
    r2 = metrics.get('r2_score')
    mae = metrics.get('mae')
    
    with open(report_path, "w", encoding="utf-8") as f:
        f.write(f"# Regresyon Analiz Raporu: {report_name}\n\n")
        f.write(f"**Rapor Tarihi:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write("## 1. Performans Ã–zeti\n\n")
        if r2 is not None:
            f.write(f"- **RÂ² Skoru:** `{r2:.4f}`\n")
        if mae is not None:
            f.write(f"- **Ortalama Mutlak Hata (MAE):** `{mae:.4f}`\n\n")

        f.write("## 2. Tahmin KarÅŸÄ±laÅŸtÄ±rmasÄ±\n\n")
        f.write("AÅŸaÄŸÄ±daki grafik, modelin test seti Ã¼zerindeki tahminlerini (turuncu) gerÃ§ek deÄŸerlerle (mavi) karÅŸÄ±laÅŸtÄ±rÄ±r.\n\n")
        if os.path.exists(comparison_img_path):
            f.write(f"![Tahmin KarÅŸÄ±laÅŸtÄ±rma GrafiÄŸi](images/{os.path.basename(comparison_img_path)})\n\n")
        
        f.write("## 3. EÄŸitim SÃ¼reci\n\n")
        f.write("Bu grafik, modelin eÄŸitim sÄ±rasÄ±ndaki kayÄ±p deÄŸerinin epoch'lara gÃ¶re deÄŸiÅŸimini gÃ¶sterir.\n\n")
        if os.path.exists(loss_img_path):
            f.write(f"![EÄŸitim KaybÄ±](images/{os.path.basename(loss_img_path)})\n\n")

        f.write("## 4. Deney KonfigÃ¼rasyonu\n\n")
        f.write("```json\n")
        f.write(json.dumps(config, indent=4, default=str))
        f.write("\n```\n")
========== FILE: learner/src/azuraforge_learner/__init__.py ==========
# learner/src/azuraforge_learner/__init__.py

from .events import Event
from .callbacks import Callback
from .losses import Loss, MSELoss
from .layers import Layer, Linear, ReLU, Sigmoid, LSTM
from .models import Sequential
from .optimizers import Optimizer, SGD, Adam
from .learner import Learner
from .pipelines import BasePipeline, TimeSeriesPipeline # YENÄ°

__all__ = [
    "Event", "Callback",
    "Loss", "MSELoss", 
    "Layer", "Linear", "ReLU", "Sigmoid", "LSTM",
    "Sequential", 
    "Optimizer", "SGD", "Adam",
    "Learner",
    "BasePipeline", 
    "TimeSeriesPipeline" # YENÄ°
]
========== FILE: learner/tests/azuraforge_learner/test_learner_components.py ==========
import pytest
import numpy as np

from azuraforge_learner import Learner, Sequential, Linear, ReLU, MSELoss, SGD

def test_learner_fit_simple_regression():
    X_train = np.array([[-1.0], [0.0], [1.0], [2.0]], dtype=np.float32)
    y_train = np.array([[-1.0], [1.0], [3.0], [5.0]], dtype=np.float32)
    
    model = Sequential(Linear(1, 1))
    criterion = MSELoss()
    optimizer = SGD(model.parameters(), lr=0.1)
    learner = Learner(model, criterion, optimizer)
    
    initial_loss = learner.evaluate(X_train, y_train)['val_loss']
    
    learner.fit(X_train, y_train, epochs=30)
    
    final_loss = learner.history['loss'][-1]
    
    print(f"Initial Loss: {initial_loss}, Final Loss: {final_loss}")
    assert final_loss < initial_loss / 5

def test_sequential_model_forward_pass():
    model = Sequential(Linear(2, 4), ReLU(), Linear(4, 1))
    from azuraforge_core import Tensor
    
    input_tensor = Tensor(np.random.randn(10, 2))
    output_tensor = model(input_tensor)
    
    assert output_tensor.data.shape == (10, 1)

========== FILE: learner/tests/azuraforge_learner/test_pipelines.py ==========
import pytest
import numpy as np
import pandas as pd
from unittest.mock import MagicMock, patch

from azuraforge_learner.pipelines import TimeSeriesPipeline, _create_sequences

# --- Helper Fonksiyon Testleri ---

def test_create_sequences():
    """_create_sequences fonksiyonunun doÄŸru ÅŸekil ve iÃ§erikte diziler oluÅŸturduÄŸunu test eder."""
    data = np.arange(10).reshape(-1, 1) # [0, 1, ..., 9]
    seq_length = 3
    
    X, y = _create_sequences(data, seq_length)
    
    # Beklenen Ã§Ä±ktÄ± sayÄ±sÄ±: 10 - 3 = 7
    assert X.shape == (7, 3, 1)
    assert y.shape == (7, 1)
    
    # Ä°lk sekansÄ± kontrol et
    assert np.array_equal(X[0], np.array([[0], [1], [2]]))
    assert np.array_equal(y[0], np.array([3]))
    
    # Son sekansÄ± kontrol et
    assert np.array_equal(X[-1], np.array([[6], [7], [8]]))
    assert np.array_equal(y[-1], np.array([9]))

# --- TimeSeriesPipeline Testleri ---

# Test iÃ§in somut bir Pipeline sÄ±nÄ±fÄ± oluÅŸturalÄ±m
class MockTimeSeriesPipeline(TimeSeriesPipeline):
    def _load_data_from_source(self) -> pd.DataFrame:
        dates = pd.to_datetime(pd.date_range(start="2023-01-01", periods=100))
        data = {'Close': np.linspace(100, 200, 100), 'Volume': np.random.rand(100) * 1000}
        return pd.DataFrame(data, index=dates)

    def _get_target_and_feature_cols(self) -> tuple[str, list[str]]:
        return "Close", ["Close", "Volume"]

    def _create_model(self, input_shape: tuple):
        # GerÃ§ek bir model oluÅŸturmaya gerek yok, sadece bir mock nesne dÃ¶ndÃ¼r
        return MagicMock()

@pytest.fixture
def pipeline_instance():
    """Her test iÃ§in taze bir pipeline Ã¶rneÄŸi oluÅŸturur."""
    config = {
        "pipeline_name": "test_pipeline",
        "model_params": {"sequence_length": 10},
        "training_params": {"test_size": 0.2},
        "feature_engineering": {"target_col_transform": "none"},
        "system": {"caching_enabled": False}
    }
    return MockTimeSeriesPipeline(config)

def test_pipeline_data_split(pipeline_instance):
    """Pipeline'Ä±n veriyi doÄŸru ÅŸekilde train/test olarak ayÄ±rdÄ±ÄŸÄ±nÄ± test eder."""
    
    # run metodunun iÃ§indeki ilgili kÄ±sÄ±mlarÄ± taklit ederek test edelim
    # Normalde run() metodunu Ã§aÄŸÄ±rÄ±rdÄ±k ama o Ã§ok fazla ÅŸey yapÄ±yor.
    # Bu yÃ¼zden sadece veri hazÄ±rlama adÄ±mlarÄ±nÄ± test ediyoruz.
    
    with patch.object(pipeline_instance, '_create_learner', return_value=MagicMock()) as mock_create_learner:
        with patch('azuraforge_learner.pipelines.generate_regression_report'): # RaporlamayÄ± mock'la
             # `run` metodunu Ã§aÄŸÄ±rdÄ±ÄŸÄ±mÄ±zda, iÃ§indeki bazÄ± adÄ±mlarÄ± doÄŸrulamak istiyoruz.
             # LivePredictionCallback'in sahte sonuÃ§lar dÃ¶ndÃ¼rmesini saÄŸlÄ±yoruz
            with patch('azuraforge_learner.pipelines.LivePredictionCallback') as mock_live_cb:
                # Callback'in last_results Ã¶zelliÄŸine sahte bir deÄŸer atÄ±yoruz
                mock_instance = mock_live_cb.return_value
                mock_instance.last_results = {'metrics': {}, 'history': {'loss': [0.1]}}

                pipeline_instance.run()

    # run() Ã§aÄŸrÄ±ldÄ±ktan sonra, pipeline'Ä±n iÃ§ state'ini kontrol edelim
    total_samples = 100 - pipeline_instance.config["model_params"]["sequence_length"] # 90
    expected_test_size = int(total_samples * 0.2) # 18
    expected_train_size = total_samples - expected_test_size # 72
    
    assert pipeline_instance.X_train.shape[0] == expected_train_size
    assert pipeline_instance.y_train.shape[0] == expected_train_size
    assert pipeline_instance.X_test.shape[0] == expected_test_size
    assert pipeline_instance.y_test.shape[0] == expected_test_size
    assert len(pipeline_instance.time_index_test) == expected_test_size


def test_pipeline_log_transform(pipeline_instance):
    """Pipeline'Ä±n logaritmik dÃ¶nÃ¼ÅŸÃ¼mÃ¼ ve ters dÃ¶nÃ¼ÅŸÃ¼mÃ¼ doÄŸru yaptÄ±ÄŸÄ±nÄ± test eder."""
    pipeline_instance.config["feature_engineering"]["target_col_transform"] = "log"
    
    # Orijinal deÄŸerler (Ã¶lÃ§eklenmiÅŸ ve log alÄ±nmÄ±ÅŸ gibi davranalÄ±m)
    y_true_scaled = np.array([[0.5], [0.6]])
    y_pred_scaled = np.array([[0.51], [0.59]])
    
    # Ters Ã¶lÃ§ekleme iÃ§in sahte scaler'lar
    mock_scaler = MagicMock()
    # np.log1p(100) -> 4.615, np.log1p(200) -> 5.303. Scaler bu aralÄ±kta Ã§alÄ±ÅŸsÄ±n.
    # inverse_transform'un un-log'lanmÄ±ÅŸ ama hala Ã¶lÃ§ekli deÄŸerler dÃ¶ndÃ¼rdÃ¼ÄŸÃ¼nÃ¼ varsayalÄ±m.
    mock_scaler.inverse_transform.side_effect = lambda x: np.expm1(x * 5) # Basit bir ters Ã¶lÃ§ekleme taklidi
    pipeline_instance.scaler = mock_scaler
    
    # Metodu Ã§aÄŸÄ±r
    y_true_final, y_pred_final = pipeline_instance._inverse_transform_all(y_true_scaled, y_pred_scaled)
    
    # SonuÃ§larÄ±n beklendiÄŸi gibi Ã¼ssÃ¼ alÄ±nmÄ±ÅŸ (un-logged) olduÄŸunu kontrol et
    # mock_scaler.inverse_transform'dan gelen deÄŸerlerin expm1'den geÃ§tiÄŸini doÄŸrulamalÄ±yÄ±z.
    # Beklenen davranÄ±ÅŸ:
    # 1. unscaled_transformed = mock_scaler.inverse_transform(y_true_scaled)
    # 2. y_true_final = np.expm1(unscaled_transformed)
    # Bizim testimizde mock_scaler.inverse_transform zaten expm1'i iÃ§eriyor gibi davrandÄ±k.
    # Bu yÃ¼zden doÄŸrudan sonuÃ§larÄ± kontrol edebiliriz.
    
    # Testi daha basit yapalÄ±m:
    # GerÃ§ek deÄŸerler
    original_value = np.array([[100]]) 
    # Log dÃ¶nÃ¼ÅŸÃ¼mÃ¼
    log_value = np.log1p(original_value) 
    # Ters dÃ¶nÃ¼ÅŸÃ¼m
    unlog_value = np.expm1(log_value)
    
    assert np.allclose(original_value, unlog_value)
========== FILE: tools/README.md ==========
Here are some example usages of the enhanced Project Snapshot Tool:

### 1. Basic Snapshot Creation
```bash
# Create a snapshot of the current directory (default settings)
python tools\snapshot_tool.py collect project_snapshot.txt

# Create a snapshot with custom include/exclude patterns
python tools\snapshot_tool.py collect my_snapshot.txt \
    --include-dir src \
    --include-dir config \
    --include-ext .java \
    --include-ext .xml \
    --exclude-pattern target \
    --exclude-pattern *.iml
```

### 2. Snapshot with Comment Cleaning
```bash
# Create snapshot while removing comments from code files
python tools\snapshot_tool.py collect clean_snapshot.txt --clean-comments
```

### 3. Snapshot from Specific Base Directory
```bash
# Create snapshot from a different base directory
python tools\snapshot_tool.py collect ../snapshots/project_backup.txt --base-dir ~/projects/my_project
```

### 4. Restoration Examples
```bash
# Dry run (simulate restoration without writing files)
python tools\snapshot_tool.py restore project_snapshot.txt --dry-run

# Actual restoration to current directory
python tools\snapshot_tool.py restore project_snapshot.txt

# Restoration to different directory with overwrite
python tools\snapshot_tool.py restore project_snapshot.txt \
    --target-dir ~/projects/restored_project \
    --overwrite
```

### 5. Complex Example with Multiple Directories
```bash
# Snapshot specific directories with custom settings
python tools\snapshot_tool.py collect full_backup.txt \
    --include-dir src \
    --include-dir tests \
    --include-dir config \
    --include-ext .py \
    --include-ext .yaml \
    --include-ext Dockerfile \
    --exclude-pattern __pycache__ \
    --exclude-pattern *.log \
    --exclude-pattern temp \
    --base-dir ~/projects/my_project \
    --clean-comments
```

### 6. Checking What Will Be Included
```bash
# First see what would be included with current settings
python tools\snapshot_tool.py collect test_snapshot.txt --dry-run

# Then create the actual snapshot
python tools\snapshot_tool.py collect test_snapshot.txt
```

The tool will:
1. For `collect` command:
   - Show warnings for non-existent directories
   - Display total files included at the end
   - Preserve all file content exactly (or clean comments if requested)

2. For `restore` command:
   - Show detailed progress of each file being restored
   - Provide a summary of files restored/skipped
   - Handle directory creation automatically

Remember that the default settings already include most common development files while excluding build artifacts and temporary files, so you can often just use the basic command for most cases.
========== FILE: tools/snapshot_tool.py ==========
import os
import sys
import json
import argparse
from typing import List, Dict, Any, Set, Optional, Tuple
import re

# Enhanced configuration for included/excluded paths and extensions
DEFAULT_INCLUDE_DIRS = ["."]
DEFAULT_INCLUDE_EXTENSIONS = [
    ".toml",
    ".py",
    ".yaml",
    ".yml",
    ".json",
    ".md",
    ".txt",
    ".html",
    ".bat",
    ".sh",
    ".jsx",
    ".js",
    ".json",
    ".css",
    "Dockerfile",  # Explicitly include Dockerfile
    "docker-compose.yml",  # Common docker-compose filename
    "Makefile",
    "Procfile",
    ".env",
    ".gitignore",
    ".dockerignore"
]
DEFAULT_EXCLUDE_PATTERNS = [
    "__pycache__",
    ".git",
    ".venv",
    ".vscode",
    ".idea",
    "build",
    "dist",
    "*.egg-info",
    "*.pyc",
    "*.so",
    "*.pyd",
    ".pytest_cache",
    ".mypy_cache",
    ".dataset",
    "dataset",
    ".logs",
    "logs",
    ".output",
    "output",
    "inputs",
    "outputs",
    ".tmp",
    "checkpoints",
    "reports",
    "docs/_build",
    "site",
    "node_modules",
    ".DS_Store",
    "Thumbs.db",
    "package-lock.json",  # Explicitly exclude package-lock.json
    "yarn.lock",         # Explicitly exclude yarn.lock
    "*.lock",            # General lock files
    "*.min.js",          # Minified JS files
    "*.min.css",         # Minified CSS files
]

FILE_HEADER_TEMPLATE = "========== FILE: {file_path} =========="
SNAPSHOT_INFO_TEMPLATE = """PROJE KOD SNAPSHOT (TAM)
Toplam {total_files_placeholder} dosya bulundu ve eklendi.
Dahil Edilen Dizinler: {included_dirs_placeholder}
Dahil Edilen UzantÄ±lar: {included_extensions_placeholder}
HariÃ§ Tutulan Desenler/Yollar: {excluded_patterns_placeholder}
================================================================================
"""

def clean_code_comments(content: str, file_extension: str) -> str:
    """Removes most comments from code, attempting to preserve shebangs and type hints."""
    if file_extension not in [".py", ".sh", ".bat"]: return content
    lines = content.splitlines()
    cleaned_lines = []
    for line in lines:
        stripped_line = line.strip()
        if file_extension == ".py":
            # Preserve special comments like '# type:' and shebangs
            if stripped_line.startswith("# type:") or stripped_line.startswith("# noqa"): 
                cleaned_lines.append(line)
            elif stripped_line.startswith("#!/"): 
                cleaned_lines.append(line)
            # Remove inline comments
            elif "#" in line and not stripped_line.startswith("#"): 
                cleaned_lines.append(line.split("#", 1)[0].rstrip())
            # Remove full-line comments
            elif stripped_line.startswith("#"):
                continue # Skip full line comments
            else: 
                cleaned_lines.append(line)
        elif file_extension == ".sh":
            if stripped_line.startswith("#!/"): 
                cleaned_lines.append(line)
            elif not stripped_line.startswith("#"): 
                cleaned_lines.append(line)
        elif file_extension == ".bat":
            if not stripped_line.lower().startswith("rem "): 
                cleaned_lines.append(line)
        else: 
            cleaned_lines.append(line)
    return "\n".join(cleaned_lines)


def should_exclude(item_path: str, root_path: str, exclude_patterns: List[str]) -> bool:
    """Enhanced exclusion check with better pattern matching."""
    normalized_item_path = os.path.normpath(item_path)
    normalized_root_path = os.path.normpath(os.path.abspath(root_path))
    
    try:
        relative_item_path = os.path.relpath(normalized_item_path, normalized_root_path)
    except ValueError:
        relative_item_path = normalized_item_path
    
    relative_item_path_slashes = relative_item_path.replace(os.sep, "/")
    filename = os.path.basename(normalized_item_path)

    for pattern in exclude_patterns:
        # Exact filename match (e.g., "package-lock.json")
        if filename == pattern:
            return True
            
        # Extension pattern (e.g., "*.lock")
        if pattern.startswith("*.") and filename.endswith(pattern[1:]):
            return True
            
        # Directory name match (e.g., "node_modules")
        if "/" not in pattern and "." not in pattern and not pattern.startswith("*"):
            path_segments = relative_item_path_slashes.split("/")
            if pattern in path_segments:
                return True
                
        # Path prefix match (e.g., "docs/_build")
        if pattern in relative_item_path_slashes:
            return True
            
    return False


def collect_project_files_full(
    output_file: str,
    include_dirs: Optional[List[str]] = None,
    include_extensions: Optional[List[str]] = None,
    exclude_patterns: Optional[List[str]] = None,
    base_dir: str = ".",
    clean_comments: bool = False,
) -> None:
    if include_dirs is None: include_dirs = DEFAULT_INCLUDE_DIRS
    if include_extensions is None: include_extensions = DEFAULT_INCLUDE_EXTENSIONS
    if exclude_patterns is None: exclude_patterns = DEFAULT_EXCLUDE_PATTERNS

    abs_base_dir = os.path.abspath(base_dir)
    
    snapshot_content_header = SNAPSHOT_INFO_TEMPLATE.format(
        total_files_placeholder="{total_files_counter}",
        included_dirs_placeholder=", ".join(include_dirs),
        included_extensions_placeholder=", ".join(include_extensions),
        excluded_patterns_placeholder=", ".join(exclude_patterns),
    )

    all_found_relative_paths: Set[str] = set()
    content_parts: List[str] = [snapshot_content_header]
    processed_files_count = 0

    for inc_dir_pattern in include_dirs:
        current_scan_dir = os.path.abspath(os.path.join(abs_base_dir, inc_dir_pattern))
        if not os.path.exists(current_scan_dir):
            print(f"Warning: Include directory '{inc_dir_pattern}' (resolved to '{current_scan_dir}') does not exist. Skipping.")
            continue

        for root, dirs, files in os.walk(current_scan_dir, topdown=True):
            # Filter directories in-place to prevent os.walk from entering excluded ones
            dirs[:] = [
                d for d in dirs
                if not should_exclude(os.path.join(root, d), abs_base_dir, exclude_patterns)
            ]
            
            for file_name in files:
                file_path = os.path.join(root, file_name)
                relative_file_path = os.path.relpath(file_path, abs_base_dir)
                display_path = relative_file_path.replace(os.sep, "/")

                # Skip if already processed
                if display_path in all_found_relative_paths:
                    continue 

                # Apply exclusion patterns to files
                if should_exclude(file_path, abs_base_dir, exclude_patterns):
                    continue

                _, file_extension = os.path.splitext(file_name)
                filename_lower = file_name.lower()
                
                # Check if file should be included based on:
                # 1. Extension match (e.g., ".py")
                # 2. Full filename match (e.g., "Dockerfile")
                # 3. Extensionless but in include_extensions (e.g., "Makefile")
                should_include = (
                    file_extension.lower() in [ext.lower() for ext in include_extensions if ext.startswith('.')] or
                    filename_lower in [ext.lower() for ext in include_extensions if not ext.startswith('.')] or
                    (not file_extension and filename_lower in [ext.lower() for ext in include_extensions if not ext.startswith('.')])
                )
                
                if should_include:
                    all_found_relative_paths.add(display_path)
                    try:
                        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                            file_content = f.read()
                        
                        if clean_comments:
                            file_content = clean_code_comments(file_content, file_extension)
                        
                        content_parts.append(f"\n{FILE_HEADER_TEMPLATE.format(file_path=display_path)}\n")
                        content_parts.append(file_content)
                        processed_files_count += 1
                    except Exception as e:
                        print(f"Error reading file {relative_file_path}: {e}")
                        content_parts.append(f"\nError reading file {relative_file_path}: {e}\n")

    final_header_with_count = content_parts[0].replace("{total_files_counter}", str(processed_files_count))
    content_parts[0] = final_header_with_count

    with open(output_file, "w", encoding="utf-8") as f:
        f.write("".join(content_parts))

    print(f"Project snapshot (full) generated: {output_file}")
    print(f"Total {processed_files_count} files included.")


def restore_from_full_snapshot(
    snapshot_file: str,
    target_dir: str = ".",
    dry_run: bool = False,
    overwrite_existing: bool = False,
) -> None:
    print(f"Restoring project from snapshot: {snapshot_file}")
    if dry_run: print("DRY RUN: No files will be written.")

    try:
        with open(snapshot_file, "r", encoding="utf-8") as f:
            full_content = f.read()
    except FileNotFoundError:
        print(f"Error: Snapshot file '{snapshot_file}' not found.")
        return
    except Exception as e:
        print(f"Error reading snapshot file: {e}")
        return

    file_block_pattern = re.compile(
        r"^========== FILE: (.*?) ==========\n"
        r"(.*?)"
        r"(?=\n========== FILE: |\Z)",
        re.MULTILINE | re.DOTALL
    )
    
    info_header_last_line = SNAPSHOT_INFO_TEMPLATE.splitlines()[-1]
    content_start_index = full_content.find(info_header_last_line)
    if content_start_index == -1:
        print("Error: Could not find the end of the snapshot info header.")
        return
    
    content_to_parse = full_content[content_start_index + len(info_header_last_line):].lstrip('\n')

    files_restored = 0
    files_skipped = 0
    files_overwritten = 0
    
    matches = file_block_pattern.finditer(content_to_parse)

    for match in matches:
        relative_file_path = match.group(1).strip()
        content_part = match.group(2) 

        os_specific_relative_path = relative_file_path.replace("/", os.sep)
        target_file_path = os.path.join(target_dir, os_specific_relative_path)
        
        print(f"Processing file: {relative_file_path} (Content length: {len(content_part)}) -> {target_file_path}")

        if os.path.exists(target_file_path) and not overwrite_existing:
            print(f"  SKIPPED: File '{target_file_path}' already exists (overwrite_existing is False).")
            files_skipped += 1
            continue

        if os.path.exists(target_file_path) and overwrite_existing:
            print(f"  OVERWRITING: File '{target_file_path}'.")
            files_overwritten += 1

        if not dry_run:
            try:
                os.makedirs(os.path.dirname(target_file_path), exist_ok=True)
                with open(target_file_path, "w", encoding="utf-8") as f:
                    f.write(content_part)
                files_restored += 1
            except Exception as e:
                print(f"  ERROR: Could not write file '{target_file_path}': {e}")
        else:
            if not os.path.exists(os.path.dirname(target_file_path)):
                print(f"  DRY RUN: Would create directory {os.path.dirname(target_file_path)}")
            print(f"  DRY RUN: Would write {len(content_part)} bytes to {target_file_path}")
            files_restored += 1

    print("\n--- Restoration Summary ---")
    print(f"Files processed for restoration: {files_restored}")
    if not dry_run:
        print(f"Files actually written/overwritten: {files_restored - files_skipped}")
        print(f"Files overwritten: {files_overwritten}")
    print(f"Files skipped (already exist and overwrite=False): {files_skipped}")


def main():
    parser = argparse.ArgumentParser(description="Enhanced Project Snapshot Tool")
    subparsers = parser.add_subparsers(dest="command", required=True)

    parser_collect = subparsers.add_parser(
        "collect", help="Collect project files into a single snapshot file."
    )
    parser_collect.add_argument(
        "output_file",
        type=str,
        default="project_snapshot_full.txt",
        nargs="?",
        help="Path to the output snapshot file (default: project_snapshot_full.txt)",
    )
    parser_collect.add_argument(
        "--include-dir",
        action="append",
        dest="include_dirs",
        help="Directory to include (relative to base_dir or absolute). Can be used multiple times.",
    )
    parser_collect.add_argument(
        "--include-ext",
        action="append",
        dest="include_extensions",
        help="File extension to include (e.g., .py, .md). Can be used multiple times.",
    )
    parser_collect.add_argument(
        "--exclude-pattern",
        action="append",
        dest="exclude_patterns",
        help="Pattern/path to exclude. Can be used multiple times.",
    )
    parser_collect.add_argument(
        "--base-dir",
        type=str,
        default=".",
        help="Base directory for the project (default: current directory).",
    )
    parser_collect.add_argument(
        "--clean-comments",
        action="store_true",
        help="Attempt to remove comments from collected code files (.py, .sh, .bat).",
    )

    parser_restore = subparsers.add_parser(
        "restore", help="Restore project files from a snapshot."
    )
    parser_restore.add_argument(
        "snapshot_file", type=str, help="Path to the snapshot file to restore from."
    )
    parser_restore.add_argument(
        "--target-dir",
        type=str,
        default=".",
        help="Directory where files will be restored (default: current directory).",
    )
    parser_restore.add_argument(
        "--dry-run",
        action="store_true",
        help="Simulate restoration without writing any files.",
    )
    parser_restore.add_argument(
        "--overwrite",
        action="store_true",
        dest="overwrite_existing",
        help="Overwrite files if they already exist in the target directory.",
    )

    args = parser.parse_args()

    if args.command == "collect":
        final_include_dirs = args.include_dirs if args.include_dirs is not None else DEFAULT_INCLUDE_DIRS
        final_include_extensions = args.include_extensions if args.include_extensions is not None else DEFAULT_INCLUDE_EXTENSIONS
        final_exclude_patterns = args.exclude_patterns if args.exclude_patterns is not None else DEFAULT_EXCLUDE_PATTERNS
        
        collect_project_files_full(
            output_file=args.output_file,
            include_dirs=final_include_dirs,
            include_extensions=final_include_extensions,
            exclude_patterns=final_exclude_patterns,
            base_dir=args.base_dir,
            clean_comments=args.clean_comments,
        )
    elif args.command == "restore":
        restore_from_full_snapshot(
            snapshot_file=args.snapshot_file,
            target_dir=args.target_dir,
            dry_run=args.dry_run,
            overwrite_existing=args.overwrite_existing,
        )


if __name__ == "__main__":
    print("Enhanced Project Snapshot Tool")
    print("Collects project files into a single snapshot file or restores from a snapshot.")
    print("Use 'collect' to create a snapshot and 'restore' to restore files from it.")    
    main()
========== FILE: worker/Dockerfile ==========
# Base image olarak Python 3.10'un slim versiyonunu kullan
FROM python:3.10-slim-bullseye

# Gerekli sistem paketlerini kur
RUN apt-get update && \
    apt-get install -y git --no-install-recommends && \
    rm -rf /var/lib/apt/lists/*

# Ã‡alÄ±ÅŸma dizinini ayarla
WORKDIR /app

# === BASÄ°T VE GARANTÄ° YÃ–NTEM ===
# Ã–nce projenin TÃœM dosyalarÄ±nÄ± kopyala
COPY . .

# AdÄ±m 5: CuPy ve proje baÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± kur
# RUN pip install --no-cache-dir cupy-cuda12x
RUN pip install --no-cache-dir -e .
# === BÄ°TTÄ° ===

# AdÄ±m 6: Konteyner baÅŸlatÄ±ldÄ±ÄŸÄ±nda Ã§alÄ±ÅŸtÄ±rÄ±lacak komut
CMD ["start-worker"]
========== FILE: worker/pyproject.toml ==========
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "azuraforge-worker"
version = "0.2.4" # Versiyonu artÄ±rÄ±yoruz
description = "The Celery worker for the AzuraForge Platform. Discovers and runs pipeline plugins."
requires-python = ">=3.8"
dependencies = [
    # === DEÄÄ°ÅÄ°KLÄ°K BURADA: app-stock-predictor'Ä±n en son versiyonuna iÅŸaret ediyor ===
    "azuraforge-app-stock-predictor @ git+https://github.com/AzuraForge/app-stock-predictor.git@v0.1.3",
    "celery[redis]",
    "pyyaml",
    "SQLAlchemy",
    "psycopg2-binary",
]

[project.scripts]
start-worker = "azuraforge_worker.main:run_azuraforge_worker"
========== FILE: worker/README.md ==========
# AzuraForge Worker Servisi

Bu servis, AzuraForge platformunun aÄŸÄ±r iÅŸ yÃ¼kÃ¼nÃ¼ taÅŸÄ±yan, arka plan gÃ¶revlerini iÅŸleyen motorudur.

## ğŸ¯ Ana Sorumluluklar

1.  **GÃ¶rev Ä°ÅŸleyici (Celery Worker):**
    *   `Redis`'teki gÃ¶rev kuyruÄŸunu dinler ve `API` tarafÄ±ndan gÃ¶nderilen yeni gÃ¶revleri (Ã¶rn: model eÄŸitimi) alÄ±r.
    *   Platforma "eklenti" olarak kurulan AI pipeline'larÄ±nÄ± (`azuraforge-app-*`) keÅŸfeder ve Ã§alÄ±ÅŸtÄ±rÄ±r.

2.  **Raporlama ve SonuÃ§ Ãœretimi:**
    *   Tamamlanan her deney iÃ§in sonuÃ§larÄ± (`results.json`) ve gÃ¶rsel raporlarÄ± (`report.md`) oluÅŸturur ve paylaÅŸÄ±lan `/reports` dizinine yazar.

3.  **Redis Pub/Sub YayÄ±ncÄ±sÄ±:**
    *   EÄŸitim sÄ±rasÄ±nda, `RedisProgressCallback` aracÄ±lÄ±ÄŸÄ±yla, anlÄ±k ilerleme verilerini (epoch, kayÄ±p deÄŸeri vb.) ilgili Redis kanalÄ±na (`task-progress:*`) yayÄ±nlayarak `API` servisinin canlÄ± takip yapmasÄ±nÄ± saÄŸlar.

## ğŸ› ï¸ Yerel GeliÅŸtirme ve Test

Bu servisi yerel ortamda Ã§alÄ±ÅŸtÄ±rmak ve test etmek iÃ§in, ana `platform` reposundaki **[GeliÅŸtirme Rehberi](../../platform/docs/DEVELOPMENT_GUIDE.md)**'ni takip edin.

Servis baÄŸÄ±mlÄ±lÄ±klarÄ± kurulduktan ve sanal ortam aktive edildikten sonra, aÅŸaÄŸÄ±daki komutla Worker'Ä± baÅŸlatabilirsiniz:

```bash
# worker/ kÃ¶k dizinindeyken
start-worker
```

Worker, Redis'e baÄŸlanacak ve yeni gÃ¶revleri beklemeye baÅŸlayacaktÄ±r.

**Birim Testleri (YakÄ±nda):**
Birim testlerini Ã§alÄ±ÅŸtÄ±rmak iÃ§in:
```bash
pytest
```

========== FILE: worker/setup.py ==========
from setuptools import setup, find_packages

setup(
    package_dir={"": "src"},
    packages=find_packages(where="src"),
)

========== FILE: worker/src/azuraforge_worker/callbacks.py ==========
# worker/src/azuraforge_worker/callbacks.py

import json
import os
import redis
from typing import Any, Optional
from azuraforge_learner import Callback
import logging # Loglama modÃ¼lÃ¼nÃ¼ import ediyoruz

class RedisProgressCallback(Callback):
    """
    Learner'dan gelen olaylarÄ± dinler ve Redis Pub/Sub kanalÄ± Ã¼zerinden
    ilerleme durumunu yayÄ±nlar.
    """
    def __init__(self, task_id: str):
        super().__init__()
        self.task_id = task_id
        self._redis_client: Optional[redis.Redis] = None
        try:
            redis_url = os.environ.get("REDIS_URL", "redis://redis:6379/0")
            self._redis_client = redis.from_url(redis_url)
            logging.info(f"RedisProgressCallback initialized for task {task_id}. Connected to Redis.")
        except Exception as e:
            logging.error(f"HATA: RedisProgressCallback iÃ§inde Redis'e baÄŸlanÄ±lamadÄ±: {e}")

    def on_epoch_end(self, event: Any) -> None:
        """
        Her epoch sonunda Learner tarafÄ±ndan tetiklenir ve
        ilerleme verisini ilgili Redis kanalÄ±na yayÄ±nlar.
        """
        if not self._redis_client or not self.task_id:
            return
            
        payload = event.payload
        if not payload:
            logging.warning(f"RedisProgressCallback: Empty payload for task {self.task_id}.")
            return

        try:
            channel = f"task-progress:{self.task_id}"
            
            # --- YENÄ° LOGLAMA Ä°LE TEÅHÄ°S ---
            validation_data = payload.get('validation_data')
            if validation_data:
                y_true_len = len(validation_data.get('y_true', []))
                y_pred_len = len(validation_data.get('y_pred', []))
                x_axis_len = len(validation_data.get('x_axis', []))
                logging.info(f"RedisProgressCallback: Publishing progress for task {self.task_id}, epoch {payload.get('epoch')}. Loss: {payload.get('loss'):.4f}. Validation data size: y_true={y_true_len}, y_pred={y_pred_len}, x_axis={x_axis_len}")
            else:
                logging.info(f"RedisProgressCallback: Publishing progress for task {self.task_id}, epoch {payload.get('epoch')}. Loss: {payload.get('loss'):.4f}. No validation data in payload.")
            # --- TEÅHÄ°S SONU ---

            message = json.dumps(payload)
            self._redis_client.publish(channel, message)
            
        except Exception as e:
            logging.error(f"HATA: Redis'e ilerleme durumu yayÄ±nlanamadÄ±: {e}", exc_info=True)
========== FILE: worker/src/azuraforge_worker/celery_app.py ==========
# worker/src/azuraforge_worker/celery_app.py

import os
from celery import Celery
from celery.signals import worker_process_init, worker_process_shutdown

REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")

celery_app = Celery(
    "azuraforge_worker",
    broker=REDIS_URL,
    backend=REDIS_URL,
    include=["azuraforge_worker.tasks.training_tasks"]
)

# Bu deÄŸiÅŸken, her bir worker sÃ¼recinin kendi motorunu tutmasÄ±nÄ± saÄŸlar.
engine = None

@worker_process_init.connect
def init_worker_db_connection(**kwargs):
    """Her bir Celery alt sÃ¼reci baÅŸladÄ±ÄŸÄ±nda Ã§aÄŸrÄ±lÄ±r."""
    global engine
    print("Initializing DB connection for worker process...")
    # === DEÄÄ°ÅÄ°KLÄ°K BURADA ===
    # database.py'den 'sa_create_engine' olarak import edip ismini deÄŸiÅŸtiriyoruz.
    from .database import sa_create_engine as db_create_engine
    # === DEÄÄ°ÅÄ°KLÄ°K SONU ===
    
    # DATABASE_URL'yi doÄŸrudan ortamdan alÄ±yoruz.
    db_url = os.getenv("DATABASE_URL")
    if not db_url:
        raise RuntimeError("DATABASE_URL not set, cannot initialize DB engine.")
        
    engine = db_create_engine(db_url)
    print(f"DB connection for worker process {os.getpid()} initialized.")


@worker_process_shutdown.connect
def shutdown_worker_db_connection(**kwargs):
    """Her bir Celery alt sÃ¼reci kapandÄ±ÄŸÄ±nda Ã§aÄŸrÄ±lÄ±r."""
    global engine
    if engine:
        print(f"Disposing DB connection for worker process {os.getpid()}...")
        engine.dispose()
========== FILE: worker/src/azuraforge_worker/database.py ==========
# worker/src/azuraforge_worker/database.py

import os
from sqlalchemy import create_engine as sa_create_engine, Column, String, JSON, DateTime
from sqlalchemy.orm import sessionmaker, declarative_base
from sqlalchemy.sql import func

Base = declarative_base()

class Experiment(Base):
    __tablename__ = "experiments"
    id = Column(String, primary_key=True, index=True)
    task_id = Column(String, index=True, nullable=False)
    batch_id = Column(String, index=True, nullable=True)
    batch_name = Column(String, nullable=True)
    pipeline_name = Column(String, index=True, nullable=False)
    status = Column(String, index=True, default="PENDING")
    config = Column(JSON, nullable=True)
    results = Column(JSON, nullable=True)
    error = Column(JSON, nullable=True)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    completed_at = Column(DateTime(timezone=True), nullable=True)
    failed_at = Column(DateTime(timezone=True), nullable=True)

    def __repr__(self):
        return f"<Experiment(id='{self.id}', status='{self.status}')>"

_SessionLocal = None

def get_session_local():
    """SessionLocal fabrikasÄ±nÄ± yalnÄ±zca gerektiÄŸinde oluÅŸturur (singleton)."""
    global _SessionLocal
    if _SessionLocal is None:
        # celery_app'ten her sÃ¼reÃ§ iÃ§in Ã¶zel olarak oluÅŸturulmuÅŸ engine'i al
        from .celery_app import engine
        if engine is None:
            raise RuntimeError("Database engine not initialized for this worker process.")
        _SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
    return _SessionLocal

def init_db():
    """Ana sÃ¼reÃ§te veritabanÄ± tablolarÄ±nÄ± oluÅŸturur."""
    DATABASE_URL = os.getenv("DATABASE_URL")
    if not DATABASE_URL:
        raise ValueError("DATABASE_URL ortam deÄŸiÅŸkeni ayarlanmamÄ±ÅŸ!")
    
    # Tablo oluÅŸturma iÅŸlemi iÃ§in geÃ§ici bir motor oluÅŸtur.
    engine = sa_create_engine(DATABASE_URL)
    Base.metadata.create_all(bind=engine)
    engine.dispose()
========== FILE: worker/src/azuraforge_worker/main.py ==========
# worker/src/azuraforge_worker/main.py

import logging
import sys
import platform
import multiprocessing
import os

from .celery_app import celery_app

def get_concurrency():
    """Cihaz tÃ¼rÃ¼ne gÃ¶re uygun concurrency deÄŸerini belirler."""
    device = os.environ.get("AZURAFORGE_DEVICE", "cpu").lower()
    if device == "gpu":
        # Tek bir GPU varken, Ã§ok fazla paralel sÃ¼reÃ§ baÅŸlatmak verimsizdir.
        concurrency = 4
        logging.info(f"GPU modu aktif. Concurrency = {concurrency} (sabit).")
        return concurrency
    else:
        concurrency = multiprocessing.cpu_count() / 4
        logging.info(f"CPU modu aktif. Concurrency = {concurrency} (CPU Ã§ekirdek sayÄ±sÄ±).")
        return concurrency

def run_azuraforge_worker():
    """
    Bu fonksiyon, worker'Ä± programatik olarak, subprocess kullanmadan baÅŸlatÄ±r.
    """
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s:%(lineno)d - %(levelname)s - %(message)s',
        stream=sys.stdout
    )
    logging.info("ğŸ‘·â€â™‚ï¸ Starting AzuraForge Worker via Celery's programmatic API...")
    
    # Celery worker'Ä±nÄ± baÅŸlatmak iÃ§in argÃ¼man listesi oluÅŸtur
    worker_argv = [
        'worker',
        '--loglevel=info',
        # 'prefork' Linux'ta varsayÄ±lan olduÄŸu iÃ§in belirtmeye gerek yok.
        # 'solo' da hata ayÄ±klama modundan kaldÄ±rÄ±ldÄ±.
        f'--concurrency={get_concurrency()}',
    ]
    
    # Celery uygulamasÄ±nÄ±n worker_main metodunu bu argÃ¼manlarla Ã§aÄŸÄ±r
    celery_app.worker_main(argv=worker_argv)

if __name__ == "__main__":
    run_azuraforge_worker()
========== FILE: worker/src/azuraforge_worker/__init__.py ==========
from .celery_app import celery_app

# Bu, diÄŸer projelerin 'from azuraforge_worker import celery_app' yapabilmesini saÄŸlar.
__all__ = ("celery_app",)

========== FILE: worker/src/azuraforge_worker/tasks/training_tasks.py ==========
# worker/src/azuraforge_worker/tasks/training_tasks.py

import logging
import os
import traceback
from datetime import datetime
from importlib.metadata import entry_points
from contextlib import contextmanager

from ..celery_app import celery_app
from ..callbacks import RedisProgressCallback
from ..database import Experiment, get_session_local # DEÄÄ°ÅTÄ°

# --- VeritabanÄ± Oturum YÃ¶netimi ---
@contextmanager
def get_db():
    """VeritabanÄ± oturumu iÃ§in bir context manager saÄŸlar."""
    # SessionLocal'Ä± dinamik olarak al
    SessionLocal = get_session_local()
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

# --- Pipeline KeÅŸfi (DeÄŸiÅŸiklik Yok) ---
def discover_pipelines():
    logging.info("Worker: Discovering installed AzuraForge pipeline plugins and configurations...")
    discovered = {}
    try:
        pipeline_entry_points = entry_points(group='azuraforge.pipelines')
        for ep in pipeline_entry_points:
            discovered[ep.name] = {'pipeline_class': ep.load()}
        
        config_entry_points = entry_points(group='azuraforge.configs')
        for ep in config_entry_points:
            if ep.name in discovered:
                discovered[ep.name]['get_config_func'] = ep.load()
    except Exception as e:
        logging.error(f"Worker: Error discovering pipelines or configs: {e}", exc_info=True)
    
    for p_id, p_info in discovered.items():
        logging.info(f"Worker: Discovered pipeline '{p_id}' (Config available: {'get_config_func' in p_info})")
    return discovered

AVAILABLE_PIPELINES_AND_CONFIGS = discover_pipelines()
REPORTS_BASE_DIR = os.path.abspath(os.getenv("REPORTS_DIR", "/app/reports"))
os.makedirs(REPORTS_BASE_DIR, exist_ok=True)

# --- Celery GÃ¶revi (Tamamen Yenilendi) ---
@celery_app.task(bind=True, name="start_training_pipeline")
def start_training_pipeline(self, config: dict):
    task_id = self.request.id
    pipeline_name = config.get("pipeline_name", "unknown_pipeline")
    run_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    experiment_id = f"{pipeline_name}_{run_timestamp}_{task_id[:8]}"

    # Raporlama iÃ§in hala dosya sistemini kullanabiliriz
    experiment_dir = os.path.join(REPORTS_BASE_DIR, pipeline_name, experiment_id)
    os.makedirs(experiment_dir, exist_ok=True)
    
    # KonfigÃ¼rasyonu zenginleÅŸtir
    config['experiment_id'] = experiment_id
    config['task_id'] = task_id
    config['experiment_dir'] = experiment_dir
    config['start_time'] = datetime.now().isoformat()

    try:
        if not pipeline_name or pipeline_name not in AVAILABLE_PIPELINES_AND_CONFIGS:
            raise ValueError(f"Pipeline '{pipeline_name}' not found or installed.")

        # --- VeritabanÄ± KaydÄ± BaÅŸlatma ---
        with get_db() as db:
            new_experiment = Experiment(
                id=experiment_id,
                task_id=task_id,
                pipeline_name=pipeline_name,
                status="STARTED",
                config=config
            )
            db.add(new_experiment)
            db.commit()
            logging.info(f"Worker: Experiment {experiment_id} 'STARTED' olarak veritabanÄ±na kaydedildi.")

        # Pipeline'Ä± Ã§alÄ±ÅŸtÄ±r
        PipelineClass = AVAILABLE_PIPELINES_AND_CONFIGS[pipeline_name]['pipeline_class']
        pipeline_instance = PipelineClass(config)
        redis_callback = RedisProgressCallback(task_id=task_id)
        results = pipeline_instance.run(callbacks=[redis_callback])

        # --- VeritabanÄ± KaydÄ±nÄ± BaÅŸarÄ±yla GÃ¼ncelleme ---
        with get_db() as db:
            exp_to_update = db.query(Experiment).filter(Experiment.id == experiment_id).first()
            if exp_to_update:
                exp_to_update.status = "SUCCESS"
                exp_to_update.results = results
                exp_to_update.completed_at = datetime.now(datetime.utcnow().tzinfo)
                db.commit()
                logging.info(f"Worker: Experiment {experiment_id} 'SUCCESS' olarak gÃ¼ncellendi.")
        
        logging.info(f"Worker: Task {task_id} completed successfully.")
        return {"experiment_id": experiment_id, "status": "SUCCESS"}

    except Exception as e:
        tb_str = traceback.format_exc()
        logging.error(f"PIPELINE CRITICAL FAILURE in task {task_id} (experiment: {experiment_id}): {e}")
        logging.error(f"FULL TRACEBACK:\n{tb_str}")
        
        # --- VeritabanÄ± KaydÄ±nÄ± Hatayla GÃ¼ncelleme ---
        with get_db() as db:
            exp_to_update = db.query(Experiment).filter(Experiment.id == experiment_id).first()
            if exp_to_update:
                exp_to_update.status = "FAILURE"
                exp_to_update.error = {"message": str(e), "traceback": tb_str}
                exp_to_update.failed_at = datetime.now(datetime.utcnow().tzinfo)
                db.commit()
                logging.error(f"Worker: Experiment {experiment_id} 'FAILURE' olarak gÃ¼ncellendi.")
        
        raise e
========== FILE: worker/src/azuraforge_worker/tasks/__init__.py ==========
