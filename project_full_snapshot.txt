PROJE KOD SNAPSHOT (TAM)
Toplam 80 dosya bulundu ve eklendi.
Dahil Edilen Dizinler: .
Dahil Edilen UzantÄ±lar: .toml, .py, .yaml, .yml, .json, .md, .txt, html, .bat, .sh, .jsx, .js, .json, .css
HariÃ§ Tutulan Desenler/Yollar: __pycache__, .git, .venv, .vscode, .idea, build, dist, *.egg-info, *.pyc, *.so, *.pyd, .pytest_cache, .mypy_cache, .dataset, dataset, .logs, logs, .output, output, inputs, outputs, .tmp, checkpoints, reports, docs/_build, site, node_modules, .DS_Store, Thumbs.db, *.lock
================================================================================

========== FILE: docker-compose.yml ==========
services:
  # 1. Redis Servisi (Mesaj KuyruÄŸu ve SonuÃ§ Deposu)
  redis:
    image: "redis:alpine"
    container_name: azuraforge_redis
    ports: ["6379:6379"]
    volumes: ["redis_data:/data"]

  # --- KÃœTÃœPHANE SERVÄ°SLERÄ° (SADECE BUILD VE SAÄžLIK KONTROLÃœ Ä°Ã‡Ä°N) ---
  # Bu servisler, ana uygulamalarÄ±n (API, Worker) baÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± kurabilmesi iÃ§in Ã¶nceden build edilir.
  # BaÄŸÄ±mlÄ±lÄ±k zincirinde alt seviyede olduklarÄ± iÃ§in 'depends_on' gerekmez,
  # API ve Worker Dockerfile'larÄ± onlarÄ± zaten pip ile Ã§eker.
  # Buradaki tanÄ±mlar, onlarÄ±n da build edildiÄŸini doÄŸrulamak iÃ§indir.

  # 2. Core KÃ¼tÃ¼phanesi
  core_lib:
    container_name: azuraforge_core_lib_build_test
    build:
      context: ./core # 'core' reposunun bulunduÄŸu klasÃ¶rÃ¼ gÃ¶ster
      dockerfile: Dockerfile # 'core' reposunun iÃ§indeki Dockerfile
    command: python -c "import azuraforge_core; print('AzuraForge Core built and imported successfully in Docker!')"
    # volumes: - ./core:/app # GeliÅŸtirme sÄ±rasÄ±nda kodu anÄ±nda yansÄ±tmak iÃ§in
    # Bu servis sadece build ediliyor, Ã§alÄ±ÅŸtÄ±rÄ±lmÄ±yor. Mount'a gerek yok.

  # 3. Learner KÃ¼tÃ¼phanesi
  learner_lib:
    container_name: azuraforge_learner_lib_build_test
    build:
      context: ./learner
      dockerfile: Dockerfile
    command: python -c "import azuraforge_learner; print('AzuraForge Learner built and imported successfully in Docker!')"
    # volumes: - ./learner:/app # Sadece build ediliyor.

  # 4. Applications Katalogu
  applications_catalog:
    container_name: azuraforge_applications_catalog_build_test
    build:
      context: ./applications
      dockerfile: Dockerfile
    command: python -c "import azuraforge_applications; print('AzuraForge Applications Catalog built and imported successfully in Docker!')"
    # volumes: - ./applications:/app # Sadece build ediliyor.

  # 5. App Stock Predictor (Uygulama Eklentisi)
  app_stock_predictor:
    container_name: azuraforge_app_stock_predictor_build_test
    build:
      context: ./app-stock-predictor
      dockerfile: Dockerfile
    command: python -c "import azuraforge_stockapp; print('AzuraForge App Stock Predictor built and imported successfully in Docker!')"
    # volumes: - ./app-stock-predictor:/app # Sadece build ediliyor.

  # --- ANA PLATFORM SERVÄ°SLERÄ° ---
  # Bu servisler, tÃ¼m ekosistemin temelidir ve diÄŸer kÃ¼tÃ¼phanelere baÄŸÄ±mlÄ±dÄ±r.

  # 6. API Servisi
  api:
    container_name: azuraforge_api
    build:
      context: ./api # 'api' reposunun bulunduÄŸu klasÃ¶rÃ¼ gÃ¶ster
      dockerfile: Dockerfile # 'api' reposunun iÃ§indeki Dockerfile
    command: start-api # 'api' reposundaki entrypoint script'i
    ports: ["8000:8000"]
    volumes:
      - ./api:/app # API'Ä±n kendi kodu
      - ${REPORTS_DIR}:/app/reports # Ortak rapor dizini (Host makineden mount ediliyor)
    environment:
      - REDIS_URL=${REDIS_URL}
      - REPORTS_DIR=/app/reports
    depends_on: [redis] # Redis'e baÄŸÄ±mlÄ±

  # 7. Worker Servisi
  worker:
    container_name: azuraforge_worker
    build:
      context: ./worker
      dockerfile: Dockerfile
    command: start-worker
    volumes:
      - ./worker:/app # Worker'Ä±n kendi kodu
      - ${REPORTS_DIR}:/app/reports # Raporlar iÃ§in
    environment:
      - REDIS_URL=${REDIS_URL}
      - REPORTS_DIR=/app/reports
    depends_on: [redis]

  # 8. Dashboard Servisi
  dashboard:
    container_name: azuraforge_dashboard
    build:
      context: ./dashboard
      dockerfile: Dockerfile
    command: npm run dev -- --host 0.0.0.0
    ports: ["5173:5173"]
    volumes:
      - ./dashboard:/app
      - /app/node_modules
    depends_on: [api]

volumes:
  redis_data: # Redis verilerini kalÄ±cÄ± tutmak iÃ§in volume

========== FILE: README.md ==========
# AzuraForge Platform ðŸš€

**AzuraForge Platform**, yapay zeka modellerini sÄ±fÄ±rdan oluÅŸturmak, eÄŸitmek ve yÃ¶netmek iÃ§in tasarlanmÄ±ÅŸ modÃ¼ler, daÄŸÄ±tÄ±k ve eklenti tabanlÄ± bir MLOps platformudur. Modern mikroservis mimarisi prensipleriyle inÅŸa edilmiÅŸtir.

Bu depo, AzuraForge ekosistemindeki tÃ¼m ana servisleri (API, Worker, Dashboard) ve kÃ¼tÃ¼phaneleri (Core, Learner, Applications) bir araya getiren **orkestrasyon katmanÄ±dÄ±r**.

## ðŸŽ¯ Temel AmaÃ§lar ve Felsefe

*   **SÄ±fÄ±rdan Ä°nÅŸa:** Derin Ã¶ÄŸrenme motoru ve temel bileÅŸenler sÄ±fÄ±rdan geliÅŸtirilmiÅŸtir.
*   **ModÃ¼lerlik ve BaÄŸÄ±msÄ±zlÄ±k:** Her bileÅŸen (kÃ¼tÃ¼phane, API, worker, UI, uygulama) kendi baÄŸÄ±msÄ±z repo'sunda yaÅŸar ve kendi sorumluluÄŸuna sahiptir.
*   **Olay GÃ¼dÃ¼mlÃ¼ Mimari:** Servisler arasÄ± iletiÅŸim olay tabanlÄ± (Celery, Redis, WebSockets) gerÃ§ekleÅŸir.
*   **Eklenti TabanlÄ±:** Yeni yapay zeka modelleri ve uygulamalarÄ±, platformun Ã§ekirdek koduna dokunmadan birer eklenti (plugin) olarak eklenebilir.
*   **Ã–lÃ§eklenebilirlik:** DaÄŸÄ±tÄ±k servisler sayesinde yatayda Ã¶lÃ§eklenebilir.
*   **Profesyonel GeliÅŸtirici Deneyimi:** Otomatik kurulum, test ve dokÃ¼mantasyon ile geliÅŸtirme sÃ¼recini kolaylaÅŸtÄ±rmak.

## ðŸ›ï¸ Mimari Genel BakÄ±ÅŸ

AzuraForge platformu, aÅŸaÄŸÄ±daki baÄŸÄ±msÄ±z GitHub depolarÄ±ndan oluÅŸan bir mikroservis mimarisini benimser:

-   **`core`** (`azuraforge-core`): Otomatik tÃ¼rev yeteneklerine sahip temel matematik motoru (NumPy/CuPy).
-   **`learner`** (`azuraforge-learner`): `core` Ã¼zerinde geliÅŸtirilmiÅŸ yÃ¼ksek seviyeli derin Ã¶ÄŸrenme kÃ¼tÃ¼phanesi (Katmanlar, OptimizatÃ¶rler, KayÄ±p FonksiyonlarÄ±, `Learner` sÄ±nÄ±fÄ±).
-   **`applications`** (`azuraforge-applications`): Platform iÃ§in resmi uygulama eklentilerinin katalogu (JSON dosyasÄ±).
-   **`app-stock-predictor`** (`azuraforge-app-stock-predictor`): GerÃ§ek bir uygulama eklentisi Ã¶rneÄŸi (Hisse Senedi Tahmini).
-   **`api`** (`azuraforge-api`): RESTful API ve WebSocket endpoint'leri sunan iletiÅŸim katmanÄ±.
-   **`worker`** (`azuraforge-worker`): Arka plan gÃ¶revlerini iÅŸleyen ve uygulama eklentilerini Ã§alÄ±ÅŸtÄ±ran iÅŸÃ§i servisi.
-   **`dashboard`** (`azuraforge-dashboard`): Platform iÃ§in web tabanlÄ± kullanÄ±cÄ± arayÃ¼zÃ¼.

Bu repo, tÃ¼m bu servisleri tek bir `docker-compose` komutuyla ayaÄŸa kaldÄ±ran ana orkestrasyon katmanÄ±dÄ±r.

## ðŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§ (Docker Compose ile)

TÃ¼m platformu yerel makinenizde tek bir komutla baÅŸlatmak iÃ§in:

1.  **Docker Desktop'Ä±n yÃ¼klÃ¼ ve Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun.**
2.  **Bu repoyu klonlayÄ±n:**
    ```bash
    git clone https://github.com/AzuraForge/platform.git
    cd platform
    ```
3.  **.env dosyasÄ±nÄ± oluÅŸturun:**
    Proje kÃ¶k dizininde `.env` adÄ±nda bir dosya oluÅŸturun ve iÃ§ine raporlarÄ±n kaydedileceÄŸi dizini belirtin.
    ```
    # .env
    REDIS_URL=redis://redis:6379/0
    # Rapor dizini: Worker'Ä±n sonuÃ§larÄ± yazacaÄŸÄ± ve API'nin okuyacaÄŸÄ± host makinedeki dizin
    # Windows iÃ§in C:/azuraforge_platform_reports veya ./reports
    # Linux/macOS iÃ§in: ./reports
    REPORTS_DIR=./reports 
    ```
    **Ã–NEMLÄ°:** Docker Compose'u Ã§alÄ±ÅŸtÄ±rmadan Ã¶nce bu dizini host makinenizde oluÅŸturduÄŸunuzdan emin olun. Ã–rneÄŸin, Linux/macOS'ta:
    ```bash
    mkdir -p ./reports
    ```
    (Windows kullanÄ±yorsanÄ±z `REPORTS_DIR`'i `C:/azuraforge_platform_reports` gibi bir mutlak yola ayarlamanÄ±z ve bu klasÃ¶rÃ¼ oluÅŸturmanÄ±z daha gÃ¼venli olabilir).
4.  **Platformu baÅŸlatÄ±n:**
    ```bash
    docker-compose up --build -d
    ```
    (`-d` parametresi arka planda Ã§alÄ±ÅŸtÄ±rmayÄ± saÄŸlar.)

5.  **Platforma eriÅŸin:**
    -   **Dashboard:** `http://localhost:5173`
    -   **API DokÃ¼mantasyonu:** `http://localhost:8000/api/v1/docs`

## ðŸ› ï¸ GeliÅŸtirme Rehberi ve Ä°Ã§ Detaylar

Bu rehber, platformun nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ±, nasÄ±l katkÄ±da bulunacaÄŸÄ±nÄ±zÄ± ve geliÅŸtirme ortamÄ±nÄ±zÄ± nasÄ±l yÃ¶neteceÄŸinizi detaylandÄ±rÄ±r.

**[Tam GeliÅŸtirme Rehberine Git](./docs/DEVELOPMENT_GUIDE.md)**

## ðŸ¤ KatkÄ±da Bulunma

Projenin geliÅŸimine katkÄ±da bulunmak iÃ§in [CONTRIBUTING.md](./docs/CONTRIBUTING.md) dosyasÄ±nÄ± inceleyin.

## ðŸ—ºï¸ Yol HaritasÄ± ve Gelecek Vizyonu

Projenin tamamlanan aÅŸamalarÄ±, mevcut durumu ve gelecek hedefleri hakkÄ±nda bilgi almak iÃ§in [PROJECT_JOURNEY.md](./docs/PROJECT_JOURNEY.md) dosyasÄ±nÄ± okuyun.

========== FILE: api/pyproject.toml ==========
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "azuraforge-api"
version = "0.1.0"
description = "The API server for the AzuraForge Platform."
requires-python = ">=3.8"

dependencies = [
    "azuraforge-learner @ git+https://github.com/AzuraForge/learner.git@main",
    # Yeni Ã¶nerilen: (PyPI'da olsaydÄ±) ( Ancak ÅŸu an iÃ§in henÃ¼z PyPI'da deÄŸil )
    # "azuraforge-learner==0.1.1", # Belirli bir sÃ¼rÃ¼mÃ¼ hedefle
    # Veya hala yerel geliÅŸtirme iÃ§in, ama geliÅŸtirme rehberindeki -e kurulumu Ã¶ncelikli
    # "azuraforge-learner @ git+https://github.com/AzuraForge/learner.git@v0.1.1", # EÄŸer tag'ler kullanÄ±lÄ±yorsa

    "azuraforge-worker @ git+https://github.com/AzuraForge/worker.git@main",
    # Yeni Ã¶nerilen:
    # "azuraforge-worker==0.1.0", # Belirli bir sÃ¼rÃ¼mÃ¼ hedefle

    "azuraforge-applications @ git+https://github.com/AzuraForge/applications.git@main",
    # Yeni Ã¶nerilen:
    # "azuraforge-applications==0.1.0", # Belirli bir sÃ¼rÃ¼mÃ¼ hedefle

    "fastapi", "uvicorn[standard]", "pydantic-settings", "python-dotenv", "pyyaml",
]

[project.scripts]
start-api = "azuraforge_api.main:run_server"
========== FILE: api/README.md ==========
# api

========== FILE: api/setup.py ==========
from setuptools import setup, find_packages

setup(
    # Bu satÄ±r, setuptools'a paketlerin 'src' klasÃ¶rÃ¼nÃ¼n iÃ§inde
    # olduÄŸunu sÃ¶yler.
    package_dir={"": "src"},
    
    # Bu satÄ±r, 'src' klasÃ¶rÃ¼nÃ¼n iÃ§indeki tÃ¼m Python paketlerini
    # (azuraforge_api ve altÄ±ndakiler) otomatik olarak bulur.
    packages=find_packages(where="src"),
)

========== FILE: api/src/azuraforge_api/main.py ==========
import uvicorn
from fastapi import FastAPI, APIRouter # DÃœZELTME: APIRouter'Ä± import et
from fastapi.middleware.cors import CORSMiddleware

from .core.config import settings
from .routes import experiments, pipelines, streaming

def create_app() -> FastAPI:
    app = FastAPI(title=settings.PROJECT_NAME, version="0.1.0")
    
    # CORS ayarlarÄ±nÄ± dinamik olarak belirle
    if settings.CORS_ORIGINS == "*":
        allowed_origins = ["*"]
    else:
        allowed_origins = [origin.strip() for origin in settings.CORS_ORIGINS.split(',')]

    app.add_middleware(
        CORSMiddleware,
        allow_origins=allowed_origins,
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    # DÃœZELTME: Ä°Ã§ iÃ§e FastAPI uygulamasÄ± yerine tek bir APIRouter kullanÄ±yoruz.
    # Bu, "AttributeError: 'FastAPI' object has no attribute 'default_response_class'"
    # hatasÄ±nÄ± Ã§Ã¶zer.
    api_router = APIRouter()
    api_router.include_router(experiments.router)
    api_router.include_router(pipelines.router)
    
    # Åžimdi bu birleÅŸtirilmiÅŸ router'Ä± tek bir prefix ile ana uygulamaya ekliyoruz.
    app.include_router(api_router, prefix=settings.API_V1_PREFIX)
    
    # WebSocket router'Ä± prefix dÄ±ÅŸÄ±nda, doÄŸrudan ana uygulamaya ekleniyor.
    app.include_router(streaming.router)
    
    @app.get("/", tags=["Root"])
    def read_root():
        return {"message": f"Welcome to {settings.PROJECT_NAME}"}
        
    return app

app = create_app()

def run_server():
    print(f"ðŸš€ Starting {settings.PROJECT_NAME}...")
    uvicorn.run("azuraforge_api.main:app", host="0.0.0.0", port=8000, reload=True)
========== FILE: api/src/azuraforge_api/__init__.py ==========

========== FILE: api/src/azuraforge_api/core/config.py ==========
from pydantic_settings import BaseSettings, SettingsConfigDict

class Settings(BaseSettings):
    PROJECT_NAME: str = "AzuraForge API"
    API_V1_PREFIX: str = "/api/v1"
    
    # Yeni CORS ayarÄ±
    # VirgÃ¼lle ayrÄ±lmÄ±ÅŸ URL'ler veya tÃ¼mÃ¼ne izin vermek iÃ§in "*"
    CORS_ORIGINS: str = "*" # VarsayÄ±lan olarak tÃ¼mÃ¼ne izin ver (geliÅŸtirme iÃ§in)
    
    model_config = SettingsConfigDict(env_file=".env", env_file_encoding='utf-8')

settings = Settings()

========== FILE: api/src/azuraforge_api/core/__init__.py ==========

========== FILE: api/src/azuraforge_api/routes/experiments.py ==========
from fastapi import APIRouter, HTTPException
from typing import List, Dict, Any
from ..services import experiment_service

# DÃœZELTME: tags'i buraya al, prefix'i ana dosyada bÄ±rak
router = APIRouter(tags=["Experiments"])

@router.get("/experiments", response_model=List[Dict[str, Any]])
def get_all_experiments():
    return experiment_service.list_experiments()

@router.post("/experiments", status_code=202, response_model=Dict[str, Any])
def create_new_experiment(config: Dict[str, Any]):
    return experiment_service.start_experiment(config)

@router.get("/experiments/{task_id}/status", response_model=Dict[str, Any])
def get_experiment_status(task_id: str):
    return experiment_service.get_task_status(task_id)
========== FILE: api/src/azuraforge_api/routes/pipelines.py ==========
from fastapi import APIRouter, HTTPException
from typing import List, Dict, Any
from ..services import experiment_service

router = APIRouter(tags=["Pipelines"])

@router.get("/pipelines", response_model=List[Dict[str, Any]])
def get_all_available_pipelines():
    return experiment_service.get_available_pipelines()

@router.get("/pipelines/{pipeline_id}/config", response_model=Dict[str, Any])
def get_pipeline_default_config(pipeline_id: str):
    try:
        return experiment_service.get_default_pipeline_config(pipeline_id)
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
========== FILE: api/src/azuraforge_api/routes/streaming.py ==========
import asyncio
import logging
from fastapi import APIRouter, WebSocket, WebSocketDisconnect
from celery.result import AsyncResult

from azuraforge_worker import celery_app

router = APIRouter()

@router.websocket("/ws/task_status/{task_id}")
async def websocket_task_status(websocket: WebSocket, task_id: str):
    await websocket.accept()
    logging.info(f"WebSocket connection accepted for task: {task_id}")
    
    task_result = AsyncResult(task_id, app=celery_app)
    
    try:
        # DÃœZELTME: BaÄŸlantÄ± kurulur kurulmaz ilk durumu gÃ¶nder.
        # Bu, hÄ±zlÄ± biten gÃ¶revlerin son durumunun anÄ±nda UI'a ulaÅŸmasÄ±nÄ± saÄŸlar.
        initial_status = {
            "state": task_result.state,
            "details": task_result.info if task_result.state == 'PROGRESS' else None,
            "result": task_result.result if task_result.ready() else None
        }
        await websocket.send_json(initial_status)

        # GÃ¶rev tamamlanana kadar dÃ¶ngÃ¼de kal
        while not task_result.ready():
            if task_result.state == 'PROGRESS':
                await websocket.send_json({
                    "state": task_result.state,
                    "details": task_result.info,
                })
            await asyncio.sleep(1)
        
        # DÃ¶ngÃ¼ bittiÄŸinde son durumu tekrar gÃ¶nder (garanti amaÃ§lÄ±)
        await websocket.send_json({
            "state": task_result.state,
            "result": task_result.result,
        })

    except WebSocketDisconnect:
        logging.warning(f"WebSocket disconnected for task: {task_id}")
    except Exception as e:
        logging.error(f"An error occurred in WebSocket for task {task_id}: {e}")
        try:
            await websocket.send_json({
                "state": "ERROR",
                "details": {"message": str(e), "task_id": task_id}
            })
        except Exception:
            pass 
    finally:
        logging.info(f"Closing WebSocket for task {task_id}")
        await websocket.close()
========== FILE: api/src/azuraforge_api/routes/__init__.py ==========

========== FILE: api/src/azuraforge_api/services/experiment_service.py ==========
import json
import os
import glob
from importlib import resources
from importlib.metadata import entry_points
from typing import List, Dict, Any, Optional
from celery.result import AsyncResult 

from azuraforge_worker import celery_app
from azuraforge_worker.tasks.training_tasks import start_training_pipeline
from azuraforge_worker.tasks.training_tasks import AVAILABLE_PIPELINES_AND_CONFIGS

REPORTS_BASE_DIR = os.path.abspath(os.getenv("REPORTS_DIR", "/app/reports"))

def get_available_pipelines() -> List[Dict[str, Any]]:
    # Mevcut kodunuzu kullanarak resmi uygulamalar kataloÄŸunu Ã§ekmeye devam edin
    # Bu, dashboard'daki "pipeline adÄ±" ve "aÃ§Ä±klama" gibi meta verileri saÄŸlar.
    official_apps_data = []
    try:
        with resources.open_text("azuraforge_applications", "official_apps.json") as f:
            official_apps_data = json.load(f)
    except (FileNotFoundError, ModuleNotFoundError) as e:
        print(f"ERROR: Could not find or read the official apps catalog. {e}")
        # Hata durumunda boÅŸ liste dÃ¶ndÃ¼rmek yerine hata fÄ±rlatÄ±labilir veya varsayÄ±lan bir ÅŸey saÄŸlanabilir.

    # Åžimdi worker'dan keÅŸfedilen pipeline'lar ile bu meta veriyi birleÅŸtir.
    # Sadece worker'Ä±n gerÃ§ekten keÅŸfettiÄŸi pipeline'larÄ± sun.
    available_pipelines_with_configs = []
    for app_meta in official_apps_data:
        app_id = app_meta.get("id")
        if app_id in AVAILABLE_PIPELINES_AND_CONFIGS:
            # Sadece keÅŸfedilenleri ekle
            available_pipelines_with_configs.append(app_meta)
    
    return available_pipelines_with_configs

# Yeni fonksiyon: Belirli bir pipeline'Ä±n varsayÄ±lan konfigÃ¼rasyonunu dÃ¶ndÃ¼rÃ¼r
def get_default_pipeline_config(pipeline_id: str) -> Dict[str, Any]:
    """Belirli bir pipeline'Ä±n varsayÄ±lan konfigÃ¼rasyonunu dÃ¶ndÃ¼rÃ¼r."""
    pipeline_info = AVAILABLE_PIPELINES_AND_CONFIGS.get(pipeline_id)
    if not pipeline_info:
        raise ValueError(f"Pipeline '{pipeline_id}' not found or its config function is missing.")
    
    get_config_func = pipeline_info.get('get_config_func')
    if not get_config_func:
        return {"message": "No specific default configuration available for this pipeline. Worker will use its internal defaults.", "pipeline_name": pipeline_id}

    return get_config_func()


def list_experiments() -> List[Dict[str, Any]]:
    """
    DÃœZELTME: ArtÄ±k her deney iÃ§in results.json dosyasÄ±nÄ±n tamamÄ±nÄ± dÃ¶ndÃ¼rÃ¼yor.
    Bu, UI'Ä±n geniÅŸletilebilir satÄ±rlarda tÃ¼m detaylarÄ± gÃ¶stermesini saÄŸlar.
    """
    experiment_files = glob.glob(f"{REPORTS_BASE_DIR}/**/results.json", recursive=True)
    experiments = []
    for f_path in experiment_files:
        try:
            with open(f_path, 'r') as f:
                data = json.load(f)
                # Direkt olarak dosyanÄ±n iÃ§eriÄŸini listeye ekle
                experiments.append(data)
        except Exception as e:
            print(f"Warning: Could not read results.json from {f_path}: {e}")
            continue
    
    # SÄ±ralama: Ã–nce Ã§alÄ±ÅŸanlar, sonra en yeni tamamlananlar
    def sort_key(exp):
        status_order = {'STARTED': 1, 'PROGRESS': 2, 'PENDING': 3, 'UNKNOWN': 4, 'DISCONNECTED': 5, 'FAILURE': 6, 'ERROR': 7, 'SUCCESS': 8}
        status = exp.get('status', 'UNKNOWN')
        # EÄŸer canlÄ± bir gÃ¶revse, Celery'den anlÄ±k durumunu al. Bu, PENDING'den STARTED'a geÃ§iÅŸi yakalar.
        if status in ['STARTED', 'PROGRESS', 'PENDING']:
             task = AsyncResult(exp.get('task_id'), app=celery_app)
             status = task.state
             exp['status'] = status # Deney objesini de anlÄ±k durumla gÃ¼ncelle
        
        timestamp = exp.get('completed_at') or exp.get('failed_at') or exp.get('config', {}).get('start_time', '1970-01-01T00:00:00')
        return (status_order.get(status, 99), timestamp)

    experiments.sort(key=sort_key, reverse=False) # Status'e gÃ¶re artan, tarihe gÃ¶re azalan sÄ±ralama iÃ§in
    # reverse=False olacak ama sÄ±ralama anahtarÄ±nÄ± (timestamp) negatif yapmak aynÄ± etkiyi verir.
    # En basit yol:
    experiments.sort(key=lambda x: x.get('config', {}).get('start_time', ''), reverse=True)
    
    return experiments


def start_experiment(config: Dict[str, Any]) -> Dict[str, Any]:
    pipeline_name = config.get("pipeline_name", "unknown")
    print(f"Service: Sending task for pipeline '{pipeline_name}' to Celery with config: {config}") # Logu gÃ¼ncellendi
    task = start_training_pipeline.delay(config) 
    return {"message": "Experiment submitted to worker.", "task_id": task.id}

def get_task_status(task_id: str) -> Dict[str, Any]:
    # ArtÄ±k /experiments endpoint'i tÃ¼m veriyi dÃ¶ndÃ¼rdÃ¼ÄŸÃ¼ iÃ§in bu endpoint'e olan ihtiyaÃ§ azalÄ±yor.
    # Ama WebSocket'in ilk veri Ã§ekiÅŸi iÃ§in hala deÄŸerli olabilir.
    task_result = AsyncResult(task_id, app=celery_app)
    status = task_result.state
    
    details = task_result.info 
    
    # BaÅŸarÄ± durumunda, Celery result.result'Ä± doÄŸrudan alÄ±p gÃ¶nder
    if status == 'SUCCESS':
        # BaÅŸarÄ±lÄ± biten gÃ¶revlerin rapor dosyasÄ±ndan okunmasÄ±nÄ± saÄŸla
        # Bu, sayfa yenilense bile tÃ¼m verinin (Ã¶zellikle loss geÃ§miÅŸinin) gÃ¶rÃ¼nmesini garanti eder.
        matching_experiments = list_experiments()
        for exp in matching_experiments:
            if exp.get('task_id') == task_id: # task_id'yi de almanÄ±z gerekebilir list_experiments iÃ§inde
                return exp # Zaten gerekli tÃ¼m verileri iÃ§eren objeyi dÃ¶ndÃ¼r

        # EÄŸer rapor dosyasÄ±ndan bulunamazsa, Celery sonucunu dÃ¶ndÃ¼r
        return {"task_id": task_id, "status": status, "result": task_result.result}
    
    elif status == 'FAILURE':
        error_message = details.get('error_message', 'Bilinmeyen bir hata oluÅŸtu.')
        traceback_info = details.get('traceback', 'DetaylÄ± hata izleme bilgisi yok.')
        
        # KullanÄ±cÄ± dostu hata mesajÄ± Ã¼retimi (Ã¶rnek)
        user_friendly_message = "Deney eÄŸitimi sÄ±rasÄ±nda bir hata oluÅŸtu. LÃ¼tfen yapÄ±landÄ±rmanÄ±zÄ± kontrol edin veya AI modelinde bir sorun olabilir."
        if "yfinance.download" in error_message or "No data downloaded" in error_message:
            user_friendly_message = "Veri Ã§ekilirken bir sorun oluÅŸtu. Ticker sembolÃ¼nÃ¼ veya baÅŸlangÄ±Ã§ tarihini kontrol edin."
        elif "Not enough data" in error_message:
            user_friendly_message = "EÄŸitim iÃ§in yeterli veri bulunamadÄ±. LÃ¼tfen daha uzun bir tarih aralÄ±ÄŸÄ± seÃ§in."
        elif "Pipeline execution failed" in error_message:
            user_friendly_message = "Pipeline'Ä±n kendisi Ã§alÄ±ÅŸÄ±rken bir hata ile karÅŸÄ±laÅŸtÄ±. Detaylar iÃ§in aÅŸaÄŸÄ±daki teknik hata mesajÄ±nÄ± inceleyin."


        # EÄŸer rapor dosyasÄ±ndan bulunursa, hata bilgilerini oradan al.
        # Bu bÃ¶lÃ¼m, Ã¶nceki get_task_status'taki mantÄ±kla Ã§akÄ±ÅŸmamasÄ± iÃ§in biraz daha dikkatli ele alÄ±nmalÄ±.
        # En iyisi, task_id'ye gÃ¶re rapor dosyasÄ±ndan full veriyi Ã§ekip, Celery'den sadece canlÄ± durumu almak.
        # Åžimdilik direkt Celery sonucunu zenginleÅŸtirelim.
        
        # Buradaki return, direkt Celery'den alÄ±nan 'FAILURE' state'indeki veriyi dÃ¶ndÃ¼rÃ¼r.
        # Rapor dosyasÄ±ndaki "error" alanÄ±nÄ± burada "user_friendly_error" olarak ekleyebiliriz.
        return {
            "task_id": task_id, 
            "status": status, 
            "result": details, # Celery meta verisi (error_message, traceback iÃ§erir)
            "user_friendly_error": user_friendly_message
        }
    
    # PROGRESS, PENDING, STARTED gibi durumlar iÃ§in
    return {"task_id": task_id, "status": status, "details": details}
========== FILE: api/src/azuraforge_api/services/__init__.py ==========

========== FILE: api/src/azuraforge_api/tasks/training_tasks.py ==========

========== FILE: api/src/azuraforge_api/tasks/__init__.py ==========

========== FILE: app-stock-predictor/pyproject.toml ==========
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "azuraforge-app-stock-predictor"
version = "0.1.0"
description = "A stock prediction pipeline application for the AzuraForge platform."
requires-python = ">=3.8"
dependencies = [
    "azuraforge-learner @ git+https://github.com/AzuraForge/learner.git@main",
    "yfinance",
    "pandas",
    "scikit-learn",
    "PyYAML", # <-- YENÄ° BAÄžIMLILIK
]

[project.entry-points]
# Var olan giriÅŸ noktasÄ±
"azuraforge.pipelines" = { stock_predictor = "azuraforge_stockapp.pipeline:StockPredictionPipeline" }

# --- YENÄ° GÄ°RÄ°Åž NOKTASI GRUBU ---
"azuraforge.configs" = { stock_predictor = "azuraforge_stockapp.pipeline:get_default_config" }

========== FILE: app-stock-predictor/README.md ==========
# app-stock-predictor

========== FILE: app-stock-predictor/setup.py ==========
from setuptools import setup, find_packages
setup(
    package_dir={"": "src"},
    packages=find_packages(where="src"),
    # DÃœZELTME: Paket kurulduÄŸunda .yml gibi Python dÄ±ÅŸÄ± dosyalarÄ±n da
    # kopyalanmasÄ±nÄ± saÄŸlar. Bu, API ve Worker loglarÄ±ndaki hatayÄ± Ã§Ã¶zer.
    include_package_data=True, 
    package_data={
        # "azuraforge_stockapp" paketi iÃ§indeki tÃ¼m .yml dosyalarÄ±nÄ± dahil et.
        "azuraforge_stockapp": ["config/*.yml"], 
    },
)
========== FILE: app-stock-predictor/src/azuraforge_stockapp/pipeline.py ==========
import logging
import yfinance as yf
import numpy as np
from sklearn.preprocessing import MinMaxScaler
import os 
from typing import Any
import yaml
from importlib import resources # YENÄ°: Paket iÃ§i kaynak okumak iÃ§in en gÃ¼venilir yol

from azuraforge_learner import Learner, Sequential, Linear, MSELoss, SGD, ReLU

def get_default_config():
    """
    Bu pipeline'Ä±n varsayÄ±lan konfigÃ¼rasyonunu bir Python sÃ¶zlÃ¼ÄŸÃ¼ olarak dÃ¶ndÃ¼rÃ¼r.
    DÃœZELTME: importlib.resources kullanarak dosya okuma hatasÄ±nÄ± giderir.
    """
    try:
        # "azuraforge_stockapp.config" modÃ¼lÃ¼ iÃ§indeki dosyayÄ± gÃ¼venli bir ÅŸekilde aÃ§ar
        with resources.open_text("azuraforge_stockapp.config", "stock_predictor_config.yml") as f:
            config = yaml.safe_load(f)
        return config
    except (FileNotFoundError, ModuleNotFoundError, Exception) as e:
        logging.error(f"Error loading default config for stock_predictor: {e}", exc_info=True)
        return {"error": f"Could not load default config: {e}"}

class StockPredictionPipeline:
    def __init__(self, config: dict, celery_task: Any = None):
        self.celery_task = celery_task
        self.logger = logging.getLogger(self.__class__.__name__)
        logging.basicConfig(level="INFO", format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

        # Gelen konfigÃ¼rasyonu varsayÄ±lanÄ±n Ã¼zerine yazarak tam bir config oluÅŸtur
        default_config = get_default_config()
        if "error" in default_config:
            self.logger.error("Could not load default config. Using only provided config.")
            self.config = config
        else:
            # Derin birleÅŸtirme (nested dict'ler iÃ§in)
            def deep_merge(source, destination):
                for key, value in source.items():
                    if isinstance(value, dict):
                        node = destination.setdefault(key, {})
                        deep_merge(value, node)
                    else:
                        destination[key] = value
                return destination
            
            # Ã–nce varsayÄ±lanÄ± al, sonra gelen config ile Ã¼zerine yaz
            merged_config = default_config.copy()
            self.config = deep_merge(config, merged_config)

    def run(self):
        data_sourcing_config = self.config.get("data_sourcing", {})
        training_params_config = self.config.get("training_params", {})
        
        ticker = data_sourcing_config.get("ticker", "MSFT")
        start_date = data_sourcing_config.get("start_date", "2021-01-01")
        epochs = int(training_params_config.get("epochs", 10)) # UI'dan string gelebilir, int'e Ã§evir
        lr = float(training_params_config.get("lr", 0.01)) # UI'dan string gelebilir, float'a Ã§evir

        self.logger.info(f"--- Running Stock Prediction Pipeline for {ticker} ({epochs} epochs, lr={lr}) ---")
        
        try:
            data = yf.download(ticker, start=start_date, progress=False, actions=False, auto_adjust=True)
            if data.empty:
                raise ValueError(f"No data downloaded for ticker: {ticker} from {start_date}")
            self.logger.info(f"Downloaded {len(data)} rows of data.")
            close_prices = data[['Close']].values.astype(np.float32)
        except Exception as e:
            self.logger.error(f"Data download failed for {ticker}: {e}")
            raise 

        scaler = MinMaxScaler(feature_range=(-1, 1))
        scaled_prices = scaler.fit_transform(close_prices)
        
        if len(scaled_prices) < 2:
            self.logger.warning("Not enough data to create sequences for training.")
            return {"status": "completed", "ticker": ticker, "final_loss": float('inf'), "message": "Not enough data for training"}

        X, y = scaled_prices[:-1], scaled_prices[1:]
        
        model = Sequential(Linear(1, 64), ReLU(), Linear(64, 1))
        criterion = MSELoss()
        optimizer = SGD(model.parameters(), lr=lr)
        
        learner = Learner(model, criterion, optimizer, current_task=self.celery_task)

        self.logger.info(f"Starting training for {epochs} epochs...")
        history = learner.fit(X, y, epochs=epochs)
        
        final_loss = history['loss'][-1]
        self.logger.info(f"Training complete. Final loss: {final_loss:.6f}")
        
        return {"status": "completed", "ticker": ticker, "final_loss": final_loss, "loss": history['loss']}
========== FILE: app-stock-predictor/src/azuraforge_stockapp/__init__.py ==========

========== FILE: app-stock-predictor/src/azuraforge_stockapp/config/stock_predictor_config.yml ==========
pipeline_name: "stock_predictor"

data_sourcing:
  ticker: "MSFT" # Microsoft
  start_date: "2021-01-01"

training_params:
  epochs: 10 # Test iÃ§in kÄ±sa tutalÄ±m
  lr: 0.01

========== FILE: app-stock-predictor/src/azuraforge_stockapp/config/__init__.py ==========

========== FILE: applications/pyproject.toml ==========
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "azuraforge-applications"
version = "0.1.0"
description = "A catalog of official applications for the AzuraForge platform."

========== FILE: applications/README.md ==========
# applications

========== FILE: applications/setup.py ==========
from setuptools import setup, find_packages

setup(
    package_dir={"": "src"},
    packages=find_packages(where="src"),
    # EN Ã–NEMLÄ° KISIM: Paket kurulduÄŸunda .json dosyasÄ±nÄ±n da kopyalanmasÄ±nÄ± saÄŸlar
    include_package_data=True, 
    package_data={
        "azuraforge_applications": ["*.json"], # "azuraforge_apps_catalog" -> "azuraforge_applications"
    },
)


========== FILE: applications/src/azuraforge_applications/official_apps.json ==========
[
  {
    "id": "stock_predictor",
    "name": "Hisse Senedi Fiyat Tahmini",
    "repository": "https://github.com/AzuraForge/app-stock-predictor",
    "description": "LSTM tabanlÄ± hisse senedi fiyat tahmini yapar."
  },
  {
    "id": "weather_forecaster",
    "name": "Hava Durumu Tahmini",
    "repository": "https://github.com/AzuraForge/app-weather-forecaster",
    "description": "Gelecekteki hava durumunu tahmin eder (HenÃ¼z GeliÅŸtirilmedi)."
  }
]

========== FILE: applications/src/azuraforge_applications/__init__.py ==========


========== FILE: core/pyproject.toml ==========
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "azuraforge-core"
version = "0.1.2"
authors = [{ name = "Azmi Sahin" }]
description = "The core automatic differentiation engine (Tensor object) for the AzuraForge ecosystem."
readme = "README.md"
requires-python = ">=3.8"
license = { text = "MIT" }
classifiers = ["Programming Language :: Python :: 3"]
dependencies = ["numpy"]

# --- YENÄ° BÃ–LÃœM ---
[project.optional-dependencies]
dev = ["pytest"]

========== FILE: core/README.md ==========
# core

========== FILE: core/setup.py ==========
from setuptools import setup, find_packages
setup(
    package_dir={"": "src"},
    packages=find_packages(where="src"),
)

========== FILE: core/src/azuraforge_core/tensor.py ==========
import os
from typing import Callable, List, Optional, Set, Tuple, Union, Any, cast
import numpy as np

DEVICE = os.environ.get("AZURAFORGE_DEVICE", "cpu").lower()

xp: Any
if DEVICE == "gpu":
    try:
        import cupy
        xp = cupy
    except ImportError:
        import numpy
        xp = numpy
        DEVICE = "cpu"
else:
    import numpy
    xp = numpy

ArrayType = Any
ScalarType = Union[int, float, bool, np.number, xp.number]

def _empty_backward_op() -> None: pass

class Tensor:
    def __init__(self, data: Any, _children: Tuple["Tensor", ...] = (), _op: str = "", requires_grad: bool = False):
        if isinstance(data, Tensor): self.data = data.data.copy()
        else: self.data = xp.array(data, dtype=np.float64)
        
        self.requires_grad = requires_grad
        self.grad: Optional[ArrayType] = xp.zeros_like(self.data) if requires_grad else None
        self._backward: Callable[[], None] = _empty_backward_op
        self._prev: Set["Tensor"] = set(_children)
        self._op: str = _op

    def backward(self, grad_output: Optional[ArrayType] = None) -> None:
        if not self.requires_grad: return
        topo: List[Tensor] = []
        visited: Set[Tensor] = set()
        def build_topo(v):
            if v not in visited:
                visited.add(v); [build_topo(child) for child in v._prev]; topo.append(v)
        build_topo(self)
        for t in topo:
            if t.grad is not None: t.grad.fill(0.0)
        self.grad = xp.ones_like(self.data) if grad_output is None else xp.asarray(grad_output, dtype=np.float64).reshape(self.data.shape)
        for v in reversed(topo): v._backward()

    def to_cpu(self) -> np.ndarray:
        if hasattr(self.data, 'get'): return self.data.get()
        return np.array(self.data, copy=True)

    def __add__(self, other: Any) -> "Tensor":
        other = _ensure_tensor(other)
        out = Tensor(self.data + other.data, (self, other), "+", self.requires_grad or other.requires_grad)
        def _backward():
            if self.requires_grad: self.grad += _unbroadcast_to(self.data.shape, out.grad)
            if other.requires_grad: other.grad += _unbroadcast_to(other.data.shape, out.grad)
        out._backward = _backward
        return out

    def __mul__(self, other: Any) -> "Tensor":
        other = _ensure_tensor(other)
        out = Tensor(self.data * other.data, (self, other), "*", self.requires_grad or other.requires_grad)
        def _backward():
            if self.requires_grad: self.grad += _unbroadcast_to(self.data.shape, other.data * out.grad)
            if other.requires_grad: other.grad += _unbroadcast_to(other.data.shape, self.data * out.grad)
        out._backward = _backward
        return out

    def __pow__(self, power: float) -> "Tensor":
        out = Tensor(self.data ** power, (self,), f"**{power}", self.requires_grad)
        def _backward():
            if self.requires_grad: self.grad += (power * (self.data ** (power - 1))) * out.grad
        out._backward = _backward
        return out

    def dot(self, other: "Tensor") -> "Tensor":
        out = Tensor(self.data @ other.data, (self, other), "@", self.requires_grad or other.requires_grad)
        def _backward():
            if self.requires_grad: self.grad += out.grad @ other.data.T
            if other.requires_grad: other.grad += self.data.T @ out.grad
        out._backward = _backward
        return out

    def sum(self, axis=None, keepdims=False) -> "Tensor":
        out = Tensor(xp.sum(self.data, axis=axis, keepdims=keepdims), (self,), "sum", self.requires_grad)
        def _backward(_axis=axis, _keepdims=keepdims):
            if self.requires_grad and self.grad is not None:
                grad_val = out.grad
                if _axis is not None and not _keepdims:
                    grad_val = xp.expand_dims(grad_val, axis=_axis)
                self.grad += grad_val
        out._backward = _backward
        return out

    def mean(self, axis=None, keepdims=False) -> "Tensor":
        sum_val = self.sum(axis=axis, keepdims=keepdims)
        num_elements = float(np.prod(self.data.shape) / np.prod(sum_val.data.shape))
        return sum_val * (1.0 / num_elements)
    
    def relu(self) -> "Tensor":
        out = Tensor(xp.maximum(0, self.data), (self,), "ReLU", self.requires_grad)
        def _backward():
            if self.requires_grad: self.grad += (self.data > 0) * out.grad
        out._backward = _backward
        return out

    # YENÄ°: Sigmoid aktivasyon fonksiyonu
    def sigmoid(self) -> "Tensor":
        s = 1 / (1 + xp.exp(-self.data))
        out = Tensor(s, (self,), "Sigmoid", self.requires_grad)
        def _backward():
            if self.requires_grad: self.grad += out.data * (1 - out.data) * out.grad
        out._backward = _backward
        return out
        
    def __repr__(self): return f"Tensor(data={self.data}, requires_grad={self.requires_grad})"
    def __neg__(self): return self * -1
    def __sub__(self, other): return self + (-other)
    def __truediv__(self, other): return self * (_ensure_tensor(other) ** -1)
    __radd__ = __add__
    def __rmul__(self, other): return self * other
    def __rsub__(self, other): return _ensure_tensor(other) - self
    def __rtruediv__(self, other): return _ensure_tensor(other) / self

def _ensure_tensor(val: Any) -> "Tensor":
    return val if isinstance(val, Tensor) else Tensor(val)

def _unbroadcast_to(target_shape: Tuple[int, ...], grad: ArrayType) -> ArrayType:
    """Bir gradyanÄ±, orijinal tensÃ¶rÃ¼n (yayÄ±nlamadan Ã¶nceki) ÅŸekline geri kÃ¼Ã§Ã¼ltÃ¼r."""
    if target_shape == grad.shape:
        return grad
    
    # Boyut sayÄ±sÄ±nÄ± eÅŸitle
    ndim_diff = grad.ndim - len(target_shape)
    if ndim_diff > 0:
        grad = grad.sum(axis=tuple(range(ndim_diff)))

    # Boyutu 1 olan eksenler boyunca topla
    axes_to_sum = []
    for i, dim in enumerate(target_shape):
        if dim == 1:
            axes_to_sum.append(i)
    
    if axes_to_sum:
        grad = grad.sum(axis=tuple(axes_to_sum), keepdims=True)
        
    return grad
========== FILE: core/src/azuraforge_core/__init__.py ==========
from .tensor import Tensor, xp, DEVICE, ArrayType, ScalarType, _unbroadcast_to

__all__ = ["Tensor", "xp", "DEVICE", "ArrayType", "ScalarType", "_unbroadcast_to"]

========== FILE: core/tests/azuraforge_core/test_tensor.py ==========
import pytest
import numpy as np

# Test edilecek paketi import et
from azuraforge_core import Tensor

def test_tensor_creation_and_defaults():
    """Tensor nesnesinin doÄŸru ÅŸekilde ve varsayÄ±lan deÄŸerlerle oluÅŸturulduÄŸunu test eder."""
    t = Tensor([1, 2, 3])
    assert isinstance(t.data, np.ndarray)
    assert t.requires_grad is False
    assert t.grad is None

def test_tensor_requires_grad():
    """`requires_grad=True` olduÄŸunda gradyan dizisinin oluÅŸturulduÄŸunu test eder."""
    t = Tensor([1, 2], requires_grad=True)
    assert t.requires_grad is True
    assert isinstance(t.grad, np.ndarray)
    assert np.array_equal(t.grad, np.array([0.0, 0.0]))

def test_addition_backward():
    """Basit toplama iÅŸlemi iÃ§in geri yayÄ±lÄ±mÄ±n doÄŸru Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± test eder."""
    a = Tensor([1, 2, 3], requires_grad=True)
    b = Tensor(5, requires_grad=True)
    
    # c = a.sum() + b  ->  dc/da = [1, 1, 1], dc/db = 1
    c = a.sum() + b
    
    c.backward()

    assert a.grad is not None
    assert b.grad is not None
    assert np.array_equal(a.grad, [1, 1, 1])
    assert b.grad == 1.0

def test_multiplication_backward():
    """Basit Ã§arpma iÅŸlemi iÃ§in geri yayÄ±lÄ±mÄ±n doÄŸru Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± test eder."""
    x = Tensor(2.0, requires_grad=True)
    y = Tensor(3.0, requires_grad=True)
    
    z = x * y
    
    z.backward() # dz/dx = y = 3,  dz/dy = x = 2

    assert x.grad == 3.0
    assert y.grad == 2.0

def test_chained_rule_backward():
    """Zincir kuralÄ±nÄ±n birden Ã§ok iÅŸlemde doÄŸru Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± test eder."""
    x = Tensor(2.0, requires_grad=True)
    y = Tensor(3.0, requires_grad=True)

    z = x * y  # dz/dx = y, dz/dy = x
    q = z + x  # dq/dz = 1, dq/dx = 1
    
    # Zincir KuralÄ±:
    # dq/dx = (dq/dz * dz/dx) + dq/dx = (1 * y) + 1 = 3 + 1 = 4
    # dq/dy = (dq/dz * dz/dy) = 1 * x = 2
    q.backward()

    assert x.grad == 4.0
    assert y.grad == 2.0

def test_dot_product_backward():
    """Matris Ã§arpÄ±mÄ± iÃ§in geri yayÄ±lÄ±mÄ± test eder."""
    a_data = np.random.randn(2, 3)
    b_data = np.random.randn(3, 4)
    a = Tensor(a_data, requires_grad=True)
    b = Tensor(b_data, requires_grad=True)
    
    c = a.dot(b)
    
    # GradyanÄ± 1'lerden oluÅŸan bir matrisle baÅŸlat
    c.backward(np.ones_like(c.data))
    
    # Manuel olarak hesaplanan gradyanlar
    grad_a_manual = np.ones_like(c.data) @ b_data.T
    grad_b_manual = a_data.T @ np.ones_like(c.data)
    
    assert np.allclose(a.grad, grad_a_manual)
    assert np.allclose(b.grad, grad_b_manual)


def test_relu_backward():
    """ReLU aktivasyonu iÃ§in geri yayÄ±lÄ±mÄ± test eder."""
    a = Tensor([-1, 0, 5], requires_grad=True)
    r = a.relu()
    r.backward(np.array([10, 20, 30]))

    # Gradyan sadece pozitif deÄŸerler iÃ§in akar (self.data > 0)
    assert np.array_equal(a.grad, [0, 0, 30])

========== FILE: dashboard/eslint.config.js ==========
import js from '@eslint/js'
import globals from 'globals'
import reactHooks from 'eslint-plugin-react-hooks'
import reactRefresh from 'eslint-plugin-react-refresh'
import { defineConfig, globalIgnores } from 'eslint/config'

export default defineConfig([
  globalIgnores(['dist']),
  {
    files: ['**/*.{js,jsx}'],
    extends: [
      js.configs.recommended,
      reactHooks.configs['recommended-latest'],
      reactRefresh.configs.vite,
    ],
    languageOptions: {
      ecmaVersion: 2020,
      globals: globals.browser,
      parserOptions: {
        ecmaVersion: 'latest',
        ecmaFeatures: { jsx: true },
        sourceType: 'module',
      },
    },
    rules: {
      'no-unused-vars': ['error', { varsIgnorePattern: '^[A-Z_]' }],
    },
  },
])

========== FILE: dashboard/index.html ==========
<!doctype html>
<html lang="tr">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/favicon.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>AzuraForge | MLOps Dashboard</title>
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.jsx"></script>
  </body>
</html>
========== FILE: dashboard/package.json ==========
{
  "name": "dashboard",
  "private": true,
  "version": "0.0.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "vite build",
    "lint": "eslint .",
    "preview": "vite preview"
  },
  "dependencies": {
    "@fontsource/inter": "^5.2.6",
    "axios": "^1.10.0",
    "chart.js": "^4.5.0",
    "chartjs-plugin-annotation": "^3.1.0",
    "chartjs-plugin-zoom": "^2.2.0",
    "prop-types": "^15.8.1",
    "react": "^19.1.0",
    "react-chartjs-2": "^5.3.0",
    "react-dom": "^19.1.0",
    "react-router-dom": "^7.6.3",
    "react-toastify": "^11.0.5"
  },
  "devDependencies": {
    "@eslint/js": "^9.30.0",
    "@types/react": "^19.1.8",
    "@types/react-dom": "^19.1.6",
    "@vitejs/plugin-react": "^4.6.0",
    "eslint": "^9.30.0",
    "eslint-plugin-react-hooks": "^5.2.0",
    "eslint-plugin-react-refresh": "^0.4.20",
    "globals": "^16.2.0",
    "npm-check-updates": "^18.0.1",
    "vite": "^7.0.0"
  }
}

========== FILE: dashboard/README.md ==========
# React + Vite

This template provides a minimal setup to get React working in Vite with HMR and some ESLint rules.

Currently, two official plugins are available:

- [@vitejs/plugin-react](https://github.com/vitejs/vite-plugin-react/blob/main/packages/plugin-react) uses [Babel](https://babeljs.io/) for Fast Refresh
- [@vitejs/plugin-react-swc](https://github.com/vitejs/vite-plugin-react/blob/main/packages/plugin-react-swc) uses [SWC](https://swc.rs/) for Fast Refresh

## Expanding the ESLint configuration

If you are developing a production application, we recommend using TypeScript with type-aware lint rules enabled. Check out the [TS template](https://github.com/vitejs/vite/tree/main/packages/create-vite/template-react-ts) for information on how to integrate TypeScript and [`typescript-eslint`](https://typescript-eslint.io) in your project.

========== FILE: dashboard/vite.config.js ==========
import { defineConfig } from 'vite'
import react from '@vitejs/plugin-react'

// https://vite.dev/config/
export default defineConfig({
  plugins: [react()],
})

========== FILE: dashboard/src/App.css ==========
/* ==========================================================================
   1. KÃ–K DEÄžÄ°ÅžKENLER (TasarÄ±m Sistemi)
   ========================================================================== */
:root {
  /* Koyu Tema (VarsayÄ±lan) */
  --primary-color: #42b983; --primary-color-dark: #369c70; --secondary-color: #3b82f6;
  --text-color: #e2e8f0; --text-color-darker: #94a3b8; --text-inverse: #ffffff;
  --bg-color: #0f172a; --content-bg: #1e293b; --border-color: #334155; --hover-bg: #2a3a52;
  --success-color: #22c55e; --error-color: #ef4444; --warning-color: #f59e0b; --info-color: #3b82f6;
  --font-sans: 'Inter', system-ui, sans-serif; --font-mono: ui-monospace, Menlo, monospace;
  --shadow-md: 0 4px 6px -1px rgb(0 0 0 / 0.1), 0 2px 4px -2px rgb(0 0 0 / 0.1);
  --shadow-lg: 0 10px 15px -3px rgb(0 0 0 / 0.1), 0 4px 6px -2px rgb(0 0 0 / 0.1);
  --border-radius: 12px;
}
body.light-theme {
  /* AÃ§Ä±k Tema */
  --text-color: #1e293b; --text-color-darker: #475569; --bg-color: #f8fafc;
  --content-bg: #ffffff; --border-color: #e2e8f0; --hover-bg: #f1f5f9;
  --success-color: #16a34a; --error-color: #dc2626; --warning-color: #d97706; --info-color: #2563eb;
}
/* ==========================================================================
   2. TEMEL RESET VE GLOBAL STÄ°LLER
   ========================================================================== */
*, *::before, *::after { box-sizing: border-box; }
body {
  margin: 0; font-family: var(--font-sans); background-color: var(--bg-color); color: var(--text-color);
  font-synthesis: none; text-rendering: optimizeLegibility; -webkit-font-smoothing: antialiased;
  transition: background-color 0.3s, color 0.3s;
}
#root, .app-layout { display: flex; width: 100vw; height: 100vh; overflow: hidden; }
/* ==========================================================================
   3. ANA YAPI (Layout)
   ========================================================================== */
.sidebar {
  width: 260px; background-color: var(--content-bg); border-right: 1px solid var(--border-color);
  padding: 20px; display: flex; flex-direction: column; flex-shrink: 0;
  transition: background-color 0.3s, border-color 0.3s, width 0.3s ease; z-index: 2000;
}
.sidebar nav ul { list-style: none; padding: 0; margin: 0; }
.sidebar nav a {
  display: flex; align-items: center; gap: 15px; padding: 12px 15px; text-decoration: none;
  color: var(--text-color-darker); border-radius: 8px; margin-bottom: 8px;
  transition: background-color 0.2s ease, color 0.2s ease; font-weight: 500;
}
.sidebar nav a:hover { background-color: var(--hover-bg); color: var(--text-color); }
.sidebar nav a.active { background-color: var(--primary-color); color: var(--text-inverse); font-weight: 600; }
.main-content { flex-grow: 1; padding: 30px; overflow-y: auto; position: relative; }
.page-header { margin-bottom: 30px; }
.page-header h1 {
  font-size: 2.2em; font-weight: 700; color: var(--text-color); margin: 0 0 5px 0;
  display: flex; align-items: center; gap: 15px; transition: color 0.3s;
}
.page-header p { color: var(--text-color-darker); font-size: 1.1em; margin: 0; }
/* ==========================================================================
   4. LOGO VE TEMA DEÄžÄ°ÅžTÄ°RME
   ========================================================================== */
.logo-container {
  display: flex; align-items: center; justify-content: flex-start; gap: 12px;
  margin-bottom: 40px; padding: 0 10px;
}
.logo-container h1 {
  color: var(--text-color); font-size: 1.6em; font-weight: 600; margin: 0;
  letter-spacing: 0.5px; transition: color 0.3s;
}
.theme-toggle-button {
  background-color: var(--bg-color); color: var(--text-color-darker); border: 1px solid var(--border-color);
  border-radius: 8px; padding: 8px; cursor: pointer; display: flex; align-items: center;
  justify-content: center; margin-top: 20px; transition: all 0.2s ease;
}
.theme-toggle-button:hover { border-color: var(--primary-color); color: var(--primary-color); }
/* ==========================================================================
   5. BÄ°LEÅžENLER (Components)
   ========================================================================== */
.card, .table-container, .form-container, .pipeline-details {
  background-color: var(--content-bg); border: 1px solid var(--border-color);
  border-radius: var(--border-radius); box-shadow: var(--shadow-md);
  transition: background-color 0.3s, border-color 0.3s;
}
.table-container { padding: 0; }
.card { padding: 25px; }
.button-primary {
  background-color: var(--primary-color); color: var(--text-inverse); padding: 10px 20px; border: none;
  border-radius: 8px; cursor: pointer; font-size: 1em; font-weight: 600;
  transition: background-color 0.2s ease, transform 0.2s ease; display: inline-flex;
  align-items: center; gap: 8px;
}
.button-primary:hover:not(:disabled) { background-color: var(--primary-color-dark); transform: translateY(-2px); }
.button-primary:disabled { background-color: var(--border-color); cursor: not-allowed; opacity: 0.6; transform: none; }
.status-badge {
  display: inline-flex; align-items: center; gap: 6px; padding: 4px 12px; border-radius: 9999px;
  font-size: 0.8em; font-weight: 600; text-transform: uppercase; justify-content: center;
}
.status-badge::before { content: ''; display: inline-block; width: 8px; height: 8px; border-radius: 50%; }
.status-badge.status-started, .status-badge.status-progress, .status-badge.status-connecting { background-color: rgba(59, 130, 246, 0.2); color: #60a5fa; }
.status-badge.status-started::before, .status-badge.status-progress::before, .status-badge.status-connecting::before { background-color: var(--info-color); animation: pulse 2s infinite; }
@keyframes pulse { 0%, 100% { opacity: 1; } 50% { opacity: 0.5; } }
.status-badge.status-success { background-color: rgba(34, 197, 94, 0.2); color: #4ade80; }
.status-badge.status-success::before { background-color: var(--success-color); }
.status-badge.status-failure, .status-badge.status-error { background-color: rgba(239, 68, 68, 0.2); color: #f87171; }
.status-badge.status-failure::before, .status-badge.status-error::before { background-color: var(--error-color); }
.status-badge.status-pending, .status-badge.status-unknown, .status-badge.status-disconnected { background-color: rgba(148, 163, 184, 0.2); color: var(--text-color-darker); }
.status-badge.status-pending::before, .status-badge.status-unknown::before, .status-badge.status-disconnected::before { background-color: var(--text-color-darker); }
.form-group { margin-bottom: 20px; }
.form-group label { display: block; margin-bottom: 8px; font-weight: 500; color: var(--text-color-darker); }
.form-group input, .form-group select {
  width: 100%; padding: 12px; background-color: var(--bg-color); border: 1px solid var(--border-color);
  border-radius: 8px; color: var(--text-color); font-size: 1em; font-family: var(--font-sans);
  transition: background-color 0.3s, border-color 0.3s, color 0.3s;
}
.form-group input:focus, .form-group select:focus {
  outline: none; border-color: var(--primary-color); box-shadow: 0 0 0 2px color-mix(in srgb, var(--primary-color) 30%, transparent);
}
table { width: 100%; border-collapse: collapse; }
table th, table td { padding: 12px 20px; text-align: left; border-bottom: 1px solid var(--border-color); vertical-align: middle; transition: border-color 0.3s; }
table th {
  background-color: var(--bg-color); color: var(--text-color-darker); font-weight: 600;
  text-transform: uppercase; font-size: 0.8em; letter-spacing: 0.5px;
  transition: background-color 0.3s, color 0.3s;
}
table tbody tr:hover { background-color: var(--hover-bg); }
table tbody tr.selected-row { background-color: color-mix(in srgb, var(--primary-color) 10%, transparent); border-left: 3px solid var(--primary-color); }
.actions-cell { position: relative; text-align: right !important; }
.actions-button {
  background: none; border: none; font-size: 24px; line-height: 1; color: var(--text-color-darker);
  cursor: pointer; border-radius: 4px; padding: 0 8px;
}
.actions-button:hover { background-color: var(--border-color); color: var(--text-color); }
.actions-menu {
  position: absolute; right: 20px; top: 50px; background-color: var(--hover-bg); border: 1px solid var(--border-color);
  border-radius: 8px; box-shadow: var(--shadow-lg); z-index: 100; display: flex;
  flex-direction: column; padding: 8px; min-width: 180px;
}
.actions-menu button {
  background: none; border: none; color: var(--text-color); padding: 10px 15px; text-align: left;
  cursor: pointer; border-radius: 6px; display: flex; align-items: center; gap: 10px; font-size: 0.9em;
}
.actions-menu button:hover { background-color: var(--secondary-color); color: var(--text-inverse); }
/* ==========================================================================
   6. Ã–ZEL PANELLER VE MODALLAR
   ========================================================================== */
.live-tracker-pane {
  position: sticky; top: -30px; z-index: 1000; margin: -30px -30px 30px -30px;
  padding: 20px 30px; background: color-mix(in srgb, var(--content-bg) 70%, transparent);
  backdrop-filter: blur(10px); -webkit-backdrop-filter: blur(10px);
  border-bottom: 1px solid var(--border-color); box-shadow: var(--shadow-lg);
}
.comparison-modal-overlay {
  position: fixed; top: 0; left: 0; right: 0; bottom: 0;
  background-color: color-mix(in srgb, var(--bg-color) 70%, transparent); backdrop-filter: blur(8px);
  z-index: 5000; display: flex; align-items: center; justify-content: center;
}
.comparison-modal-content {
  background-color: var(--bg-color); border: 1px solid var(--border-color); border-radius: 16px;
  width: 90%; max-width: 1200px; height: 90vh; box-shadow: var(--shadow-lg);
  display: flex; flex-direction: column;
}
.comparison-header {
  display: flex; justify-content: space-between; align-items: center; padding: 20px 30px;
  border-bottom: 1px solid var(--border-color); flex-shrink: 0;
}
.comparison-header h2 { margin: 0; font-size: 1.5em; }
.close-button {
  background: none; border: none; font-size: 24px; color: var(--text-color-darker);
  cursor: pointer; transition: color 0.2s;
}
.close-button:hover { color: var(--text-color); }
.comparison-body {
  padding: 30px; flex-grow: 1; overflow-y: auto; display: flex;
  flex-direction: column; gap: 30px;
}
.comparison-chart-container { height: 400px; min-height: 300px; width: 100%; position: relative; }
.chart-instructions {
  position: absolute; bottom: 5px; right: 10px; font-size: 0.75em;
  color: var(--text-color-darker); background-color: color-mix(in srgb, var(--content-bg) 80%, transparent);
  padding: 2px 8px; border-radius: 4px; opacity: 0.7;
}
.color-indicator {
  display: inline-block; width: 12px; height: 12px; border-radius: 50%;
  margin-right: 10px; vertical-align: middle;
}
/* ==========================================================================
   7. ÃœÃ‡ÃœNCÃœ PARTÄ° KÃœTÃœPHANE STÄ°LLERÄ°
   ========================================================================== */
.Toastify__toast {
  background-color: var(--content-bg) !important; color: var(--text-color) !important;
  border: 1px solid var(--border-color) !important; border-radius: 8px !important;
  font-family: var(--font-sans) !important;
}
.Toastify__progress-bar { background: var(--primary-color) !important; }
.Toastify__close-button { color: var(--text-color) !important; }

/* ==========================================================================
   8. YENÄ° DENEY SAYFASI Ã–ZEL STÄ°LLERÄ°
   ========================================================================== */

.new-experiment-layout {
  display: flex;
  flex-direction: column;
  height: 100%; /* Ana konteynerin tÃ¼m yÃ¼ksekliÄŸi kaplamasÄ± iÃ§in */
}

.new-experiment-form {
  flex-grow: 1; /* Kalan tÃ¼m alanÄ± kapla */
  display: flex;
  flex-direction: column;
  overflow: hidden; /* Ä°Ã§erik taÅŸmasÄ±nÄ± engelle */
  border-radius: var(--border-radius);
  background-color: var(--content-bg);
  border: 1px solid var(--border-color);
}

.form-main-content {
  flex-grow: 1;
  overflow-y: auto; /* Sadece bu alan kaydÄ±rÄ±labilir olacak */
  padding: 25px;
  display: flex;
  flex-direction: column;
  gap: 25px;
}

.form-action-bar {
  flex-shrink: 0; /* Bu panelin kÃ¼Ã§Ã¼lmesini engelle */
  padding: 15px 25px;
  background-color: var(--bg-color);
  border-top: 1px solid var(--border-color);
  display: flex;
  justify-content: space-between;
  align-items: center;
  transition: background-color 0.3s, border-color 0.3s;
}

.pipeline-info {
  display: flex;
  align-items: center;
  gap: 10px;
  color: var(--text-color-darker);
}
.pipeline-info span {
  font-weight: 600;
  color: var(--text-color);
}

/* Form iÃ§indeki iÃ§ iÃ§e gruplar iÃ§in daha iyi stiller */
.form-fieldset {
  border: 1px solid var(--border-color);
  border-radius: 8px;
  padding: 20px;
  margin: 0 0 20px 0;
}
.form-fieldset legend {
  padding: 0 10px;
  font-weight: 600;
  color: var(--text-color);
}
========== FILE: dashboard/src/App.jsx ==========
import { useState, useContext } from 'react';
import { Routes, Route, Link, useNavigate, useLocation } from 'react-router-dom';
import { ToastContainer } from 'react-toastify';
import 'react-toastify/dist/ReactToastify.css';

import './App.css'; 
import { ThemeContext } from './context/ThemeContext';
import NewExperiment from './pages/NewExperiment';
import DashboardOverview from './pages/DashboardOverview';
import LiveTrackerPane from './components/LiveTrackerPane';
import Logo from './components/Logo';
import ThemeToggle from './components/ThemeToggle';

function App() {
  const [trackingTaskId, setTrackingTaskId] = useState(null);
  const navigate = useNavigate();
  const location = useLocation();
  const { theme } = useContext(ThemeContext);

  const handleExperimentStarted = (taskId) => {
    if (taskId) setTrackingTaskId(taskId);
  };
  
  const handleCloseTracker = () => {
    setTrackingTaskId(null);
  };

  const isActive = (path) => {
    if (path === '/' && (location.pathname === '/' || location.pathname.startsWith('/experiments'))) return true;
    return location.pathname === path;
  };

  return (
    <div className="app-layout">
      <ToastContainer position="bottom-right" autoClose={5000} theme={theme} />
      <aside className="sidebar">
        <Logo />
        <nav style={{ flexGrow: 1 }}>
          <ul>
            <li><Link to="/" className={isActive('/') ? 'active' : ''}><span role="img" aria-label="dashboard">ðŸ“Š</span><span>Genel BakÄ±ÅŸ</span></Link></li>
            <li><Link to="/new-experiment" className={isActive('/new-experiment') ? 'active' : ''}><span role="img" aria-label="rocket">ðŸš€</span><span>Yeni Deney</span></Link></li>
          </ul>
        </nav>
        <ThemeToggle />
      </aside>
      <main className="main-content">
        {trackingTaskId && <LiveTrackerPane taskId={trackingTaskId} onClose={handleCloseTracker} />}
        <Routes>
          <Route path="/" element={<DashboardOverview onNewExperimentClick={() => navigate('/new-experiment')} setTrackingTaskId={setTrackingTaskId} />} />
          <Route path="/new-experiment" element={<NewExperiment onExperimentStarted={handleExperimentStarted} />} />
        </Routes>
      </main>
    </div>
  );
}

export default App;
========== FILE: dashboard/src/index.css ==========
/*
  Bu dosya, gelecekte Ã§ok temel, uygulama geneli reset kurallarÄ± iÃ§in kullanÄ±labilir.
  Åžimdilik, projenin tÃ¼m Ã¶zel stil mantÄ±ÄŸÄ± App.css dosyasÄ±nda merkezileÅŸtirilmiÅŸtir.
*/
========== FILE: dashboard/src/main.jsx ==========
import React from 'react';
import { createRoot } from 'react-dom/client';
import { BrowserRouter } from 'react-router-dom';
import { ThemeProvider } from './context/ThemeContext';

// Fontsource
import '@fontsource/inter/400.css';
import '@fontsource/inter/500.css';
import '@fontsource/inter/600.css';
import '@fontsource/inter/700.css';

// Stil dosyalarÄ±
import './index.css'; 
import './App.css';
import App from './App.jsx';

createRoot(document.getElementById('root')).render(
  // StrictMode'u bilinÃ§li olarak kapalÄ± tutuyoruz
  // <React.StrictMode>
    <ThemeProvider>
      <BrowserRouter> 
        <App />
      </BrowserRouter>
    </ThemeProvider>
  // </React.StrictMode>
);
========== FILE: dashboard/src/components/ComparisonView.jsx ==========
// Bu dosyanÄ±n iÃ§eriÄŸi Ã¶nceki cevaptaki ile aynÄ±, tam halini tekrar veriyorum
import PropTypes from 'prop-types';
import { Line } from 'react-chartjs-2';
import { Chart as ChartJS, CategoryScale, LinearScale, PointElement, LineElement, Title, Tooltip, Legend } from 'chart.js';
import zoomPlugin from 'chartjs-plugin-zoom';

ChartJS.register(CategoryScale, LinearScale, PointElement, LineElement, Title, Tooltip, Legend, zoomPlugin);

const chartColors = ['#42b983', '#3b82f6', '#ef4444', '#f59e0b', '#8b5cf6', '#ec4899'];

function ComparisonView({ experiments, onClose }) {
  const chartData = {
    labels: Array.from({ length: Math.max(...experiments.map(e => e.results?.loss?.length || 0)) }, (_, i) => `E${i + 1}`),
    datasets: experiments.map((exp, i) => ({
      label: `${exp.config.data_sourcing.ticker} (${exp.experiment_id.slice(-6)})`,
      data: exp.results?.loss || [],
      borderColor: chartColors[i % chartColors.length],
      backgroundColor: `${chartColors[i % chartColors.length]}33`,
      tension: 0.1, fill: false, borderWidth: 2, pointRadius: 1, pointHoverRadius: 5,
    })),
  };

  const chartOptions = {
      responsive: true, maintainAspectRatio: false,
      interaction: { mode: 'index', intersect: false, },
      plugins: { 
        legend: { position: 'top', labels: { font: { size: 14 } } },
        tooltip: {
          backgroundColor: 'var(--content-bg)',
          borderColor: 'var(--border-color)',
          borderWidth: 1,
        },
        zoom: {
          pan: { enabled: true, mode: 'xy', modifierKey: 'alt', },
          zoom: { wheel: { enabled: true }, pinch: { enabled: true }, mode: 'xy' }
        }
      },
      scales: {
          y: { title: { display: true, text: 'KayÄ±p DeÄŸeri (Loss)' }, beginAtZero: false, }, 
          x: { title: { display: true, text: 'Epoch' }, grid: { display: false } } 
      }
  };

  return (
    <div className="comparison-modal-overlay" onClick={onClose}>
      <div className="comparison-modal-content" onClick={e => e.stopPropagation()}>
        <div className="comparison-header">
          <h2>Deney KarÅŸÄ±laÅŸtÄ±rmasÄ± ({experiments.length} adet)</h2>
          <button className="close-button" onClick={onClose}>Ã—</button>
        </div>
        <div className="comparison-body">
          <div className="comparison-chart-container">
            <Line data={chartData} options={chartOptions} />
            <p className="chart-instructions">YakÄ±nlaÅŸtÄ±rmak iÃ§in fare tekerleÄŸini kullanÄ±n. SÄ±fÄ±rlamak iÃ§in Ã§ift tÄ±klayÄ±n. KaydÄ±rmak iÃ§in <strong>Alt + SÃ¼rÃ¼kle</strong>.</p>
          </div>
          <h4 className="section-title" style={{ marginTop: 0 }}>Ã–zet Tablosu</h4>
          <div className="table-container">
            <table>
              <thead><tr><th>Deney ID</th><th>Ticker</th><th>Epochs</th><th>LR</th><th>Final KayÄ±p</th></tr></thead>
              <tbody>
                {experiments.map((exp, i) => (
                  <tr key={exp.experiment_id}>
                    <td><span className="color-indicator" style={{backgroundColor: chartColors[i % chartColors.length]}}></span>{exp.experiment_id.slice(0, 18)}...</td>
                    <td>{exp.config.data_sourcing.ticker}</td>
                    <td>{exp.config.training_params.epochs}</td>
                    <td>{exp.config.training_params.lr}</td>
                    <td>{exp.results.final_loss?.toFixed(6) ?? 'N/A'}</td>
                  </tr>
                ))}
              </tbody>
            </table>
          </div>
        </div>
      </div>
    </div>
  );
}
ComparisonView.propTypes = { experiments: PropTypes.array.isRequired, onClose: PropTypes.func.isRequired, };
export default ComparisonView;
========== FILE: dashboard/src/components/ExperimentRow.jsx ==========
// Bu dosyanÄ±n iÃ§eriÄŸi de Ã¶nceki cevaptaki ile aynÄ±, tam halini tekrar veriyorum
import { useState } from 'react';
import PropTypes from 'prop-types';
import { toast } from 'react-toastify';

const Icon = ({ path }) => <svg width="16" height="16" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d={path} /></svg>;
Icon.propTypes = { path: PropTypes.string.isRequired };

const ICONS = {
  rerun: "M12 4V1L8 5l4 4V6c3.31 0 6 2.69 6 6s-2.69 6-6 6-6-2.69-6-6H4c0 4.42 3.58 8 8 8s8-3.58 8-8-3.58-8-8-8z",
  copy: "M16 1H4c-1.1 0-2 .9-2 2v14h2V3h12V1zm3 4H8c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h11c1.1 0 2-.9 2-2V7c0-1.1-.9-2-2-2zm0 16H8V7h11v14z",
  satellite: "M6.34 5.34L4.93 3.93l-1.41 1.41 1.41 1.41C3.89 7.79 3.5 9.05 3.5 10.41c0 .46.06.91.16 1.34l-2.48 1.43c-.22.13-.34.38-.34.65v1.14c0 .27.11.52.34.65l2.48 1.43c-.1.43-.16.88-.16 1.34 0 2.21 1.79 4 4 4s4-1.79 4-4c0-1.37-.69-2.63-1.76-3.34l1.41-1.41-1.41-1.41-1.41 1.41C9.11 6.1 9.5 4.95 9.5 3.59c0-.46-.06-.91-.16-1.34l2.48-1.43c.22-.13.34-.38.34-.65V-.98c0-.27-.11-.52-.34-.65L9.34.8c.1-.43.16-.88.16-1.34 0-2.21-1.79-4-4-4s-4 1.79-4 4c0 1.37.69 2.63 1.76 3.34zm.24 9.35l-1.24-.71c.12-.52.19-1.06.19-1.61s-.07-1.09-.19-1.61l1.24-.71C7.58 10.9 8.5 12.05 8.5 13.41s-.92 2.51-1.92 3.28zM12 17.5c-2.21 0-4-1.79-4-4s1.79-4 4-4 4 1.79 4 4-1.79 4-4 4z",
};

function ExperimentRow({ experiment, isSelected, onSelect, onReRun, setTrackingTaskId }) {
  const [actionsOpen, setActionsOpen] = useState(false);
  const { experiment_id, status, config, results, completed_at, failed_at, task_id } = experiment;

  const handleCopyConfig = () => {
    navigator.clipboard.writeText(JSON.stringify(config, null, 2));
    toast.success('KonfigÃ¼rasyon panoya kopyalandÄ±!');
    setActionsOpen(false);
  };
  const isRunning = ['STARTED', 'PROGRESS', 'PENDING'].includes(status);
  return (
    <tr className={isSelected ? 'selected-row' : ''}>
      <td><input type="checkbox" checked={isSelected} onChange={onSelect} title="KarÅŸÄ±laÅŸtÄ±rmak iÃ§in seÃ§"/></td>
      <td><span className={`status-badge status-${status?.toLowerCase() || 'unknown'}`}>{status || 'Bilinmiyor'}</span></td>
      <td><div className="detail-cell"><strong>{config?.pipeline_name || 'N/A'}</strong><span className="exp-id">{experiment_id}</span></div></td>
      <td><div className="detail-cell"><span>Ticker: <strong>{config?.data_sourcing?.ticker || 'N/A'}</strong></span><span>Epochs: <strong>{config?.training_params?.epochs || 'N/A'}</strong>, LR: <strong>{config?.training_params?.lr || 'N/A'}</strong></span></div></td>
      <td><div className="detail-cell"><span>Final KayÄ±p: <strong>{results?.final_loss !== undefined ? results.final_loss.toFixed(6) : 'N/A'}</strong></span></div></td>
      <td><div className="detail-cell"><span>BaÅŸlangÄ±Ã§: {new Date(config.start_time).toLocaleString()}</span><span>BitiÅŸ: {completed_at || failed_at ? new Date(completed_at || failed_at).toLocaleString() : 'N/A'}</span></div></td>
      <td className="actions-cell">
        <button className="actions-button" onClick={() => setActionsOpen(!actionsOpen)}>â‹®</button>
        {actionsOpen && (
          <div className="actions-menu" onMouseLeave={() => setActionsOpen(false)}>
            {isRunning && <button onClick={() => { setTrackingTaskId(task_id); setActionsOpen(false); }}><Icon path={ICONS.satellite} /> CanlÄ± Ä°zle</button>}
            <button onClick={() => { onReRun(); setActionsOpen(false); }}><Icon path={ICONS.rerun} /> Yeniden Ã‡alÄ±ÅŸtÄ±r</button>
            <button onClick={handleCopyConfig}><Icon path={ICONS.copy} /> Config'i Kopyala</button>
          </div>
        )}
      </td>
    </tr>
  );
}
ExperimentRow.propTypes = { experiment: PropTypes.object.isRequired, isSelected: PropTypes.bool.isRequired, onSelect: PropTypes.func.isRequired, onReRun: PropTypes.func.isRequired, setTrackingTaskId: PropTypes.func.isRequired, };
export default ExperimentRow;
========== FILE: dashboard/src/components/ExperimentsList.jsx ==========
import PropTypes from 'prop-types';
import ExperimentRow from './ExperimentRow';

function ExperimentsList({ experiments, selectedIds, onSelect, onReRun, setTrackingTaskId }) {
  if (!experiments || experiments.length === 0) {
    return <p style={{textAlign: 'center', padding: '20px'}}>Filtrelerinize uyan bir deney bulunamadÄ±.</p>;
  }
  return (
    <div className="table-container">
      <table>
        <thead>
          <tr>
            <th style={{width: '40px'}}></th>
            <th>Durum</th>
            <th>Deney DetaylarÄ±</th>
            <th>Parametreler</th>
            <th>SonuÃ§lar</th>
            <th>Zamanlama</th>
            <th style={{width: '50px'}}>Aksiyon</th>
          </tr>
        </thead>
        <tbody>
          {experiments.map((exp) => (
            <ExperimentRow 
              key={exp.experiment_id} 
              experiment={exp} 
              isSelected={selectedIds.has(exp.experiment_id)}
              onSelect={() => onSelect(exp.experiment_id)}
              onReRun={() => onReRun(exp.config)}
              setTrackingTaskId={setTrackingTaskId}
            />
          ))}
        </tbody>
      </table>
    </div>
  );
}
ExperimentsList.propTypes = { experiments: PropTypes.array.isRequired, selectedIds: PropTypes.object.isRequired, onSelect: PropTypes.func.isRequired, onReRun: PropTypes.func.isRequired, setTrackingTaskId: PropTypes.func.isRequired, };
export default ExperimentsList;
========== FILE: dashboard/src/components/LiveTrackerPane.jsx ==========
import { useState, useEffect, useRef, useMemo } from 'react';
import PropTypes from 'prop-types';
import { Line } from 'react-chartjs-2';
import { Chart } from 'chart.js';
import 'chart.js/auto';
import { getCssVar } from '../utils/cssUtils';

// Sabit baÅŸlangÄ±Ã§ durumlarÄ±
const initialStatus = { 
  state: 'CONNECTING', 
  details: { status_text: 'Worker\'a baÄŸlanÄ±lÄ±yor...' }
};

function LiveTrackerPane({ taskId, onClose }) {
  // Tek bir state objesi
  const [liveData, setLiveData] = useState({ 
    status: initialStatus,
    chart: { labels: [], datasets: [{ data: [] }] } // BaÅŸlangÄ±Ã§ta boÅŸ
  });
  
  const socketRef = useRef(null);
  
  const chartOptions = useMemo(() => {
    return {
      responsive: true, maintainAspectRatio: false,
      animation: { duration: 300, easing: 'linear' },
      plugins: { 
        legend: { display: false }, 
        tooltip: {
          enabled: true, backgroundColor: getCssVar('--content-bg'),
          titleColor: getCssVar('--text-color'), bodyColor: getCssVar('--text-color'),
          borderColor: getCssVar('--border-color'), borderWidth: 1, padding: 10,
          displayColors: false,
          callbacks: {
            title: (ctx) => ctx[0].label,
            label: (ctx) => `KayÄ±p: ${ctx.parsed.y.toFixed(6)}`,
          }
        },
      },
      scales: { 
        y: { 
          grid: { color: getCssVar('--border-color'), borderDash: [2, 4], drawTicks: false },
          ticks: { padding: 10, maxTicksLimit: 5, font: { size: 12 }, color: getCssVar('--text-color-darker') },
        }, 
        x: {
          grid: { display: false },
          ticks: { padding: 10, maxRotation: 0, autoSkip: true, maxTicksLimit: 7, font: { size: 12 }, color: getCssVar('--text-color-darker') },
        } 
      }
    };
  }, []);

  useEffect(() => {
    if (!taskId) return;

    // YENÄ°: BaÅŸlangÄ±Ã§ chart state'ini renklerle birlikte burada ayarla
    const initialChartWithColors = {
      labels: [],
      datasets: [{
        label: 'EÄŸitim KaybÄ±', data: [], fill: true,
        borderColor: getCssVar('--primary-color'),
        backgroundColor: `color-mix(in srgb, ${getCssVar('--primary-color')} 20%, transparent)`,
        tension: 0.4, borderWidth: 2,
        pointRadius: (ctx) => ctx.dataIndex === ctx.dataset.data.length - 1 ? 6 : 0,
        pointBorderColor: getCssVar('--text-inverse'),
        pointBackgroundColor: getCssVar('--primary-color'),
        pointHoverRadius: 8,
      }]
    };
    
    setLiveData({ status: initialStatus, chart: initialChartWithColors });

    const newSocket = new WebSocket(`ws://localhost:8000/ws/task_status/${taskId}`);
    socketRef.current = newSocket;
    
    newSocket.onmessage = (event) => {
      const data = JSON.parse(event.data);
      
      setLiveData(prev => {
        let newChart = prev.chart;
        if (data.state === 'PROGRESS' && data.details?.loss !== undefined) {
          const epochLabel = `E${data.details.epoch}`;
          if (!prev.chart.labels.includes(epochLabel)) {
            const newLabels = [...prev.chart.labels, epochLabel].slice(-30);
            const newLossData = [...prev.chart.datasets[0].data, data.details.loss].slice(-30);
            newChart = { ...prev.chart, labels: newLabels, datasets: [{ ...prev.chart.datasets[0], data: newLossData }] };
          }
        } else if (data.result?.results?.loss) {
          const finalLossHistory = data.result.results.loss;
          newChart = {
            ...prev.chart,
            labels: Array.from({ length: finalLossHistory.length }, (_, i) => `E${i + 1}`),
            datasets: [{ ...prev.chart.datasets[0], data: finalLossHistory }]
          };
        }
        return { status: data, chart: newChart };
      });
    };
    
    newSocket.onerror = () => setLiveData(prev => ({ ...prev, status: { state: 'ERROR', details: { status_text: 'WebSocket baÄŸlantÄ± hatasÄ±!' } }}));
    newSocket.onclose = () => setLiveData(prev => (['SUCCESS', 'FAILURE', 'ERROR'].includes(prev.status.state)) ? prev : { ...prev, status: { ...prev.status, state: 'DISCONNECTED' }});
    
    return () => { newSocket.close(1000, "Component unmounting"); };
  }, [taskId]);
  
  const { state, details, result } = liveData.status;
  const { pipeline_name, data_sourcing } = details?.config || result?.config || {};
  const { total_epochs, epoch, status_text } = details || {};
  const progressPercent = state === 'SUCCESS' ? 100 : (total_epochs ? (epoch / total_epochs) * 100 : 0);
  
  return (
    <div className="live-tracker-pane">
      <button className="close-button" onClick={onClose}>Ã—</button>
      <div style={{display: 'flex', justifyContent: 'space-between', alignItems: 'center', marginBottom: '15px'}}>
        <h4><span role="img" aria-label="satellite">ðŸ›°ï¸</span> CanlÄ± Takip: {pipeline_name || "..."} {data_sourcing?.ticker && `(${data_sourcing.ticker})`}</h4>
        <div><span className="exp-id">ID: {taskId}</span><span className={`status-badge status-${state?.toLowerCase()}`}>{state}</span></div>
      </div>
      <div style={{display: 'flex', gap: '20px', alignItems: 'center'}}>
        <div style={{flex: 1}}><p>{status_text || state}</p><progress value={progressPercent} max="100" style={{width: '100%'}}></progress></div>
        <div style={{flex: 2, height: '100px'}}>
          {liveData.chart.labels.length > 0 && <Line data={liveData.chart} options={chartOptions} />}
        </div>
      </div>
      {state === 'FAILURE' && result?.error && <p className="feedback error" style={{marginTop: '15px', whiteSpace: 'pre-wrap', maxHeight: '100px', overflowY: 'auto'}}>{result.error}</p>}
    </div>
  );
}

LiveTrackerPane.propTypes = { 
  taskId: PropTypes.string.isRequired, 
  onClose: PropTypes.func.isRequired, 
};

export default LiveTrackerPane;
========== FILE: dashboard/src/components/Logo.jsx ==========
import PropTypes from 'prop-types';

function Logo({ size = 36 }) {
  return (
    <div className="logo-container">
      <svg width={size} height={size} viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" style={{ flexShrink: 0 }}>
        <defs>
          <linearGradient id="azura-stream-gradient-component" x1="15" y1="50" x2="85" y2="50" gradientUnits="userSpaceOnUse">
            <stop offset="0%" stopColor="var(--secondary-color)"/>
            <stop offset="100%" stopColor="var(--primary-color)"/>
          </linearGradient>
        </defs>
        <path d="M50 10 L10 90 H32 L42 70 H58 L68 90 H90 Z" fill="var(--content-bg)" stroke="var(--text-color-darker)" strokeWidth="4" strokeLinejoin="round"/>
        <path d="M15 55 C 35 45, 65 45, 85 55" stroke="url(#azura-stream-gradient-component)" strokeWidth="8" strokeLinecap="round" fill="none"/>
      </svg>
      <h1>AzuraForge</h1>
    </div>
  );
}
Logo.propTypes = { size: PropTypes.number, };
export default Logo;
========== FILE: dashboard/src/components/ThemeToggle.jsx ==========
import { useContext } from 'react';
import { ThemeContext } from '../context/ThemeContext';

const SunIcon = () => <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round"><circle cx="12" cy="12" r="5"></circle><line x1="12" y1="1" x2="12" y2="3"></line><line x1="12" y1="21" x2="12" y2="23"></line><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line><line x1="1" y1="12" x2="3" y2="12"></line><line x1="21" y1="12" x2="23" y2="12"></line><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line></svg>;
const MoonIcon = () => <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path></svg>;

function ThemeToggle() {
  const { theme, toggleTheme } = useContext(ThemeContext);

  return (
    <button onClick={toggleTheme} className="theme-toggle-button" title="TemayÄ± DeÄŸiÅŸtir">
      {theme === 'light' ? <MoonIcon /> : <SunIcon />}
    </button>
  );
}

// DÃœZELTME: Eksik olan 'export default' satÄ±rÄ± eklendi.
export default ThemeToggle;
========== FILE: dashboard/src/context/ThemeContext.jsx ==========
import { createContext, useState, useEffect, useMemo } from 'react';
import PropTypes from 'prop-types';

export const ThemeContext = createContext();

export function ThemeProvider({ children }) {
  const [theme, setTheme] = useState(() => {
    const savedTheme = localStorage.getItem('theme');
    const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
    return savedTheme || (prefersDark ? 'dark' : 'light');
  });

  useEffect(() => {
    const body = document.body;
    // body'nin class'Ä±nÄ± mevcut temaya gÃ¶re ayarla
    body.classList.toggle('light-theme', theme === 'light');
    // SeÃ§imi tarayÄ±cÄ± hafÄ±zasÄ±na kaydet
    localStorage.setItem('theme', theme);
  }, [theme]);

  const toggleTheme = () => {
    setTheme(prevTheme => (prevTheme === 'light' ? 'dark' : 'light'));
  };

  // Sadece theme ve toggleTheme'i dÄ±ÅŸarÄ±ya veriyoruz.
  const value = useMemo(() => ({ theme, toggleTheme }), [theme]);

  return (
    <ThemeContext.Provider value={value}>
      {children}
    </ThemeContext.Provider>
  );
}

ThemeProvider.propTypes = {
  children: PropTypes.node.isRequired,
};
========== FILE: dashboard/src/pages/DashboardOverview.jsx ==========
import { useState, useEffect, useMemo } from 'react';
import { toast } from 'react-toastify';
import PropTypes from 'prop-types';
import ExperimentsList from '../components/ExperimentsList';
import ComparisonView from '../components/ComparisonView';
import { fetchExperiments, startNewExperiment } from '../services/api';

function DashboardOverview({ onNewExperimentClick, setTrackingTaskId }) {
  const [experiments, setExperiments] = useState([]);
  const [loading, setLoading] = useState(true);
  const [error, setError] = useState(null);
  
  // Filtreleme ve Arama State'leri
  const [searchTerm, setSearchTerm] = useState('');
  const [filterStatus, setFilterStatus] = useState('ALL');
  
  // KarÅŸÄ±laÅŸtÄ±rma State'leri
  const [selectedForComparison, setSelectedForComparison] = useState(new Set());
  const [comparisonData, setComparisonData] = useState(null);

  const getExperiments = async (showLoadingIndicator = false) => {
    if (showLoadingIndicator) setLoading(true);
    try {
      const response = await fetchExperiments();
      // Gelen veriyi baÅŸlangÄ±Ã§ zamanÄ±na gÃ¶re sÄ±rala (en yeni en Ã¼stte)
      const sortedData = response.data.sort((a, b) => new Date(b.config.start_time) - new Date(a.config.start_time));
      setExperiments(sortedData);
      setError(null);
    } catch (err) {
      setError('API sunucusuna baÄŸlanÄ±lamadÄ± veya veri Ã§ekilemedi. Servislerin Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun.');
    } finally {
      if (showLoadingIndicator) setLoading(false);
    }
  };

  useEffect(() => {
    getExperiments(true); // Sayfa ilk yÃ¼klendiÄŸinde loading gÃ¶stergesiyle veri Ã§ek
    const intervalId = setInterval(() => getExperiments(false), 5000); // Arka planda sessizce gÃ¼ncelle
    return () => clearInterval(intervalId); // Component unmount olduÄŸunda interval'Ä± temizle
  }, []);

  // Mevcut deneylerden dinamik olarak durum listesi oluÅŸturur
  const allStatuses = useMemo(() => {
    const statuses = new Set(experiments.map(exp => exp.status));
    return ['ALL', ...Array.from(statuses).sort()];
  }, [experiments]);

  // FiltrelenmiÅŸ deneyleri hesaplar
  const filteredExperiments = useMemo(() => {
    return experiments.filter(exp => {
      // Durum filtresi
      const statusMatch = filterStatus === 'ALL' || exp.status === filterStatus;
      if (!statusMatch) return false;

      // Arama terimi filtresi (ID, pipeline adÄ± veya ticker sembolÃ¼)
      if (searchTerm) {
        const lowerCaseSearchTerm = searchTerm.toLowerCase();
        const searchFields = [
          exp.experiment_id,
          exp.config?.pipeline_name,
          exp.config?.data_sourcing?.ticker
        ];
        return searchFields.some(field => field?.toLowerCase().includes(lowerCaseSearchTerm));
      }

      return true;
    });
  }, [experiments, filterStatus, searchTerm]);
  
  // KarÅŸÄ±laÅŸtÄ±rma iÃ§in deney seÃ§me/kaldÄ±rma fonksiyonu
  const handleComparisonSelect = (experimentId) => {
    setSelectedForComparison(prev => {
      const newSelection = new Set(prev);
      if (newSelection.has(experimentId)) {
        newSelection.delete(experimentId);
      } else {
        newSelection.add(experimentId);
      }
      return newSelection;
    });
  };

  // Bir deneyi konfigÃ¼rasyonuyla yeniden Ã§alÄ±ÅŸtÄ±rma fonksiyonu
  const handleReRun = async (experimentConfig) => {
    // Yeniden Ã§alÄ±ÅŸtÄ±rmadan Ã¶nce eski ID ve zaman bilgilerini temizle
    const { experiment_id, task_id, start_time, ...configToReRun } = experimentConfig;
    toast.info(`"${configToReRun.pipeline_name}" deneyi yeniden baÅŸlatÄ±lÄ±yor...`);
    try {
      const response = await startNewExperiment(configToReRun);
      toast.success(`Deney baÅŸarÄ±yla gÃ¶nderildi! ID: ${response.data.task_id}`);
      setTrackingTaskId(response.data.task_id); // Yeniden Ã§alÄ±ÅŸtÄ±rÄ±lan deneyi canlÄ± izlemeye baÅŸla
    } catch (err) {
      toast.error('Deney yeniden baÅŸlatÄ±lamadÄ±.');
    }
  };

  // KarÅŸÄ±laÅŸtÄ±rma modalÄ±nÄ± aÃ§ar
  const handleStartComparison = () => {
    const dataToCompare = experiments.filter(exp => selectedForComparison.has(exp.experiment_id));
    if (dataToCompare.length > 1) {
      setComparisonData(dataToCompare);
    }
  };

  // YÃ¼kleme ve Hata durumlarÄ± iÃ§in gÃ¶sterilecek arayÃ¼z
  if (loading) return <p style={{textAlign: 'center', padding: '40px'}}>Deney verileri yÃ¼kleniyor...</p>;
  if (error) return <p style={{textAlign: 'center', padding: '40px', color: 'var(--error-color)'}}>{error}</p>;

  return (
    <div className="dashboard-overview">
      {/* KarÅŸÄ±laÅŸtÄ±rma modalÄ±, sadece data varsa render edilir */}
      {comparisonData && <ComparisonView experiments={comparisonData} onClose={() => setComparisonData(null)}/>}
      
      <div className="page-header">
        <h1>Genel BakÄ±ÅŸ</h1>
        <p>TÃ¼m deneylerinizi tek bir yerden yÃ¶netin, takip edin ve karÅŸÄ±laÅŸtÄ±rÄ±n.</p>
      </div>
      
      {/* Filtreleme ve Aksiyonlar Paneli */}
      <div className="card" style={{ marginBottom: '25px' }}>
        <div style={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center', flexWrap: 'wrap', gap: '20px' }}>
          <div style={{ display: 'flex', gap: '20px', flexWrap: 'wrap', alignItems: 'flex-end' }}>
            <div className="form-group" style={{ marginBottom: 0 }}>
              <label htmlFor="search-term">Arama</label>
              <input type="text" id="search-term" placeholder="ID, Pipeline, Sembol ara..." value={searchTerm} onChange={(e) => setSearchTerm(e.target.value)} />
            </div>
            <div className="form-group" style={{ marginBottom: 0 }}>
              <label htmlFor="filter-status">Durum</label>
              <select id="filter-status" value={filterStatus} onChange={(e) => setFilterStatus(e.target.value)}>
                {allStatuses.map(s => <option key={s} value={s}>{s === 'ALL' ? 'TÃ¼mÃ¼' : s}</option>)}
              </select>
            </div>
          </div>
          <button 
            className="button-primary" 
            onClick={handleStartComparison} 
            disabled={selectedForComparison.size < 2} 
            title={selectedForComparison.size < 2 ? 'KarÅŸÄ±laÅŸtÄ±rmak iÃ§in en az 2 deney seÃ§in' : ''}
          >
            <span role="img" aria-label="scales">âš–ï¸</span> SeÃ§ilenleri KarÅŸÄ±laÅŸtÄ±r ({selectedForComparison.size})
          </button>
        </div>
      </div>
      
      {/* Deney Tablosu */}
      <ExperimentsList 
        experiments={filteredExperiments} 
        selectedIds={selectedForComparison} 
        onSelect={handleComparisonSelect} 
        onReRun={handleReRun} 
        setTrackingTaskId={setTrackingTaskId} 
      />
    </div>
  );
}
DashboardOverview.propTypes = { 
  onNewExperimentClick: PropTypes.func.isRequired, 
  setTrackingTaskId: PropTypes.func.isRequired, 
};
export default DashboardOverview;
========== FILE: dashboard/src/pages/NewExperiment.jsx ==========
import { useState, useEffect } from 'react';
import { useNavigate } from 'react-router-dom';
import PropTypes from 'prop-types';
import { toast } from 'react-toastify';
import { startNewExperiment, fetchAvailablePipelines, fetchPipelineDefaultConfig } from '../services/api';

// Bu bileÅŸen artÄ±k bu dosyanÄ±n iÃ§inde yerel olarak kalabilir.
const renderConfigForm = (config, setConfig) => {
  const handleChange = (e, keyPath) => {
    const { value, type } = e.target;
    const newConfig = JSON.parse(JSON.stringify(config));
    let current = newConfig;
    for (let i = 0; i < keyPath.length - 1; i++) {
      current = current[keyPath[i]] = current[keyPath[i]] || {};
    }
    current[keyPath[keyPath.length - 1]] = type === 'number' ? parseFloat(value) : value;
    setConfig(newConfig);
  };

  const traverseConfig = (obj, path = []) => Object.entries(obj).map(([key, value]) => {
    const currentPath = [...path, key];
    const fieldId = currentPath.join('-');
    const labelText = key.replace(/_/g, ' ').replace(/\b\w/g, l => l.toUpperCase());

    if (typeof value === 'object' && value !== null && !Array.isArray(value)) {
      return (
        <fieldset key={fieldId} className="form-fieldset">
          <legend>{labelText}</legend>
          {traverseConfig(value, currentPath)}
        </fieldset>
      );
    }
    const inputType = typeof value === 'number' ? 'number' : 'text';
    return (
      <div key={fieldId} className="form-group">
        <label htmlFor={fieldId}>{labelText}:</label>
        <input type={inputType} id={fieldId} value={value ?? ''} onChange={(e) => handleChange(e, currentPath)} step={inputType === 'number' ? 'any' : undefined} />
      </div>
    );
  });

  return traverseConfig(config);
};

function NewExperiment({ onExperimentStarted }) {
  const [pipelines, setPipelines] = useState([]);
  const [selectedPipelineId, setSelectedPipelineId] = useState('');
  const [currentConfig, setCurrentConfig] = useState(null);
  const [isLoading, setIsLoading] = useState(true);
  const [isSubmitting, setIsSubmitting] = useState(false);
  const navigate = useNavigate();

  useEffect(() => {
    const loadPipelines = async () => {
      setIsLoading(true);
      try {
        const response = await fetchAvailablePipelines();
        if (response.data && response.data.length > 0) {
          setPipelines(response.data);
          setSelectedPipelineId(response.data[0].id);
        }
      } catch (error) { toast.error('Pipeline listesi yÃ¼klenemedi.'); } 
      finally { setIsLoading(false); }
    };
    loadPipelines();
  }, []);

  useEffect(() => {
    const loadPipelineConfig = async (pipelineId) => {
      if (!pipelineId) return;
      setIsLoading(true);
      setCurrentConfig(null);
      try {
        const { data } = await fetchPipelineDefaultConfig(pipelineId);
        if (data && !data.error) setCurrentConfig(data);
      } catch (error) { toast.error(`KonfigÃ¼rasyon yÃ¼klenemedi: ${error.message}`); } 
      finally { setIsLoading(false); }
    };
    loadPipelineConfig(selectedPipelineId);
  }, [selectedPipelineId]);

  const handleSubmit = async (e) => {
    e.preventDefault();
    setIsSubmitting(true);
    const configToSend = { ...currentConfig, pipeline_name: selectedPipelineId };
    try {
      const { data } = await startNewExperiment(configToSend);
      toast.success(`GÃ¶rev baÅŸarÄ±yla gÃ¶nderildi! ID: ${data.task_id}`);
      if (onExperimentStarted) onExperimentStarted(data.task_id);
      navigate('/');
    } catch (err) {
      toast.error('Deney baÅŸlatÄ±lamadÄ±. API/Worker loglarÄ±nÄ± kontrol edin.');
    } finally {
      setIsSubmitting(false);
    }
  };
  
  const selectedPipelineDetails = pipelines.find(p => p.id === selectedPipelineId);

  return (
    // YENÄ° YAPI: Form, baÅŸlÄ±k ve aksiyon paneli olarak ayrÄ±ldÄ±
    <div className="new-experiment-layout">
      <div className="page-header">
        <h1>Yeni Deney BaÅŸlat</h1>
        <p>Mevcut bir AI pipeline'Ä± seÃ§erek yeni bir eÄŸitim sÃ¼reci baÅŸlatÄ±n.</p>
      </div>
      
      <form onSubmit={handleSubmit} className="new-experiment-form">
        <div className="form-main-content"> {/* KaydÄ±rÄ±labilir alan */}
          <div className="card">
            <div className="form-group">
              <label htmlFor="pipeline-select">Ã‡alÄ±ÅŸtÄ±rÄ±lacak Pipeline Eklentisi</label>
              <select id="pipeline-select" value={selectedPipelineId} onChange={(e) => setSelectedPipelineId(e.target.value)} disabled={isLoading || isSubmitting}>
                {pipelines.map(p => <option key={p.id} value={p.id}>{p.name} ({p.id})</option>)}
              </select>
            </div>
          </div>
          
          <div className="card">
            <h3>Deney Parametreleri</h3>
            {isLoading ? <p>Parametreler yÃ¼kleniyor...</p> : (
              currentConfig ? renderConfigForm(currentConfig, setCurrentConfig) : <p>Bu pipeline iÃ§in dÃ¼zenlenebilir konfigÃ¼rasyon bulunamadÄ±.</p>
            )}
          </div>
        </div>

        {/* YENÄ° YAPI: YapÄ±ÅŸkan Aksiyon Paneli */}
        <div className="form-action-bar">
          <div className="pipeline-info">
            <strong>Pipeline:</strong>
            <span>{selectedPipelineDetails?.name || '...'}</span>
          </div>
          <button type="submit" disabled={isLoading || isSubmitting} className="button-primary">
            {isSubmitting ? 'BaÅŸlatÄ±lÄ±yor...' : 'EÄŸitimi BaÅŸlat'}
          </button>
        </div>
      </form>
    </div>
  );
}

NewExperiment.propTypes = { onExperimentStarted: PropTypes.func.isRequired };
export default NewExperiment;
========== FILE: dashboard/src/services/api.js ==========
import axios from 'axios';

const API_BASE_URL = 'http://localhost:8000/api/v1';

const apiClient = axios.create({
  baseURL: API_BASE_URL,
  headers: { 'Content-Type': 'application/json' },
});

export const fetchExperiments = () => apiClient.get('/experiments');
export const startNewExperiment = (config) => apiClient.post('/experiments', config);
export const fetchAvailablePipelines = () => apiClient.get('/pipelines'); 
export const fetchPipelineDefaultConfig = (pipelineId) => apiClient.get(`/pipelines/${pipelineId}/config`);
========== FILE: dashboard/src/utils/cssUtils.js ==========
/**
 * Belirtilen CSS deÄŸiÅŸkeninin hesaplanmÄ±ÅŸ deÄŸerini dÃ¶ndÃ¼rÃ¼r.
 * @param {string} varName - '--primary-color' gibi CSS deÄŸiÅŸken adÄ±.
 * @returns {string} - DeÄŸiÅŸkenin renk deÄŸeri (Ã¶rn. '#42b983').
 */
export const getCssVar = (varName) => {
  if (typeof window === 'undefined') return '';
  return getComputedStyle(document.documentElement).getPropertyValue(varName).trim();
};
========== FILE: docs/CONTRIBUTING.md ==========
# ðŸ¤ AzuraForge Platforma KatkÄ±da Bulunma Rehberi

AzuraForge projesine gÃ¶sterdiÄŸiniz ilgi ve katkÄ±larÄ±nÄ±z iÃ§in teÅŸekkÃ¼r ederiz! Bu rehber, kod tabanÄ±nÄ±n tutarlÄ±, okunabilir, sÃ¼rdÃ¼rÃ¼lebilir ve yÃ¼ksek kalitede kalmasÄ±nÄ± saÄŸlamak iÃ§in benimsediÄŸimiz Ã§alÄ±ÅŸma prensiplerini ve standartlarÄ±nÄ± aÃ§Ä±klamaktadÄ±r.

## ðŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

EÄŸer henÃ¼z geliÅŸtirme ortamÄ±nÄ±zÄ± kurmadÄ±ysanÄ±z, lÃ¼tfen platform repo'sunun ana `README.md` dosyasÄ±ndaki "GeliÅŸtirme OrtamÄ± Kurulumu" bÃ¶lÃ¼mÃ¼nÃ¼ takip edin.

## ðŸ› ï¸ Kodlama StandartlarÄ±

Projeye eklenen her kodun aÅŸaÄŸÄ±daki standartlarÄ± karÅŸÄ±lamasÄ± beklenmektedir:

1.  **Stil KÄ±lavuzu (PEP8 & Black):**
    *   TÃ¼m Python kodlarÄ±, PEP8 stil kurallarÄ±na uymalÄ±dÄ±r.
    *   Kodunuzu `black` ile otomatik formatlayÄ±n.
    *   **Kontrol:** DeÄŸiÅŸikliklerinizi gÃ¶ndermeden Ã¶nce `black <dosya_adÄ±>` veya `black .` komutunu Ã§alÄ±ÅŸtÄ±rÄ±n.
    *   **Otomasyon:** `pre-commit` hook'larÄ± otomatik formatlamayÄ± zorunlu kÄ±lar.

2.  **Linting (`flake8`):**
    *   TÃ¼m Python kodlarÄ±, `flake8` denetiminden hatasÄ±z geÃ§melidir.
    *   **Kontrol:** `flake8 <dosya_adÄ±>` veya `flake8 src` komutunu Ã§alÄ±ÅŸtÄ±rÄ±n.
    *   **Otomasyon:** `pre-commit` hook'larÄ± linting'i zorunlu kÄ±lar.

3.  **Tip Ä°puÃ§larÄ± (Type Hinting & Mypy):**
    *   TÃ¼m fonksiyon ve metod imzalarÄ±, parametreler ve dÃ¶nÃ¼ÅŸ deÄŸerleri iÃ§in tip ipuÃ§larÄ± (`typing` modÃ¼lÃ¼ kullanÄ±larak) iÃ§ermelidir.
    *   **Kontrol:** `mypy src` komutunu Ã§alÄ±ÅŸtÄ±rÄ±n.
    *   **Otomasyon:** `pre-commit` hook'larÄ± statik tip denetimini zorunlu kÄ±lar.

4.  **DokÃ¼mantasyon (Docstrings):**
    *   TÃ¼m modÃ¼ller, sÄ±nÄ±flar, fonksiyonlar ve metodlar, ne iÅŸe yaradÄ±klarÄ±nÄ±, aldÄ±klarÄ± argÃ¼manlarÄ± (`Args:`) ve ne dÃ¶ndÃ¼rdÃ¼klerini (`Returns:`) aÃ§Ä±klayan Google-style docstring'ler iÃ§ermelidir.
    *   ArayÃ¼z (API) repolarÄ± iÃ§in `FastAPI`'nin otomatik dokÃ¼mantasyonunu besleyecek docstring'ler kullanÄ±lmalÄ±dÄ±r.

5.  **Testler (`pytest`):**
    *   Eklenen her yeni Ã¶zellik veya fonksiyon iÃ§in ilgili birim testleri (`unit tests`) `tests/` klasÃ¶rÃ¼ne eklenmelidir.
    *   YapÄ±lan bir hata dÃ¼zeltmesi (bug fix) iÃ§in, o hatanÄ±n tekrar oluÅŸmasÄ±nÄ± engelleyecek bir regresyon testi yazÄ±lmalÄ±dÄ±r.
    *   **Kontrol:** Ä°lgili reponun kÃ¶k dizinindeyken `pytest` komutunu Ã§alÄ±ÅŸtÄ±rÄ±n.

## ðŸ“ Commit MesajlarÄ±

Commit mesajlarÄ±, yapÄ±lan deÄŸiÅŸikliÄŸi net bir ÅŸekilde aÃ§Ä±klamalÄ±dÄ±r ve [Conventional Commits](https://www.conventionalcommits.org/en/v1.0.0/) standardÄ±na uymalÄ±dÄ±r. Bu, otomatik versiyonlama ve deÄŸiÅŸiklik gÃ¼nlÃ¼ÄŸÃ¼ oluÅŸturmak iÃ§in hayati Ã¶nem taÅŸÄ±r.

**Format:**
```
<tip>(<kapsam>): <aÃ§Ä±klama>

[opsiyonel gÃ¶vde]
```

**Ã–rnek Tipler:**
*   `feat`: Yeni bir Ã¶zellik ekler (Minor versiyon artÄ±rÄ±mÄ±).
*   `fix`: Bir hata dÃ¼zeltmesi (Patch versiyon artÄ±rÄ±mÄ±).
*   `docs`: Sadece dokÃ¼mantasyon deÄŸiÅŸiklikleri (Versiyon artÄ±rÄ±mÄ± yok).
*   `style`: Kod formatÄ±, eksik noktalÄ± virgÃ¼l gibi stil dÃ¼zeltmeleri (Kod mantÄ±ÄŸÄ± deÄŸiÅŸmez, versiyon artÄ±rÄ±mÄ± yok).
*   `refactor`: Kodu yeniden yapÄ±landÄ±rma, davranÄ±ÅŸ deÄŸiÅŸikliÄŸi yok (Versiyon artÄ±rÄ±mÄ± yok).
*   `perf`: Performans iyileÅŸtirmesi yapan kod deÄŸiÅŸikliÄŸi (Versiyon artÄ±rÄ±mÄ± olabilir).
*   `test`: Eksik testlerin eklenmesi veya mevcut testlerin dÃ¼zeltilmesi (Versiyon artÄ±rÄ±mÄ± yok).
*   `build`: Build sistemi veya dÄ±ÅŸ baÄŸÄ±mlÄ±lÄ±k deÄŸiÅŸiklikleri (Versiyon artÄ±rÄ±mÄ± olabilir).
*   `ci`: CI/CD yapÄ±landÄ±rma dosyalarÄ± ve script'leri deÄŸiÅŸiklikleri (Versiyon artÄ±rÄ±mÄ± yok).

**Ã–rnekler:**
*   `feat(core): Add exponential function to Tensor`
*   `fix(api): Resolve 307 redirect issues for endpoints`
*   `docs(platform): Add initial development guide`
*   `refactor(learner): Separate Layer and Model definitions`

## ðŸ”„ Pull Request (PR) SÃ¼reci

1.  **Branch OluÅŸturma:** `main` branch'inden kendi feature branch'inizi (`feat/yeni-ozellik` veya `fix/hata-adi` gibi) oluÅŸturun.
2.  **DeÄŸiÅŸikliklerinizi YapÄ±n:** YukarÄ±daki standartlara uyduÄŸunuzdan emin olun.
3.  **Test Edin:** Yerel testlerinizi (`pytest`) Ã§alÄ±ÅŸtÄ±rÄ±n ve geÃ§tiÄŸinden emin olun.
4.  **Commit ve Push:** DeÄŸiÅŸikliklerinizi anlamlÄ± commit mesajlarÄ±yla branch'inize `push` edin.
5.  **Pull Request AÃ§Ä±n:** GitHub Ã¼zerinden `main` branch'ine bir "Pull Request" (PR) aÃ§Ä±n.
6.  **CI Kontrolleri:** PR'Ä±nÄ±zÄ±n CI kontrollerinden (testler, linting) baÅŸarÄ±yla geÃ§tiÄŸinden emin olun.
7.  **Kod Ä°ncelemesi:** Kodunuz incelenecek ve gerekli geri bildirimler saÄŸlanacaktÄ±r.

Bu standartlara uyarak, AzuraForge platformunun uzun vadede saÄŸlÄ±klÄ±, sÃ¼rdÃ¼rÃ¼lebilir ve yÃ¼ksek kalitede kalmasÄ±na yardÄ±mcÄ± olursunuz.

========== FILE: docs/DEVELOPMENT_GUIDE.md ==========
# ðŸ› ï¸ AzuraForge Platform GeliÅŸtirme Rehberi

Bu belge, AzuraForge platformunda geliÅŸtirme yapmak isteyenler iÃ§in adÄ±m adÄ±m kurulum, Ã§alÄ±ÅŸma prensipleri ve katkÄ±da bulunma yÃ¶nergelerini iÃ§erir.

## ðŸŽ¯ Temel Felsefemiz

Her repomuz, kendi baÅŸÄ±na yaÅŸayan, kurulabilir ve test edilebilir baÄŸÄ±msÄ±z bir Python/JavaScript paketidir. Repolar arasÄ± baÄŸÄ±mlÄ±lÄ±klar, Git adresleri (`@git+https://...`) Ã¼zerinden kurulur.

## ðŸ“¦ Proje RepolarÄ±na Genel BakÄ±ÅŸ

AzuraForge platformu, aÅŸaÄŸÄ±daki baÄŸÄ±msÄ±z GitHub depolarÄ±ndan oluÅŸur. GeliÅŸtirme yaparken bu repolarÄ±n bir kÄ±smÄ±nÄ± veya tamamÄ±nÄ± yerel makinenizde klonlamanÄ±z gerekecektir.

*   **`core`** ([link](https://github.com/AzuraForge/core)): Temel otomatik tÃ¼rev motoru.
*   **`learner`** ([link](https://github.com/AzuraForge/learner)): `core` Ã¼zerinde yÃ¼ksek seviyeli Ã¶ÄŸrenme kÃ¼tÃ¼phanesi.
*   **`app-stock-predictor`** ([link](https://github.com/AzuraForge/app-stock-predictor)): Bir uygulama eklentisi Ã¶rneÄŸi.
*   **`applications`** ([link](https://github.com/AzuraForge/applications)): Resmi uygulama katalogu (sadece JSON veri iÃ§erir).
*   **`api`** ([link](https://github.com/AzuraForge/api)): RESTful API ve WebSocket sunucusu.
*   **`worker`** ([link](https://github.com/AzuraForge/worker)): Arka plan gÃ¶revlerini (eÄŸitimleri) iÅŸleyen Celery worker.
*   **`dashboard`** ([link](https://github.com/AzuraForge/dashboard)): React tabanlÄ± web kullanÄ±cÄ± arayÃ¼zÃ¼.

## âš™ï¸ GeliÅŸtirme OrtamÄ± Kurulumu

Bu adÄ±mlar, platformun tÃ¼m parÃ§alarÄ±nÄ± yerel geliÅŸtirme iÃ§in hazÄ±r hale getirir.

1.  **Gerekli AraÃ§lar:**
    *   **Git:** RepolarÄ± klonlamak iÃ§in.
    *   **Python 3.8+:** TÃ¼m Python bileÅŸenleri iÃ§in.
    *   **Node.js & npm:** Frontend bileÅŸeni iÃ§in.
    *   **Docker Desktop:** Redis ve Dockerize edilmiÅŸ ortamda test iÃ§in (alternatifler belirtilecektir).

2.  **RepolarÄ± Klonlama:**
    Platformda geliÅŸtirmek iÃ§in tÃ¼m ilgili repolarÄ± aynÄ± seviyede bir klasÃ¶re klonlamanÄ±z Ã¶nerilir:
    ```bash
    mkdir azuraforge-dev
    cd azuraforge-dev

    git clone https://github.com/AzuraForge/core.git
    git clone https://github.com/AzuraForge/learner.git
    git clone https://github.com/AzuraForge/app-stock-predictor.git
    git clone https://github.com/AzuraForge/applications.git
    git clone https://github.com/AzuraForge/api.git
    git clone https://github.com/AzuraForge/worker.git
    git clone https://github.com/AzuraForge/dashboard.git
    git clone https://github.com/AzuraForge/platform.git # Orkestrasyon iÃ§in
    ```

3.  **Sanal Ortam Kurulumu (Python):**
    Her Python projesinin kendi sanal ortamÄ± olabilir veya merkezi bir tane kullanabiliriz. Yerel geliÅŸtirme iÃ§in, **`api` projesinin** kÃ¶k dizininde tek bir sanal ortam oluÅŸturmak ve tÃ¼m Python baÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± oraya kurmak en pratik yoldur.

    ```bash
    cd api # `api` reposunun iÃ§ine gir
    python -m venv .venv
    .\.venv\Scripts\activate # Windows iÃ§in
    # source ./.venv/bin/activate # Linux/macOS iÃ§in
    ```

4.  **Python BaÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± Kurma (TÃ¼m Python RepolarÄ± iÃ§in):**
    Sanal ortam aktifken, tÃ¼m Python repolarÄ±nÄ± "dÃ¼zenlenebilir" (editable) modda kurmalÄ±yÄ±z. Bu, kodda yaptÄ±ÄŸÄ±nÄ±z deÄŸiÅŸikliklerin anÄ±nda yansÄ±masÄ±nÄ± saÄŸlar. **`api` projesinin** kÃ¶k dizininde olduÄŸunuzdan emin olun.

    ```bash
    # Ã–nce en alt seviyeden baÅŸlayarak kÃ¼tÃ¼phaneleri kurun
    pip install -e ../core 
    pip install -e ../learner
    pip install -e ../app-stock-predictor # Ä°lk uygulama eklentisi
    pip install -e ../applications       # Uygulama katalogu
    
    # Sonra API ve Worker'Ä± kurun
    pip install -e .                     # `api` projesini kurar
    pip install -e ../worker             # `worker` projesini kurar
    ```
    Bu komutlar, her bir reponun `pyproject.toml` dosyasÄ±nÄ± okuyacak ve tÃ¼m baÄŸÄ±mlÄ±lÄ±k zincirini doÄŸru bir ÅŸekilde Ã§Ã¶zecektir.

5.  **JavaScript BaÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± Kurma (Dashboard iÃ§in):**
    ```bash
    cd ../dashboard # `dashboard` reposunun iÃ§ine gir
    npm install
    ```

6.  **Redis Kurulumu:**
    Platform, bir Redis sunucusuna ihtiyaÃ§ duyar. En kolay yol Docker kullanmaktÄ±r:
    ```bash
    docker run -d -p 6379:6379 --name azuraforge_redis redis
    ```

## â–¶ï¸ Servisleri Ã‡alÄ±ÅŸtÄ±rma (Yerel GeliÅŸtirme)

Sanal ortamÄ±nÄ±z aktifken ve Redis Ã§alÄ±ÅŸÄ±rken, her servisi ayrÄ± bir terminalde baÅŸlatÄ±n.

1.  **API Sunucusu (`api` reposundan):**
    ```bash
    cd api
    .\.venv\Scripts\activate # Sanal ortam aktif deÄŸilse
    start-api
    ```
    (TarayÄ±cÄ±da `http://localhost:8000/api/v1/docs` adresini kontrol edin.)

2.  **Worker Servisi (`worker` reposundan):**
    ```bash
    cd worker
    .\.venv\Scripts\activate
    start-worker
    ```
    (Worker terminalinde "Discovered pipeline..." loglarÄ±nÄ± kontrol edin.)

3.  **Dashboard (`dashboard` reposundan):**
    ```bash
    cd dashboard
    npm run dev
    ```
    (TarayÄ±cÄ±da `http://localhost:5173` adresini aÃ§Ä±n.)

## ðŸ§ª Test Etme ve Hata AyÄ±klama

*   **UÃ§tan Uca AkÄ±ÅŸ:** Dashboard'dan yeni bir deney baÅŸlatarak tÃ¼m sistemin (`Dashboard -> API -> Worker -> Uygulama Eklentisi -> KÃ¼tÃ¼phane`) sorunsuz Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± doÄŸrulayÄ±n. CanlÄ± takip ekranÄ±nÄ± ve kayÄ±p grafiÄŸini izleyin.
*   **API Testleri:** `http://localhost:8000/api/v1/docs` adresinden API endpoint'lerini test edin.
*   **Birim Testleri:** Her bir repoda (Ã¶rn: `core`, `learner`, `app-stock-predictor`) kendi `pytest` testlerini Ã§alÄ±ÅŸtÄ±rÄ±n.
    ```bash
    cd core # veya learner, app-stock-predictor
    .\.venv\Scripts\activate
    pytest
    ```
    (Bu testleri koÅŸabilmek iÃ§in ilgili repoda `pip install -e ".[dev]"` yapmÄ±ÅŸ olmanÄ±z gerekir.)

## ðŸ”„ Ä°teratif GeliÅŸtirme AkÄ±ÅŸÄ±

Ã‡oÄŸu zaman, kodda kÃ¼Ã§Ã¼k deÄŸiÅŸiklikler yapÄ±p bunlarÄ± hÄ±zla test etmek istersiniz.

1.  **KÃ¼tÃ¼phanede DeÄŸiÅŸiklik (Ã¶rn: `core/src/azuraforge_core/tensor.py`):**
    *   DeÄŸiÅŸikliÄŸi yapÄ±n ve kaydedin.
    *   Bu deÄŸiÅŸikliÄŸin `learner` veya diÄŸer kÃ¼tÃ¼phanelerde anÄ±nda etkili olmasÄ± iÃ§in **ekstra bir `pip install` komutuna GEREK YOKTUR**, Ã§Ã¼nkÃ¼ `pip install -e` ile kurulduklarÄ± iÃ§in doÄŸrudan kaynak dosyayÄ± kullanÄ±rlar.
    *   `core` projesine geri dÃ¶nÃ¼p `pytest` ile kendi testlerini koÅŸun.
    *   DeÄŸiÅŸikliÄŸi `commit`'leyin ve `push`'layÄ±n.

2.  **Uygulama/Servis DeÄŸiÅŸikliÄŸi (Ã¶rn: `app-stock-predictor/src/azuraforge_stockapp/pipeline.py`):**
    *   DeÄŸiÅŸikliÄŸi yapÄ±n ve kaydedin.
    *   `api` veya `worker` servisleri otomatik olarak `reload` (yeniden yÃ¼kleme) yapacaktÄ±r (eÄŸer `uvicorn --reload` ile Ã§alÄ±ÅŸÄ±yorlarsa).
    *   `api` ve `worker`'Ä± yeniden baÅŸlatmak genellikle yeterlidir.
    *   DeÄŸiÅŸikliÄŸi `commit`'leyin ve `push`'layÄ±n.

3.  **Yeni Bir BaÄŸÄ±mlÄ±lÄ±k EklendiÄŸinde (`pyproject.toml` deÄŸiÅŸtiÄŸinde):**
    *   Ä°lgili reponun kÃ¶k dizinine gidin (Ã¶rn: `api`).
    *   Sanal ortamÄ±nÄ±zÄ± aktive edin.
    *   `pip install -e .` komutunu tekrar Ã§alÄ±ÅŸtÄ±rÄ±n. `pip`, sadece eksik olan yeni baÄŸÄ±mlÄ±lÄ±klarÄ± ekleyecektir.

## ðŸ¤ KatkÄ±da Bulunma

Bu proje bir aÃ§Ä±k kaynak projesi olarak geliÅŸtirilmektedir. KatkÄ±da bulunmak iÃ§in lÃ¼tfen `platform/docs/CONTRIBUTING.md` dosyasÄ±nÄ± inceleyin.

========== FILE: docs/PROJECT_JOURNEY.md ==========
# ðŸ—ºï¸ Proje YolculuÄŸu: AzuraForge'un GeliÅŸim Hikayesi ve Gelecek Vizyonu

Bu belge, AzuraForge platformunun baÅŸlangÄ±cÄ±ndan mevcut durumuna kadar olan geliÅŸim sÃ¼recini, karÅŸÄ±laÅŸÄ±lan zorluklarÄ±, bulunan Ã§Ã¶zÃ¼mleri ve projenin **modÃ¼ler, Ã¶lÃ§eklenebilir ve ihtiyaÃ§ odaklÄ± bir yapay zeka geliÅŸtirme platformuna** dÃ¶nÃ¼ÅŸme vizyonunu Ã¶zetlemektedir.

## ðŸŽ¯ Proje Felsefesi ve Ã‡Ä±kÄ±ÅŸ Hikayesi

AzuraForge'un her aÅŸamasÄ±nda, kalitesini ve sÃ¼rdÃ¼rÃ¼lebilirliÄŸini saÄŸlamak iÃ§in ÅŸu temel prensipleri benimsedik:

1.  **SÄ±fÄ±rdan Ä°nÅŸa ve Tam Kontrol:** Temel algoritmalarÄ± ve yapÄ±larÄ± (`Tensor` gibi) mÃ¼mkÃ¼n olduÄŸunca sÄ±fÄ±rdan implemente ederek, sistem Ã¼zerinde tam kontrol saÄŸlamak ve derinlemesine Ã¶ÄŸrenmek.
2.  **ModÃ¼lerlik ve Sorumluluk AyrÄ±mÄ± (Microservices):** Her bileÅŸenin (Ã§ekirdek kÃ¼tÃ¼phane, API, worker, uygulama eklentisi, UI) tek ve net bir gÃ¶revi vardÄ±r ve kendi baÄŸÄ±msÄ±z repo'sunda yaÅŸar.
3.  **Olay GÃ¼dÃ¼mlÃ¼ Mimari:** BileÅŸenler arasÄ± iletiÅŸim, baÄŸÄ±mlÄ±lÄ±klarÄ± azaltmak ve gerÃ§ek zamanlÄ± yetenekler saÄŸlamak iÃ§in olaylar (Celery, Redis Pub/Sub, WebSockets) Ã¼zerinden gerÃ§ekleÅŸir.
4.  **Eklenti TabanlÄ± (Plug-in Architecture):** Yeni Ã¶zellikler ve uygulamalar, mevcut platform koduna dokunmadan birer eklenti olarak kolayca eklenebilir.
5.  **KanÄ±t OdaklÄ± GeliÅŸtirme:** Her bÃ¼yÃ¼k Ã¶zellik veya yetenek, gerÃ§ek dÃ¼nya verisi Ã¼zerinde kabul edilebilir bir performansla kanÄ±tlanmalÄ±dÄ±r.
6.  **Otomatik Kalite KontrolÃ¼:** Kod kalitesi (linting, type checking, unit tests) ve versiyonlama sÃ¼reÃ§leri (CI/CD) otomatikleÅŸtirilmiÅŸtir.

## âœ… Tamamlanan Fazlar ve Elde Edilen BaÅŸarÄ±lar

### Faz 0: Fikir ve Ä°lk Denemeler (Monolitik YaklaÅŸÄ±m)

*   **DÃ¼ÅŸÃ¼nce:** Mevcut ML araÃ§larÄ±nÄ±n karmaÅŸÄ±klÄ±ÄŸÄ±na ve baÄŸÄ±mlÄ±lÄ±klarÄ±na bir tepki olarak, sÄ±fÄ±rdan bir derin Ã¶ÄŸrenme motoru (`mininn`) inÅŸa etme fikri doÄŸdu.
*   **Ä°lk Uygulama:** Hava durumu tahmini ve hisse senedi tahmini gibi basit uygulamalarla `mininn`'in yetenekleri test edildi.
*   **Ã–ÄŸrenilen Ders:** Monolitik bir yaklaÅŸÄ±mla (her ÅŸey tek bir repo'da) hÄ±zlÄ± prototipleme mÃ¼mkÃ¼n olsa da, Ã¶lÃ§eklenebilirlik ve yÃ¶netim zorluklarÄ± ortaya Ã§Ä±ktÄ±.

### Faz 1: Multi-Repo ve Mikroservis Mimarisine GeÃ§iÅŸ

*   **Karar:** Uzun vadeli sÃ¼rdÃ¼rÃ¼lebilirlik, Ã¶lÃ§eklenebilirlik ve profesyonellik iÃ§in, platformu baÄŸÄ±msÄ±z repolara sahip bir mikroservis mimarisine dÃ¶nÃ¼ÅŸtÃ¼rme kararÄ± alÄ±ndÄ±.
*   **Zorluk:** Python'da Ã§oklu repolar arasÄ± baÄŸÄ±mlÄ±lÄ±k yÃ¶netimi ve yol (path) sorunlarÄ±.
*   **Ã‡Ã¶zÃ¼m:** `pip`'in `editable` kurulumu (`-e`) ve `git+https` baÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± kullanarak, her reponun kendi `pyproject.toml` ve `setup.py` dosyalarÄ±yla kurulabilir bir paket olmasÄ± saÄŸlandÄ±. `importlib.resources` ile paket iÃ§i dosya eriÅŸimi Ã§Ã¶zÃ¼ldÃ¼.

### Faz 2: Temel KÃ¼tÃ¼phanelerin Ä°nÅŸasÄ± ve KanÄ±tÄ±

*   **`AzuraForge/core` (Matematik Motoru):** `Tensor` objesi ve otomatik tÃ¼rev yetenekleri sÄ±fÄ±rdan inÅŸa edildi. `pytest` ile birim testleri (dot, sum, add, mul, relu backward pass) baÅŸarÄ±yla yazÄ±ldÄ± ve geÃ§ti. **`to_cpu` ve `_unbroadcast_to` hatalarÄ± bu fazda tespit edilip dÃ¼zeltildi.**
*   **`AzuraForge/learner` (Ã–ÄŸrenme KÃ¼tÃ¼phanesi):** `azuraforge-core`'a baÄŸÄ±mlÄ± olarak `Layer`, `Linear`, `Loss`, `MSELoss`, `Sequential`, `Optimizer`, `SGD` gibi temel Ã¶ÄŸrenme bileÅŸenleri inÅŸa edildi. `pytest` ile basit regresyon testi (`test_learner_fit_simple_regression`) baÅŸarÄ±yla geÃ§ti.

### Faz 3: DaÄŸÄ±tÄ±k Servisler ve Eklenti Mimarisi

*   **`AzuraForge/applications` (Katalog):** Uygulama eklentilerinin JSON kataloÄŸunu barÄ±ndÄ±ran basit bir Python paketi olarak yapÄ±landÄ±rÄ±ldÄ±.
*   **`AzuraForge/app-stock-predictor` (Ä°lk Eklenti):** `azuraforge-learner`'Ä± kullanan ve platforma `entry_points` (`azuraforge.pipelines`) ile kendini tanÄ±tan ilk uygulama eklentisi inÅŸa edildi.
*   **`AzuraForge/worker` (Ä°ÅŸÃ§i Servisi):** `celery[redis]` kullanarak arka plan gÃ¶revlerini iÅŸleyen ve `importlib.metadata` ile sisteme kurulu tÃ¼m `azuraforge.pipelines` eklentilerini **otomatik olarak keÅŸfeden** ve Ã§alÄ±ÅŸtÄ±ran worker servisi kuruldu.
*   **`AzuraForge/api` (API Servisi):** `FastAPI` ile RESTful API endpoint'leri (`/experiments`, `/pipelines`) sunan ve `worker`'a gÃ¶rev gÃ¶nderen iletiÅŸim katmanÄ± inÅŸa edildi. API rotalarÄ±nÄ±n `prefix` yÃ¶netimi ve `307 Redirect` sorunlarÄ± bu fazda Ã§Ã¶zÃ¼ldÃ¼.
*   **BaÅŸarÄ±:** `api` Ã¼zerinden gÃ¶nderilen bir "stock_predictor" gÃ¶revinin, `worker` tarafÄ±ndan alÄ±nÄ±p, `app-stock-predictor` eklentisinin keÅŸfedilip, `learner` kÃ¼tÃ¼phanesi kullanÄ±larak **gerÃ§ek bir model eÄŸitiminin baÅŸarÄ±yla tamamlandÄ±ÄŸÄ±** kanÄ±tlandÄ±.

### Faz 4: KullanÄ±cÄ± ArayÃ¼zÃ¼ ve CanlÄ± Takip

*   **`AzuraForge/dashboard` (Web UI):** React tabanlÄ±, `api` servisinden deney ve pipeline listelerini Ã§eken, yeni deneyler baÅŸlatmayÄ± saÄŸlayan temel bir web arayÃ¼zÃ¼ inÅŸa edildi.
*   **CanlÄ± Takip (WebSocket Entegrasyonu):**
    *   `worker`, eÄŸitim sÄ±rasÄ±nda `Celery task.update_state` ile ilerleme durumunu Redis'e raporladÄ±.
    *   `api`, `FastAPI WebSocket` endpoint'i Ã¼zerinden bu ilerlemeyi `dashboard`'a anlÄ±k olarak iletti.
    *   `dashboard`, gelen `PROGRESS` mesajlarÄ±yla bir **ilerleme Ã§ubuÄŸunu ve kayÄ±p grafiÄŸini canlÄ± olarak gÃ¼ncelledi.**

**An itibarÄ±yla AzuraForge Platform 1.0, tÃ¼m temel mimarisi ve uÃ§tan uca Ã§alÄ±ÅŸan canlÄ± takip yetenekleriyle TAMAMLANMIÅžTIR!**

## ðŸ—ºï¸ Gelecek Fazlar ve Yol HaritasÄ±

Bu saÄŸlam temel Ã¼zerine inÅŸa edilecek adÄ±mlar, AzuraForge'u daha da zenginleÅŸtirmeyi ve kapsamÄ±nÄ± geniÅŸletmeyi hedefleyecektir.

### Faz 5: Deney YÃ¶netimini DerinleÅŸtirme

*   **KalÄ±cÄ± SonuÃ§lar:** `worker`'Ä±n `results.json`'a kaydettiÄŸi tÃ¼m detaylÄ± veriyi (eÄŸitim geÃ§miÅŸi, metrikler, konfigÃ¼rasyon) `api` Ã¼zerinden okuyup Dashboard'da gÃ¶rselleÅŸtirme.
*   **Deney Detay SayfasÄ±:** Dashboard'da her deney iÃ§in ayrÄ± bir detay sayfasÄ± oluÅŸturma.
*   **Model YÃ¶netimi:** EÄŸitilen modellerin kaydedilmesi, listelenmesi ve daha sonra Ã§Ä±karÄ±m iÃ§in yÃ¼klenebilmesi.

### Faz 6: Yeni Veri Modalitelerine AÃ§Ä±lÄ±m (GÃ¶rÃ¼ntÃ¼ Ä°ÅŸleme)

*   **`core` GeniÅŸletme:** `Conv2D`, `MaxPool2D`, `Flatten` gibi CNN katmanlarÄ±nÄ± `core` kÃ¼tÃ¼phanesine ekleme.
*   **Yeni Uygulama Eklentisi:** `azuraforge-app-image-classifier` (Ã¶rn: MNIST iÃ§in) oluÅŸturma.

### Faz 7: Hiperparametre Optimizasyonu

*   **`azuraforge-hyper-tuner`:** FarklÄ± hiperparametre kombinasyonlarÄ±yla otomatik deneyler yapabilen yeni bir uygulama eklentisi.
*   **Dashboard Entegrasyonu:** Dashboard'dan hiperparametre optimizasyonu iÅŸleri baÅŸlatma.

### Faz 8: Ãœretim OrtamÄ± HazÄ±rlÄ±ÄŸÄ± (Deployment)

*   **`platform` Orkestrasyonu:** `docker-compose.yml`'Ä± daha saÄŸlam hale getirme (Nginx, HTTPS, Load Balancing).
*   **CI/CD Pipeline'larÄ±:** TÃ¼m repolar iÃ§in otomatik test, versiyonlama ve yayÄ±nlama (PyPI/GitHub Packages) pipeline'larÄ± kurma.

========== FILE: learner/pyproject.toml ==========
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "azuraforge-learner"
version = "0.1.1"
authors = [{ name = "Azmi Sahin" }]
description = "High-level deep learning library for model training and management, using the AzuraForge Core engine."
readme = "README.md"
requires-python = ">=3.8"
license = { text = "MIT" }
dependencies = [
    # Ã‡ekirdek motorumuza olan baÄŸÄ±mlÄ±lÄ±ÄŸÄ±mÄ±z.
    "azuraforge-core @ git+https://github.com/AzuraForge/core.git@main",
    "scikit-learn",
    "numpy" # scikit-learn iÃ§in explicit belirtmek iyi bir pratik
]

[project.optional-dependencies]
dev = ["pytest"]

========== FILE: learner/README.md ==========
# AzuraForge Learner ðŸ§ 

**AzuraForge Learner**, `azuraforge-core` motorunu kullanarak modelleri kolayca oluÅŸturmak, eÄŸitmek ve yÃ¶netmek iÃ§in tasarlanmÄ±ÅŸ yÃ¼ksek seviyeli bir kÃ¼tÃ¼phanedir.

## Kurulum

```bash
pip install azuraforge-learner@git+https://github.com/AzuraForge/learner.git
```

========== FILE: learner/setup.py ==========
from setuptools import setup, find_packages
setup(
    package_dir={"": "src"},
    packages=find_packages(where="src"),
)

========== FILE: learner/src/azuraforge_learner/callbacks.py ==========
import os
import numpy as np
from .events import Event

# 'Learner' sÄ±nÄ±fÄ± henÃ¼z tanÄ±mlanmadÄ±ÄŸÄ± iÃ§in ileriye dÃ¶nÃ¼k referans (forward reference) kullanÄ±yoruz
from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from .learner import Learner

class Callback:
    """
    TÃ¼m callback'lerin temel sÄ±nÄ±fÄ±. OlaylarÄ± dinler ve ilgili metoda yÃ¶nlendirir.
    """
    def __call__(self, event: Event):
        method = getattr(self, f"on_{event.name}", None)
        if method:
            method(event)

    def on_train_begin(self, event: Event): pass
    def on_train_end(self, event: Event): pass
    def on_epoch_begin(self, event: Event): pass
    def on_epoch_end(self, event: Event): pass

class ModelCheckpoint(Callback):
    """Her epoch sonunda performansÄ± izler ve sadece en iyi modeli kaydeder."""
    def __init__(self, filepath: str, monitor: str = "val_loss", mode: str = "min", verbose: int = 1):
        self.filepath = filepath
        self.monitor = monitor
        self.mode = mode
        self.verbose = verbose
        self.best = np.inf if mode == "min" else -np.inf

        # DosyanÄ±n kaydedileceÄŸi dizini oluÅŸtur
        dir_path = os.path.dirname(self.filepath)
        if dir_path:
            os.makedirs(dir_path, exist_ok=True)

    def on_epoch_end(self, event: Event):
        current_val = event.payload.get(self.monitor)
        if current_val is None:
            if event.payload.get("epoch") == 0 and self.verbose > 0:
                print(f"ModelCheckpoint Warning: Can't find metric '{self.monitor}' to save model.")
            return

        is_better = (self.mode == "min" and current_val < self.best) or \
                    (self.mode == "max" and current_val > self.best)

        if is_better:
            if self.verbose > 0:
                print(f"ModelCheckpoint: {self.monitor} improved from {self.best:.6f} to {current_val:.6f}. Saving model to {self.filepath}")
            self.best = current_val
            event.learner.save(self.filepath)

class EarlyStopping(Callback):
    """Performans belirli bir epoch sayÄ±sÄ± boyunca iyileÅŸmediÄŸinde eÄŸitimi durdurur."""
    def __init__(self, monitor: str = "val_loss", patience: int = 10, mode: str = "min", verbose: int = 1):
        self.monitor = monitor
        self.patience = patience
        self.mode = mode
        self.verbose = verbose
        self.wait = 0
        self.best = np.inf if mode == "min" else -np.inf

    def on_train_begin(self, event: Event):
        # EÄŸitimin baÅŸÄ±nda sayaÃ§larÄ± sÄ±fÄ±rla
        self.wait = 0
        self.best = np.inf if self.mode == "min" else -np.inf

    def on_epoch_end(self, event: Event):
        current_val = event.payload.get(self.monitor)
        if current_val is None:
            return
            
        is_better = (self.mode == "min" and current_val < self.best) or \
                    (self.mode == "max" and current_val > self.best)

        if is_better:
            self.best = current_val
            self.wait = 0
        else:
            self.wait += 1
            if self.wait >= self.patience:
                if self.verbose > 0:
                    print(f"EarlyStopping: Stopping training. {self.monitor} did not improve for {self.patience} epochs.")
                event.learner.stop_training = True

========== FILE: learner/src/azuraforge_learner/events.py ==========
from dataclasses import dataclass, field
from typing import Dict, Any, Literal, TYPE_CHECKING

if TYPE_CHECKING:
    from .learner import Learner

EventName = Literal["train_begin", "train_end", "epoch_begin", "epoch_end"]

@dataclass
class Event:
    name: EventName
    learner: 'Learner'
    payload: Dict[str, Any] = field(default_factory=dict)

========== FILE: learner/src/azuraforge_learner/layers.py ==========
from typing import List
import numpy as np
from azuraforge_core import Tensor, xp

class Layer:
    def forward(self, x: Tensor) -> Tensor: raise NotImplementedError
    def parameters(self) -> List[Tensor]: return []
    def __call__(self, x: Tensor) -> Tensor: return self.forward(x)

class Linear(Layer):
    def __init__(self, input_dim: int, output_dim: int):
        limit = np.sqrt(2.0 / input_dim)
        self.weights = Tensor(xp.random.randn(input_dim, output_dim) * limit, requires_grad=True)
        self.bias = Tensor(xp.zeros(output_dim), requires_grad=True)
    def forward(self, x: Tensor) -> Tensor:
        return x.dot(self.weights) + self.bias
    def parameters(self) -> List[Tensor]:
        return [self.weights, self.bias]

class ReLU(Layer):
    def forward(self, x: Tensor) -> Tensor:
        return x.relu()

# YENÄ°: Sigmoid KatmanÄ±
class Sigmoid(Layer):
    def forward(self, x: Tensor) -> Tensor:
        return x.sigmoid()
========== FILE: learner/src/azuraforge_learner/learner.py ==========
import pickle
from typing import Any, Dict, List, Optional
import numpy as np
from azuraforge_core import Tensor
from .events import Event
from .models import Sequential
from .losses import Loss
from .optimizers import Optimizer
from .callbacks import Callback

class Learner:
    def __init__(self, model: Sequential, criterion: Loss, optimizer: Optimizer, callbacks: Optional[List[Callback]] = None, current_task: Optional[Any] = None):
        self.model = model
        self.criterion = criterion
        self.optimizer = optimizer
        self.callbacks = callbacks or []
        self.history: Dict[str, List[float]] = {}
        self.stop_training: bool = False
        self.current_task = current_task # Celery Task referansÄ±

    def _publish(self, event_name: str, payload: Optional[Dict[str, Any]] = None):
        event = Event(name=event_name, learner=self, payload=payload or {})
        for cb in self.callbacks: cb(event)
        
        # --- KRÄ°TÄ°K DÃœZELTME: Celery Task durumunu gÃ¼ncelle ---
        if self.current_task and hasattr(self.current_task, 'update_state'):
            # Celery'ye PROGRESS durumu ve meta verileri gÃ¶nder
            # payload, epoch_logs'u iÃ§eriyor
            self.current_task.update_state(state='PROGRESS', meta=payload)

    def fit(self, X_train: np.ndarray, y_train: np.ndarray, epochs: int):
        self.history = {} # Her fit Ã§aÄŸrÄ±sÄ±nda geÃ§miÅŸi sÄ±fÄ±rla
        
        # History'yi baÅŸlat
        for key in ["loss", "val_loss", "val_r2"]: # EkleyeceÄŸimiz metrikler iÃ§in yer aÃ§alÄ±m
             self.history[key] = []

        X_train_t, y_train_t = Tensor(X_train), Tensor(y_train)
        
        self._publish("train_begin", payload={"total_epochs": epochs}) # Toplam epoch sayÄ±sÄ±nÄ± gÃ¶nder
        for epoch in range(epochs):
            if self.stop_training: break
            
            # Epoch baÅŸlangÄ±cÄ± olayÄ±nÄ± yayÄ±nla
            self._publish("epoch_begin", payload={"epoch": epoch, "total_epochs": epochs})
            
            # EÄŸitim adÄ±mÄ±
            y_pred = self.model(X_train_t)
            loss = self.criterion(y_pred, y_train_t)
            
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
            
            epoch_logs = {
                "epoch": epoch + 1, # Epoch sayÄ±sÄ±nÄ± 1'den baÅŸlatalÄ±m
                "total_epochs": epochs,
                "loss": loss.data.item(),
                "status_text": f"Epoch {epoch+1}/{epochs} completed..."
            }
            
            # History'ye ekle
            self.history["loss"].append(epoch_logs["loss"])
            
            # Epoch sonu olayÄ±nÄ± yayÄ±nla (payload olarak tÃ¼m epoch loglarÄ±nÄ± gÃ¶nder)
            self._publish("epoch_end", payload=epoch_logs)
        self._publish("train_end")
        return self.history

    def predict(self, X_test: np.ndarray) -> np.ndarray:
        return self.model(Tensor(X_test)).to_cpu()

    def evaluate(self, X_val: np.ndarray, y_val: np.ndarray) -> Dict[str, float]:
        from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
        y_val_t = Tensor(y_val)
        y_pred = self.model(Tensor(X_val))
        
        val_loss = self.criterion(y_pred, y_val_t).data.item()
        y_pred_np = y_pred.to_cpu()
        
        val_r2 = r2_score(y_val, y_pred_np)
        val_mae = mean_absolute_error(y_val, y_pred_np)
        val_rmse = np.sqrt(mean_squared_error(y_val, y_pred_np))

        return {"val_loss": val_loss, "val_r2": val_r2, "val_mae": val_mae, "val_rmse": val_rmse}

========== FILE: learner/src/azuraforge_learner/losses.py ==========
from azuraforge_core import Tensor

class Loss:
    def __call__(self, y_pred: Tensor, y_true: Tensor) -> Tensor: raise NotImplementedError

class MSELoss(Loss):
    def __call__(self, y_pred: Tensor, y_true: Tensor) -> Tensor:
        return ((y_pred - y_true) ** 2).mean()

========== FILE: learner/src/azuraforge_learner/models.py ==========
from typing import List
from .layers import Layer
from azuraforge_core import Tensor

class Sequential(Layer):
    def __init__(self, *layers: Layer):
        self.layers = list(layers)
    def forward(self, x: Tensor) -> Tensor:
        for layer in self.layers:
            x = layer(x)
        return x
    def parameters(self) -> List[Tensor]:
        return [p for layer in self.layers for p in layer.parameters()]

========== FILE: learner/src/azuraforge_learner/optimizers.py ==========
from typing import List
from azuraforge_core import Tensor

class Optimizer:
    def __init__(self, params: List[Tensor], lr: float):
        self.params = [p for p in params if p.requires_grad]
        self.lr = lr
    def step(self) -> None: raise NotImplementedError
    def zero_grad(self) -> None:
        for p in self.params:
            if p.grad is not None: p.grad.fill(0.0)

class SGD(Optimizer):
    def step(self) -> None:
        for p in self.params:
            if p.grad is not None: p.data -= self.lr * p.grad

========== FILE: learner/src/azuraforge_learner/__init__.py ==========
from .events import Event
from .callbacks import Callback, ModelCheckpoint, EarlyStopping
from .losses import Loss, MSELoss
from .layers import Layer, Linear, ReLU, Sigmoid # Sigmoid eklendi
from .models import Sequential
from .optimizers import Optimizer, SGD
from .learner import Learner

__all__ = [
    "Event", "Callback", "ModelCheckpoint", "EarlyStopping",
    "Loss", "MSELoss", "Layer", "Linear", "ReLU", "Sigmoid", # Sigmoid eklendi
    "Sequential", "Optimizer", "SGD", "Learner",
]
========== FILE: learner/src/azuraforge_learner/utils/__init__.py ==========

========== FILE: learner/tests/azuraforge_learner/test_learner_components.py ==========
import pytest
import numpy as np

from azuraforge_learner import Learner, Sequential, Linear, ReLU, MSELoss, SGD

def test_learner_fit_simple_regression():
    X_train = np.array([[-1.0], [0.0], [1.0], [2.0]], dtype=np.float32)
    y_train = np.array([[-1.0], [1.0], [3.0], [5.0]], dtype=np.float32)
    
    model = Sequential(Linear(1, 1))
    criterion = MSELoss()
    optimizer = SGD(model.parameters(), lr=0.1)
    learner = Learner(model, criterion, optimizer)
    
    initial_loss = learner.evaluate(X_train, y_train)['val_loss']
    
    learner.fit(X_train, y_train, epochs=30)
    
    final_loss = learner.history['loss'][-1]
    
    print(f"Initial Loss: {initial_loss}, Final Loss: {final_loss}")
    assert final_loss < initial_loss / 5

def test_sequential_model_forward_pass():
    model = Sequential(Linear(2, 4), ReLU(), Linear(4, 1))
    from azuraforge_core import Tensor
    
    input_tensor = Tensor(np.random.randn(10, 2))
    output_tensor = model(input_tensor)
    
    assert output_tensor.data.shape == (10, 1)

========== FILE: tools/snapshot_generator.py ==========
import os
import sys
import json
import argparse
from typing import List, Dict, Any, Set, Optional, Tuple
import re

# Configuration for included/excluded paths and extensions
DEFAULT_INCLUDE_DIRS = ["."]
DEFAULT_INCLUDE_EXTENSIONS = [
    ".toml",
    ".py",
    ".yaml",
    ".yml",
    ".json",
    ".md",
    ".txt",
    "html",
    ".bat",
    ".sh",
    ".jsx",
    ".js",
    ".json",
    ".css"    
]
DEFAULT_EXCLUDE_PATTERNS = [
    "__pycache__",
    ".git",
    ".venv",
    ".vscode",
    ".idea",
    "build",
    "dist",
    "*.egg-info",
    "*.pyc",
    "*.so",
    "*.pyd",
    ".pytest_cache",
    ".mypy_cache",
    ".dataset",
    "dataset",
    ".logs",
    "logs",
    ".output",
    "output",
    "inputs",
    "outputs",
    ".tmp",
    "checkpoints",
    "reports",
    "docs/_build",
    "site",
    "node_modules",
    ".DS_Store",
    "Thumbs.db", # Windows thumbnail cache
    "*.lock", # npm lock dosyalarÄ± gibi
]

FILE_HEADER_TEMPLATE = "========== FILE: {file_path} =========="
SNAPSHOT_INFO_TEMPLATE = """PROJE KOD SNAPSHOT (TAM)
Toplam {total_files_placeholder} dosya bulundu ve eklendi.
Dahil Edilen Dizinler: {included_dirs_placeholder}
Dahil Edilen UzantÄ±lar: {included_extensions_placeholder}
HariÃ§ Tutulan Desenler/Yollar: {excluded_patterns_placeholder}
================================================================================
"""

def clean_code_comments(content: str, file_extension: str) -> str:
    """Removes most comments from code, attempting to preserve shebangs and type hints."""
    if file_extension not in [".py", ".sh", ".bat"]: return content
    lines = content.splitlines()
    cleaned_lines = []
    for line in lines:
        stripped_line = line.strip()
        if file_extension == ".py":
            # Preserve special comments like '# type:' and shebangs
            if stripped_line.startswith("# type:") or stripped_line.startswith("# noqa"): 
                cleaned_lines.append(line)
            elif stripped_line.startswith("#!/"): 
                cleaned_lines.append(line)
            # Remove inline comments
            elif "#" in line and not stripped_line.startswith("#"): 
                cleaned_lines.append(line.split("#", 1)[0].rstrip())
            # Remove full-line comments
            elif stripped_line.startswith("#"):
                continue # Skip full line comments
            else: 
                cleaned_lines.append(line)
        elif file_extension == ".sh":
            if stripped_line.startswith("#!/"): 
                cleaned_lines.append(line)
            elif not stripped_line.startswith("#"): 
                cleaned_lines.append(line)
        elif file_extension == ".bat":
            if not stripped_line.lower().startswith("rem "): 
                cleaned_lines.append(line)
        else: 
            cleaned_lines.append(line)
    return "\n".join(cleaned_lines)


def should_exclude(item_path: str, root_path: str, exclude_patterns: List[str]) -> bool:
    """Checks if a file or directory should be excluded based on the patterns."""
    normalized_item_path = os.path.normpath(item_path)
    normalized_root_path = os.path.normpath(os.path.abspath(root_path))
    
    try:
        relative_item_path = os.path.relpath(normalized_item_path, normalized_root_path)
    except ValueError:
        # If item_path is not relative to root_path (e.g., different drive on Windows)
        # or other path normalization issues, treat it as its own path.
        relative_item_path = normalized_item_path
    
    relative_item_path_slashes = relative_item_path.replace(os.sep, "/")

    for pattern in exclude_patterns:
        normalized_pattern = os.path.normpath(pattern)
        normalized_pattern_slashes = normalized_pattern.replace(os.sep, "/")

        # Wildcard extensions like "*.pyc"
        if pattern.startswith("*."):
            if relative_item_path_slashes.endswith(pattern[1:]): return True
        # Directory names or file names without path
        elif "/" not in pattern and "." not in pattern and not pattern.startswith("*"):
            path_segments = relative_item_path_slashes.split("/")
            if pattern in path_segments: return True
        # Exact file name match
        elif pattern == os.path.basename(normalized_item_path): return True
        # Full path prefix match or relative path match
        elif normalized_item_path.startswith(os.path.join(normalized_root_path, normalized_pattern)) or \
             relative_item_path_slashes.startswith(normalized_pattern_slashes): return True
        # Absolute path match
        elif os.path.isabs(normalized_pattern) and normalized_pattern == normalized_item_path: return True
    return False


def collect_project_files_full(
    output_file: str,
    include_dirs: Optional[List[str]] = None,
    include_extensions: Optional[List[str]] = None,
    exclude_patterns: Optional[List[str]] = None,
    base_dir: str = ".",
    clean_comments: bool = False,
) -> None:
    if include_dirs is None: include_dirs = DEFAULT_INCLUDE_DIRS
    if include_extensions is None: include_extensions = DEFAULT_INCLUDE_EXTENSIONS
    if exclude_patterns is None: exclude_patterns = DEFAULT_EXCLUDE_PATTERNS

    abs_base_dir = os.path.abspath(base_dir)
    
    snapshot_content_header = SNAPSHOT_INFO_TEMPLATE.format(
        total_files_placeholder="{total_files_counter}",
        included_dirs_placeholder=", ".join(include_dirs),
        included_extensions_placeholder=", ".join(include_extensions),
        excluded_patterns_placeholder=", ".join(exclude_patterns),
    )

    all_found_relative_paths: Set[str] = set() # This set stores relative paths to prevent duplicates
    content_parts: List[str] = [snapshot_content_header]
    processed_files_count = 0

    for inc_dir_pattern in include_dirs:
        current_scan_dir = os.path.abspath(os.path.join(abs_base_dir, inc_dir_pattern))
        if not os.path.exists(current_scan_dir):
            print(f"Warning: Include directory '{inc_dir_pattern}' (resolved to '{current_scan_dir}') does not exist. Skipping.")
            continue

        for root, dirs, files in os.walk(current_scan_dir, topdown=True):
            # Filter directories in-place to prevent os.walk from entering excluded ones
            dirs[:] = [
                d for d in dirs
                if not should_exclude(os.path.join(root, d), abs_base_dir, exclude_patterns)
            ]
            for file_name in files:
                file_path = os.path.join(root, file_name)

                relative_file_path = os.path.relpath(file_path, abs_base_dir)
                display_path = relative_file_path.replace(os.sep, "/")

                # Skip if already processed (e.g., if included by multiple patterns)
                if display_path in all_found_relative_paths:
                    continue 

                # Apply exclusion patterns to files
                if should_exclude(file_path, abs_base_dir, exclude_patterns):
                    continue

                _, file_extension = os.path.splitext(file_name)
                # For extension check, handle files without an explicit extension (like Dockerfile)
                name_part_for_ext_check = file_extension.lower() if file_extension else file_name.lower()

                # Check if file extension (or full name for extensionless files) is in include list
                if any(name_part_for_ext_check.endswith(ext.lower()) for ext in include_extensions) or \
                   (not file_extension and file_name.lower() in [ext.lower() for ext in include_extensions if not ext.startswith('.')]):
                    
                    all_found_relative_paths.add(display_path) # Add to set of found paths
                    try:
                        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                            file_content = f.read()
                        
                        if clean_comments:
                            file_content = clean_code_comments(file_content, file_extension)
                        
                        # Add a leading newline for separator, then the header, then content.
                        # This creates the format: \n========== FILE:PATH==========\nCONTENT
                        content_parts.append(f"\n{FILE_HEADER_TEMPLATE.format(file_path=display_path)}\n")
                        content_parts.append(file_content)
                        processed_files_count += 1
                    except Exception as e:
                        print(f"Error reading file {relative_file_path}: {e}")
                        content_parts.append(f"\nError reading file {relative_file_path}: {e}\n")

    final_header_with_count = content_parts[0].replace("{total_files_counter}", str(processed_files_count))
    content_parts[0] = final_header_with_count

    with open(output_file, "w", encoding="utf-8") as f:
        f.write("".join(content_parts))

    print(f"Project snapshot (full) generated: {output_file}")
    print(f"Total {processed_files_count} files included.")


def restore_from_full_snapshot(
    snapshot_file: str,
    target_dir: str = ".",
    dry_run: bool = False,
    overwrite_existing: bool = False,
) -> None:
    print(f"Restoring project from snapshot: {snapshot_file}")
    if dry_run: print("DRY RUN: No files will be written.")

    try:
        with open(snapshot_file, "r", encoding="utf-8") as f:
            full_content = f.read()
    except FileNotFoundError:
        print(f"Error: Snapshot file '{snapshot_file}' not found.")
        return
    except Exception as e:
        print(f"Error reading snapshot file: {e}")
        return

    # Regex to find file blocks. It captures the file path (group 1) and its content (group 2).
    # The crucial point is that group(2) captures ALL characters (including newlines due to re.DOTALL)
    # after the header line's trailing newline, until the START of the next file header or end of string.
    # The lookahead `(?=...)` is non-consuming, so the content is captured completely.
    file_block_pattern = re.compile(
        r"^========== FILE: (.*?) ==========\n"  # Match header line and its trailing newline
        r"(.*?)"                                 # Capture content (non-greedy)
        r"(?=\n========== FILE: |\Z)",           # Lookahead: followed by newline then next header, OR end of string.
                                                # \Z matches only at the end of the string.
        re.MULTILINE | re.DOTALL
    )
    
    # Find the end of the initial info header to start parsing file blocks
    info_header_last_line = SNAPSHOT_INFO_TEMPLATE.splitlines()[-1]
    content_start_index = full_content.find(info_header_last_line)
    if content_start_index == -1:
        print("Error: Could not find the end of the snapshot info header.")
        return
    
    # Slice the content to start exactly after the info header,
    # and then lstrip any *leading* newlines that might be left before the first file block.
    # This ensures the regex for the first file block can match correctly.
    content_to_parse = full_content[content_start_index + len(info_header_last_line):].lstrip('\n')

    files_restored = 0
    files_skipped = 0
    files_overwritten = 0
    
    matches = file_block_pattern.finditer(content_to_parse)

    for match in matches:
        relative_file_path = match.group(1).strip()
        # KRÄ°TÄ°K DÃœZELTME: match.group(2) Ã¼zerinde .strip() metodunu kaldÄ±rdÄ±k.
        # Bu, tÃ¼m boÅŸluk karakterlerinin (yeni satÄ±rlar dahil) korunmasÄ±nÄ± saÄŸlar.
        content_part = match.group(2) 

        os_specific_relative_path = relative_file_path.replace("/", os.sep)
        target_file_path = os.path.join(target_dir, os_specific_relative_path)
        
        # DEBUG YARDIMI: YazÄ±lacak iÃ§eriÄŸin uzunluÄŸunu gÃ¶ster
        print(f"Processing file: {relative_file_path} (Content length: {len(content_part)}) -> {target_file_path}")

        if os.path.exists(target_file_path) and not overwrite_existing:
            print(f"  SKIPPED: File '{target_file_path}' already exists (overwrite_existing is False).")
            files_skipped += 1
            continue

        if os.path.exists(target_file_path) and overwrite_existing:
            print(f"  OVERWRITING: File '{target_file_path}'.")
            files_overwritten += 1

        if not dry_run:
            try:
                os.makedirs(os.path.dirname(target_file_path), exist_ok=True)
                with open(target_file_path, "w", encoding="utf-8") as f:
                    f.write(content_part)
                files_restored += 1
            except Exception as e:
                print(f"  ERROR: Could not write file '{target_file_path}': {e}")
        else:
            if not os.path.exists(os.path.dirname(target_file_path)):
                print(f"  DRY RUN: Would create directory {os.path.dirname(target_file_path)}")
            print(f"  DRY RUN: Would write {len(content_part)} bytes to {target_file_path}")
            files_restored += 1

    print("\n--- Restoration Summary ---")
    print(f"Files processed for restoration: {files_restored}")
    if not dry_run:
        print(f"Files actually written/overwritten: {files_restored - files_skipped}")
        print(f"Files overwritten: {files_overwritten}")
    print(f"Files skipped (already exist and overwrite=False): {files_skipped}")


def main():
    parser = argparse.ArgumentParser(description="Project Snapshot Tool (Full Version)")
    subparsers = parser.add_subparsers(dest="command", required=True)

    parser_collect = subparsers.add_parser(
        "collect", help="Collect project files into a single snapshot file."
    )
    parser_collect.add_argument(
        "output_file",
        type=str,
        default="project_snapshot_full.txt",
        nargs="?",
        help="Path to the output snapshot file (default: project_snapshot_full.txt)",
    )
    parser_collect.add_argument(
        "--include-dir",
        action="append",
        dest="include_dirs",
        help="Directory to include (relative to base_dir or absolute). Can be used multiple times. Defaults to ['.']",
    )
    parser_collect.add_argument(
        "--include-ext",
        action="append",
        dest="include_extensions",
        help="File extension to include (e.g., .py, .md). Can be used multiple times. Defaults to common code/config extensions.",
    )
    parser_collect.add_argument(
        "--exclude-pattern",
        action="append",
        dest="exclude_patterns",
        help="Pattern/path to exclude. Can be used multiple times. Defaults to common ignores.",
    )
    parser_collect.add_argument(
        "--base-dir",
        type=str,
        default=".",
        help="Base directory for the project (default: current directory). Relative paths are resolved against this.",
    )
    parser_collect.add_argument(
        "--clean-comments",
        action="store_true",
        help="Attempt to remove comments from collected code files (.py, .sh, .bat).",
    )

    parser_restore = subparsers.add_parser(
        "restore", help="Restore project files from a snapshot."
    )
    parser_restore.add_argument(
        "snapshot_file", type=str, help="Path to the snapshot file to restore from."
    )
    parser_restore.add_argument(
        "--target-dir",
        type=str,
        default=".",
        help="Directory where files will be restored (default: current directory).",
    )
    parser_restore.add_argument(
        "--dry-run",
        action="store_true",
        help="Simulate restoration without writing any files.",
    )
    parser_restore.add_argument(
        "--overwrite",
        action="store_true",
        dest="overwrite_existing",
        help="Overwrite files if they already exist in the target directory.",
    )

    args = parser.parse_args()

    if args.command == "collect":
        final_include_dirs = (
            args.include_dirs if args.include_dirs is not None else DEFAULT_INCLUDE_DIRS
        )
        final_include_extensions = (
            args.include_extensions
            if args.include_extensions is not None
            else DEFAULT_INCLUDE_EXTENSIONS
        )
        final_exclude_patterns = (
            args.exclude_patterns
            if args.exclude_patterns is not None
            else DEFAULT_EXCLUDE_PATTERNS
        )
        collect_project_files_full(
            output_file=args.output_file,
            include_dirs=final_include_dirs,
            include_extensions=final_include_extensions,
            exclude_patterns=final_exclude_patterns,
            base_dir=args.base_dir,
            clean_comments=args.clean_comments,
        )
    elif args.command == "restore":
        restore_from_full_snapshot(
            snapshot_file=args.snapshot_file,
            target_dir=args.target_dir,
            dry_run=args.dry_run,
            overwrite_existing=args.overwrite_existing,
        )


if __name__ == "__main__":
    # Example usage:
    # python tools/snapshot_generator.py collect project_full_snapshot.txt
    # python tools/snapshot_generator.py restore project_full_snapshot.txt --dry-run   
    # python tools/snapshot_generator.py restore project_full_snapshot.txt --overwrite
    print("Project Snapshot Tool (Full Version)")
    print("Collects project files into a single snapshot file or restores from a snapshot.")
    print("Use 'collect' to create a snapshot and 'restore' to restore files from it.")    
    main()

========== FILE: worker/pyproject.toml ==========
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "azuraforge-worker"
version = "0.1.0"
description = "The Celery worker for the AzuraForge Platform. Discovers and runs pipeline plugins."
requires-python = ">=3.8"
dependencies = [
    # Worker'Ä±n kendisi doÄŸrudan Learner'a deÄŸil, eklentilere ihtiyaÃ§ duyar.
    # Eklentiler zaten Learner'a baÄŸÄ±mlÄ± olduÄŸu iÃ§in, pip bunu Ã§Ã¶zecektir.
    
    # GeliÅŸtirme ve test sÄ±rasÄ±nda bu eklentinin kurulu olmasÄ±nÄ± saÄŸlÄ±yoruz.
    # CanlÄ± bir ortamda, hangi eklentilerin kurulacaÄŸÄ± bir konfigÃ¼rasyon dosyasÄ± ile yÃ¶netilir.
    "azuraforge-app-stock-predictor @ git+https://github.com/AzuraForge/app-stock-predictor.git@main",
    
    "celery[redis]",
    "pyyaml",
]

[project.scripts]
start-worker = "azuraforge_worker.main:run_celery_worker"

========== FILE: worker/README.md ==========
# worker

========== FILE: worker/setup.py ==========
from setuptools import setup, find_packages

setup(
    package_dir={"": "src"},
    packages=find_packages(where="src"),
)

========== FILE: worker/src/azuraforge_worker/celery_app.py ==========
from celery import Celery
import os

REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")

celery_app = Celery(
    "azuraforge_worker",
    broker=REDIS_URL,
    backend=REDIS_URL,
    # Worker baÅŸladÄ±ÄŸÄ±nda gÃ¶revlerin nerede olduÄŸunu belirtir.
    include=["azuraforge_worker.tasks.training_tasks"]
)

========== FILE: worker/src/azuraforge_worker/main.py ==========
import subprocess
import sys

def run_celery_worker():
    """'start-worker' komutu iÃ§in giriÅŸ noktasÄ±."""
    print("ðŸ‘·â€â™‚ï¸ Starting AzuraForge Worker...")
    command = [
        sys.executable, "-m", "celery",
        "-A", "azuraforge_worker.celery_app:celery_app", # Celery app nesnesinin tam yolu
        "worker",
        "--pool=solo", # Windows uyumluluÄŸu Ã¼retim iÃ§in prefork kullanÄ±n
        "--loglevel=INFO"
    ]
    subprocess.run(command)

========== FILE: worker/src/azuraforge_worker/__init__.py ==========
from .celery_app import celery_app

# Bu, diÄŸer projelerin 'from azuraforge_worker import celery_app' yapabilmesini saÄŸlar.
__all__ = ("celery_app",)

========== FILE: worker/src/azuraforge_worker/tasks/training_tasks.py ==========
import logging
import os
import json
from datetime import datetime
from importlib.metadata import entry_points
from celery import current_task # GÃ¶revin durumunu gÃ¼ncellemek iÃ§in
import time # SimÃ¼lasyon iÃ§in
import traceback # Hata detaylarÄ±nÄ± yakalamak iÃ§in

from ..celery_app import celery_app
# applications reposundan pipeline'Ä± import etmek iÃ§in bu kodun olduÄŸu yerde deÄŸil,
# pip tarafÄ±ndan paket olarak kurulmuÅŸ azuraforge_stockapp'tan import edilecek.

# --- Eklenti KeÅŸfi ---
def discover_pipelines():
    """
    Sisteme kurulmuÅŸ tÃ¼m AzuraForge pipeline'larÄ±nÄ± ve varsa varsayÄ±lan konfigÃ¼rasyon
    fonksiyonlarÄ±nÄ± keÅŸfeder.
    DÃ¶nÃ¼ÅŸ deÄŸeri: { 'pipeline_id': { 'pipeline_class': Class, 'get_config_func': Function | None } }
    """
    logging.info("Worker: Discovering installed AzuraForge pipeline plugins and configurations...")
    discovered = {}
    try:
        # Pipeline sÄ±nÄ±flarÄ±nÄ± keÅŸfet
        pipeline_entry_points = entry_points(group='azuraforge.pipelines')
        for ep in pipeline_entry_points:
            logging.info(f"Worker: Found pipeline plugin: '{ep.name}' -> points to '{ep.value}'")
            discovered[ep.name] = {'pipeline_class': ep.load()} 
        
        # KonfigÃ¼rasyon fonksiyonlarÄ±nÄ± keÅŸfet (aynÄ± isimle eÅŸleÅŸmeli)
        config_entry_points = entry_points(group='azuraforge.configs')
        for ep in config_entry_points:
            logging.info(f"Worker: Found config entry point: '{ep.name}' -> points to '{ep.value}'")
            if ep.name in discovered:
                discovered[ep.name]['get_config_func'] = ep.load()
            else:
                logging.warning(f"Worker: Found config for '{ep.name}' but no corresponding pipeline. Skipping config.")
                
    except Exception as e:
        logging.error(f"Worker: Error discovering pipelines or configs: {e}", exc_info=True)
    
    # Debug iÃ§in keÅŸfedilenleri logla
    for p_id, p_info in discovered.items():
        logging.info(f"Worker: Discovered pipeline '{p_id}' (Config available: {'get_config_func' in p_info})")

    return discovered

AVAILABLE_PIPELINES_AND_CONFIGS = discover_pipelines() # DeÄŸiÅŸtirildi
if not AVAILABLE_PIPELINES_AND_CONFIGS: # DeÄŸiÅŸtirildi
    logging.warning("Worker: No AzuraForge pipelines found! Please install a pipeline plugin, e.g., 'azuraforge-app-stock-predictor'.")

REPORTS_BASE_DIR = os.path.abspath(os.getenv("REPORTS_DIR", "/app/reports"))
os.makedirs(REPORTS_BASE_DIR, exist_ok=True) # Dizinin var olduÄŸundan emin ol

@celery_app.task(bind=True, name="start_training_pipeline")
def start_training_pipeline(self, config: dict):
    pipeline_name = config.get("pipeline_name")
    
    # DeÄŸiÅŸtirildi: AVAILABLE_PIPELINES_AND_CONFIGS kullan
    if not pipeline_name or pipeline_name not in AVAILABLE_PIPELINES_AND_CONFIGS:
        raise ValueError(f"Pipeline '{pipeline_name}' not found or installed.")

    PipelineInfo = AVAILABLE_PIPELINES_AND_CONFIGS[pipeline_name]
    PipelineClass = PipelineInfo['pipeline_class']

    # --- Deney iÃ§in benzersiz bir klasÃ¶r ve ID oluÅŸtur ---
    run_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    experiment_id = f"{pipeline_name}_{run_timestamp}_{self.request.id}" 
    
    pipeline_specific_report_dir = os.path.join(REPORTS_BASE_DIR, pipeline_name)
    os.makedirs(pipeline_specific_report_dir, exist_ok=True)
    experiment_dir = os.path.join(pipeline_specific_report_dir, experiment_id)
    os.makedirs(experiment_dir, exist_ok=True)
    
    config['experiment_id'] = experiment_id
    config['task_id'] = self.request.id
    config['experiment_dir'] = experiment_dir
    config['start_time'] = datetime.now().isoformat() # Yeni: BaÅŸlangÄ±Ã§ zamanÄ±nÄ± ekle

    logging.info(f"Worker: Instantiating pipeline '{PipelineClass.__name__}' for experiment {experiment_id}")
    
    initial_report_data = {
        "task_id": self.request.id, "experiment_id": experiment_id, "status": "STARTED", "config": config, "results": {}
    }
    with open(os.path.join(experiment_dir, "results.json"), 'w') as f:
        json.dump(initial_report_data, f, indent=4, default=str)

    try:
        # --- KRÄ°TÄ°K DÃœZELTME: Pipeline'a Celery Task objesini iletiyoruz ---
        # Bu, pipeline'Ä±n iÃ§indeki Learner'Ä±n Celery state'i gÃ¼ncelleyebilmesini saÄŸlar.
        pipeline_instance = PipelineClass(config, celery_task=self) # <- Yeni parametre
        results = pipeline_instance.run() 

        final_report_data = {
            "task_id": self.request.id, "experiment_id": experiment_id, "status": "SUCCESS", "config": config, "results": results, "completed_at": datetime.now().isoformat()
        }
        with open(os.path.join(experiment_dir, "results.json"), 'w') as f:
            json.dump(final_report_data, f, indent=4, default=str)
            
        logging.info(f"Worker: Task {self.request.id} for pipeline '{pipeline_name}' completed successfully. Results in {experiment_dir}")
        return final_report_data

    except Exception as e:
        error_traceback = traceback.format_exc()
        error_message = f"Pipeline execution failed for {pipeline_name}: {e}\n{error_traceback}"
        logging.error(error_message)
        
        self.update_state(state='FAILURE', meta={'error_message': str(e), 'traceback': error_traceback})
        
        error_report_data = {
            "task_id": self.request.id, "experiment_id": experiment_id, "status": "FAILURE", "config": config, "error": error_message, "failed_at": datetime.now().isoformat()
        }
        with open(os.path.join(experiment_dir, "results.json"), 'w') as f:
            json.dump(error_report_data, f, indent=4)
            
        raise e
========== FILE: worker/src/azuraforge_worker/tasks/__init__.py ==========
