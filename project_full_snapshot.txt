PROJE KOD SNAPSHOT (TAM)
Toplam 95 dosya bulundu ve eklendi.
Dahil Edilen Dizinler: .
Dahil Edilen UzantÄ±lar: .toml, .py, .yaml, .yml, .json, .md, .txt, html, .bat, .sh, .jsx, .js, .json, .css
HariÃ§ Tutulan Desenler/Yollar: __pycache__, .git, .venv, .vscode, .idea, build, dist, *.egg-info, *.pyc, *.so, *.pyd, .pytest_cache, .mypy_cache, .dataset, dataset, .logs, logs, .output, output, inputs, outputs, .tmp, checkpoints, reports, docs/_build, site, node_modules, .DS_Store, Thumbs.db, *.lock
================================================================================

========== FILE: docker-compose.yml ==========
services:
  # 1. Redis Servisi
  redis:
    image: redis:alpine
    container_name: azuraforge_redis
    ports: ["6379:6379"]
    volumes: ["redis_data:/data"]

  # 2. PostgreSQL VeritabanÄ± Servisi (SaÄŸlÄ±k KontrolÃ¼ ile)
  postgres:
    image: postgres:15-alpine
    container_name: azuraforge_postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    restart: always
    # === YENÄ° BÃ–LÃœM: SAÄžLIK KONTROLÃœ ===
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 5s
      timeout: 5s
      retries: 5
    # === DEÄžÄ°ÅžÄ°KLÄ°K SONU ===

  # --- ANA PLATFORM SERVÄ°SLERÄ° ---

  # 3. API Servisi
  api:
    container_name: azuraforge_api
    build:
      context: ./api
      dockerfile: Dockerfile
    command: start-api
    ports: ["8000:8000"]
    volumes:
      - ./api:/app
      - ${REPORTS_DIR}:/app/reports
      - ${CACHE_DIR}:/app/.cache
    environment:
      - REDIS_URL=${REDIS_URL}
      - REPORTS_DIR=/app/reports
      - CACHE_DIR=/app/.cache
      - DATABASE_URL=${DATABASE_URL}
    # === DEÄžÄ°ÅžÄ°KLÄ°K BURADA ===
    # depends_on yapÄ±sÄ±, saÄŸlÄ±k kontrolÃ¼nÃ¼ bekleyecek ÅŸekilde gÃ¼ncellendi
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started # Redis iÃ§in basit baÅŸlangÄ±Ã§ yeterli
    # === DEÄžÄ°ÅžÄ°KLÄ°K SONU ===

  # 4. Worker Servisi (GPU Yetenekli)
  worker:
    container_name: azuraforge_worker
    build:
      context: ./worker
      dockerfile: Dockerfile
    command: start-worker
    volumes:
      - ./worker:/app
      - ${REPORTS_DIR}:/app/reports
      - ${CACHE_DIR}:/app/.cache
    environment:
      - REDIS_URL=${REDIS_URL}
      - REPORTS_DIR=/app/reports
      - CACHE_DIR=/app/.cache
      - DATABASE_URL=${DATABASE_URL}
      # YENÄ°: CihazÄ± GPU olarak ayarlÄ±yoruz
      - AZURAFORGE_DEVICE=gpu
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started
    # === YENÄ° BÃ–LÃœM: GPU KAYNAKLARINI AYIRMA ===
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1 # veya 'all'
              capabilities: [gpu]
    # === DEÄžÄ°ÅžÄ°KLÄ°K SONU ===

  # 5. Dashboard Servisi
  dashboard:
    container_name: azuraforge_dashboard
    build:
      context: ./dashboard
      dockerfile: Dockerfile
    command: npm run dev -- --host 0.0.0.0
    ports: ["5173:5173"]
    volumes:
      - ./dashboard:/app
      - /app/node_modules
    depends_on: [api]

volumes:
  redis_data:
  postgres_data:
========== FILE: README.md ==========
# AzuraForge Platform ðŸš€

**AzuraForge**, yapay zeka modellerini sÄ±fÄ±rdan oluÅŸturmak, eÄŸitmek, canlÄ± olarak takip etmek ve sonuÃ§larÄ±nÄ± interaktif raporlarla analiz etmek iÃ§in tasarlanmÄ±ÅŸ, **olay gÃ¼dÃ¼mlÃ¼, eklenti tabanlÄ± ve daÄŸÄ±tÄ±k bir MLOps platformudur.**

Bu depo, AzuraForge ekosistemindeki tÃ¼m ana servisleri ve kÃ¼tÃ¼phaneleri bir araya getiren **orkestrasyon katmanÄ±dÄ±r**.

## ðŸ›ï¸ Platform Mimarisi ve Felsefesi

AzuraForge, basit bir araÃ§ setinden daha fazlasÄ±dÄ±r; modern yapay zeka sistemlerinin nasÄ±l inÅŸa edilmesi gerektiÄŸine dair bir felsefeyi temsil eder. Bu felsefeyi, yol haritamÄ±zÄ± ve projenin geliÅŸim hikayesini derinlemesine anlamak iÃ§in **[Proje Vizyonu ve Yol HaritasÄ±](./docs/VISION_AND_ROADMAP.md)** belgemizi inceleyin.

Platformumuz dÃ¶rt temel prensip Ã¼zerine kuruludur: **"The AzuraForge Way"**

1.  **SÄ±fÄ±rdan Ä°nÅŸa ve Derin AnlayÄ±ÅŸ:**
    Temel algoritmalarÄ± (`Tensor`, `LSTM` vb.) sÄ±fÄ±rdan yazarak sistem Ã¼zerinde tam kontrol, ÅŸeffaflÄ±k ve derinlemesine bir "know-how" saÄŸlÄ±yoruz. DÄ±ÅŸ dÃ¼nyaya minimal baÄŸÄ±mlÄ±lÄ±kla, "kara kutu"lardan arÄ±nmÄ±ÅŸ bir yapÄ± hedefliyoruz.

2.  **ModÃ¼ler ve Ã–lÃ§eklenebilir Ekosistem:**
    Her bileÅŸen (`api`, `worker`, `learner` vb.) kendi baÄŸÄ±msÄ±z reposunda yaÅŸar, baÄŸÄ±msÄ±z olarak geliÅŸtirilebilir ve kurulabilir. Bu mikroservis yaklaÅŸÄ±mÄ±, projenin bakÄ±mÄ±nÄ± ve Ã¶lÃ§eklenmesini kolaylaÅŸtÄ±rÄ±r.

3.  **Olay GÃ¼dÃ¼mlÃ¼ ve Asenkron AkÄ±ÅŸ:**
    `Celery` ve `Redis Pub/Sub` Ã¼zerine kurulu mimari sayesinde, yoÄŸun model eÄŸitimleri bile sistemi bloklamaz. Bu, platformun en kritik gÃ¼cÃ¼dÃ¼r ve kullanÄ±cÄ±ya akÄ±cÄ±, gerÃ§ek zamanlÄ± bir deneyim sunar. Bu mimarinin detaylarÄ± iÃ§in **[Mimari Belgesi](./docs/ARCHITECTURE.md)**'ne gÃ¶z atÄ±n.

4.  **GeniÅŸletilebilir Eklenti Sistemi:**
    Yeni AI uygulamalarÄ±, platformun Ã§ekirdek koduna dokunmadan, Python'un `entry_points` mekanizmasÄ± kullanÄ±larak sisteme "eklenti" olarak dahil edilebilir. Bu, platformun yeteneklerinin organik olarak bÃ¼yÃ¼mesini saÄŸlar.

---

## âœ¨ Ana Yetenekler

*   **SÄ±fÄ±rdan Ä°nÅŸa EdilmiÅŸ Ã‡ekirdek:** Otomatik tÃ¼rev, `LSTM` gibi geliÅŸmiÅŸ katmanlar ve `Adam` optimizer iÃ§eren, saf Python/NumPy tabanlÄ± bir derin Ã¶ÄŸrenme motoru.
*   **CanlÄ± Deney Takibi:** `WebSocket` aracÄ±lÄ±ÄŸÄ±yla, devam eden bir eÄŸitimin ilerleme Ã§ubuÄŸunu, anlÄ±k kayÄ±p deÄŸerini ve tahmin grafiklerinin canlÄ± evrimini anlÄ±k olarak izleme imkanÄ±.
*   **Dinamik ve Ä°nteraktif Raporlama:** Tamamlanan her deney iÃ§in, `Dashboard` Ã¼zerinden eriÅŸilebilen, `Chart.js` ile Ã§izilmiÅŸ interaktif grafikler ve detaylÄ± metrikler iÃ§eren rapor sayfalarÄ±.
*   **Deney KarÅŸÄ±laÅŸtÄ±rma:** Birden fazla deney sonucunu tek bir arayÃ¼zde gÃ¶rsel olarak karÅŸÄ±laÅŸtÄ±rarak en iyi modeli kolayca belirleme.

---

## ðŸ—ºï¸ Ekosisteme Genel BakÄ±ÅŸ

AzuraForge platformu, aÅŸaÄŸÄ±daki baÄŸÄ±msÄ±z GitHub depolarÄ±ndan oluÅŸur:

| Repo                         | Sorumluluk                                                                       | Teknoloji      |
| ---------------------------- | -------------------------------------------------------------------------------- | -------------- |
| **Ã‡ekirdek KÃ¼tÃ¼phaneler**    |                                                                                  |                |
| `core`                       | Temel tensÃ¶r matematiÄŸi ve otomatik tÃ¼rev (geri yayÄ±lÄ±m) motoru.                   | `Python`, `NumPy` |
| `learner`                    | YÃ¼ksek seviyeli Ã¶ÄŸrenme kÃ¼tÃ¼phanesi (Katmanlar, OptimizatÃ¶rler, Pipeline'lar).     | `Python`       |
| **Uygulama Eklentileri**     |                                                                                  |                |
| `applications`               | Resmi ve test edilmiÅŸ uygulama eklentilerinin katalogunu tutar.                    | `JSON`         |
| `app-stock-predictor`        | GerÃ§ek bir zaman serisi tahmin eklentisi Ã¶rneÄŸi.                                 | `Python`       |
| **Platform Servisleri**      |                                                                                  |                |
| `api`                        | RESTful API ve WebSocket (Pub/Sub) sunan merkezi iletiÅŸim katmanÄ±.                 | `FastAPI`      |
| `worker`                     | Arka plan gÃ¶revlerini (model eÄŸitimi) iÅŸleyen ve raporlarÄ± oluÅŸturan iÅŸÃ§i servisi. | `Celery`, `Redis` |
| `dashboard`                  | React tabanlÄ±, canlÄ± takip ve raporlama yeteneklerine sahip web arayÃ¼zÃ¼.           | `React`, `Vite` |
| **Orkestrasyon (Bu Repo)**   |                                                                                  |                |
| `platform`                   | TÃ¼m servisleri `docker-compose` ile bir araya getirir ve ana dokÃ¼mantasyonu barÄ±ndÄ±rÄ±r. | `Docker`, `YAML` |

---

## ðŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§ (Docker Compose ile)

1.  **Docker Desktop'Ä±n yÃ¼klÃ¼ ve Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun.**
2.  **Bu repoyu klonlayÄ±n:** `git clone https://github.com/AzuraForge/platform.git && cd platform`
3.  **.env dosyasÄ±nÄ± oluÅŸturun:** Proje kÃ¶k dizininde `.env.example` dosyasÄ±nÄ± kopyalayarak `.env` adÄ±yla yeni bir dosya oluÅŸturun. (VarsayÄ±lan deÄŸerler genellikle yeterlidir).
    ```bash
    cp .env.example .env
    ```
4.  **Gerekli Dizinleri OluÅŸturun:**
    ```bash
    mkdir -p ./reports ./.cache
    ```
5.  **Platformu baÅŸlatÄ±n:** (Ä°lk baÅŸlatma, imajlar build edileceÄŸi iÃ§in biraz zaman alabilir.)
    ```bash
    docker-compose up --build -d
    ```
6.  **Platforma eriÅŸin:**
    *   **Dashboard:** `http://localhost:5173`
    *   **API DokÃ¼mantasyonu:** `http://localhost:8000/api/v1/docs`
7.  **KeÅŸfedin:** Dashboard'dan bir deney baÅŸlatÄ±n, canlÄ± takip panelini izleyin ve deney bittiÄŸinde "Raporu GÃ¶rÃ¼ntÃ¼le" butonuyla interaktif sonuÃ§larÄ± inceleyin.

---

## ðŸ› ï¸ GeliÅŸtirme ve KatkÄ±da Bulunma

Platformda geliÅŸtirme yapmak, yeni bir eklenti oluÅŸturmak veya projeye katkÄ±da bulunmak iÃ§in aÅŸaÄŸÄ±daki rehberlerimize gÃ¶z atÄ±n. TÃ¼m geliÅŸtirme sÃ¼reÃ§lerimiz, "The AzuraForge Way" prensiplerine dayanmaktadÄ±r.

*   **[GeliÅŸtirme Rehberi](./docs/DEVELOPMENT_GUIDE.md):** Yerel geliÅŸtirme ortamÄ±nÄ±zÄ± nasÄ±l kuracaÄŸÄ±nÄ±zÄ± ve servisleri nasÄ±l Ã§alÄ±ÅŸtÄ±racaÄŸÄ±nÄ±zÄ± Ã¶ÄŸrenin.
*   **[KatkÄ±da Bulunma Rehberi](./docs/CONTRIBUTING.md):** Kodlama standartlarÄ±mÄ±z, commit mesaj formatÄ±mÄ±z ve Pull Request sÃ¼recimiz hakkÄ±nda bilgi edinin.


========== FILE: api/pyproject.toml ==========
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "azuraforge-api"
version = "0.2.3" # Versiyonu senkronize ediyoruz
description = "The API server for the AzuraForge Platform."
requires-python = ">=3.8"

dependencies = [
    "azuraforge-learner @ git+https://github.com/AzuraForge/learner.git@v0.1.4",
    # === DEÄžÄ°ÅžÄ°KLÄ°K BURADA: worker'Ä±n en son versiyonuna iÅŸaret ediyor ===
    "azuraforge-worker @ git+https://github.com/AzuraForge/worker.git@v0.2.3",
    "azuraforge-applications @ git+https://github.com/AzuraForge/applications.git@v0.1.0",
    "fastapi",
    "uvicorn[standard]",
    "pydantic-settings",
    "python-dotenv",
    "pyyaml",
    "redis",
    "SQLAlchemy",
    "psycopg2-binary",
]

[project.scripts]
start-api = "azuraforge_api.main:run_server"

[project.urls]
"Homepage" = "https://github.com/AzuraForge/api"

[project.optional-dependencies]
dev = [
    "pytest",
    "pytest-asyncio",
    "httpx",
]

[tool.pytest.ini_options]
asyncio_mode = "auto"
========== FILE: api/README.md ==========
# AzuraForge API Servisi

Bu servis, AzuraForge platformunun merkezi iletiÅŸim katmanÄ± ve dÄ±ÅŸ dÃ¼nyaya aÃ§Ä±lan aÄŸ geÃ§ididir.

## ðŸŽ¯ Ana Sorumluluklar

1.  **RESTful API Sunucusu:**
    *   `Dashboard` ve potansiyel diÄŸer istemciler iÃ§in standart HTTP endpoint'leri saÄŸlar (`/experiments`, `/pipelines` vb.).
    *   Gelen istekleri doÄŸrular ve iÅŸlenmesi iÃ§in gÃ¶revleri `Celery` kuyruÄŸuna (Redis) iletir.

2.  **WebSocket Sunucusu:**
    *   Devam eden deneylerin durumunu canlÄ± olarak takip etmek iÃ§in (`/ws/task_status/{task_id}`) WebSocket baÄŸlantÄ±larÄ± sunar.

3.  **Redis Pub/Sub Dinleyicisi:**
    *   `Worker` tarafÄ±ndan yayÄ±nlanan ilerleme mesajlarÄ±nÄ± (`task-progress:*` kanallarÄ±) dinler ve bu mesajlarÄ± ilgili WebSocket istemcisine anÄ±nda iletir.

## ðŸ› ï¸ Yerel GeliÅŸtirme ve Test

Bu servisi yerel ortamda Ã§alÄ±ÅŸtÄ±rmak ve test etmek iÃ§in, ana `platform` reposundaki **[GeliÅŸtirme Rehberi](../../platform/docs/DEVELOPMENT_GUIDE.md)**'ni takip edin.

Servis baÄŸÄ±mlÄ±lÄ±klarÄ± kurulduktan ve sanal ortam aktive edildikten sonra, aÅŸaÄŸÄ±daki komutla API sunucusunu baÅŸlatabilirsiniz:

```bash
# api/ kÃ¶k dizinindeyken
start-api
```

Sunucu `http://localhost:8000` adresinde Ã§alÄ±ÅŸmaya baÅŸlayacaktÄ±r.

**Birim Testleri (YakÄ±nda):**
Birim testlerini Ã§alÄ±ÅŸtÄ±rmak iÃ§in:
```bash
pytest
```

========== FILE: api/setup.py ==========
from setuptools import setup, find_packages

setup(
    # Bu satÄ±r, setuptools'a paketlerin 'src' klasÃ¶rÃ¼nÃ¼n iÃ§inde
    # olduÄŸunu sÃ¶yler.
    package_dir={"": "src"},
    
    # Bu satÄ±r, 'src' klasÃ¶rÃ¼nÃ¼n iÃ§indeki tÃ¼m Python paketlerini
    # (azuraforge_api ve altÄ±ndakiler) otomatik olarak bulur.
    packages=find_packages(where="src"),
)

========== FILE: api/.github/workflows/ci.yml ==========
name: AzuraForge API CI

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

jobs:
  build-and-test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.10", "3.11"]

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        # API'nin tÃ¼m Git baÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± kur (versiyon etiketleriyle)
        pip install git+https://github.com/AzuraForge/learner.git@v0.1.3
        pip install git+https://github.com/AzuraForge/worker.git@v0.1.0
        pip install git+https://github.com/AzuraForge/applications.git@v0.1.0
        # API'nin kendisini ve test baÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± kur
        pip install -e .[dev]

    - name: Check code format with Black
      run: |
        pip install black
        black --check .
    
    - name: Lint with flake8
      run: |
        pip install flake8
        flake8 src --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 src --count --max-complexity=10 --max-line-length=127 --statistics

    - name: Test with pytest
      run: |
        pytest
========== FILE: api/src/azuraforge_api/main.py ==========
import uvicorn
from fastapi import FastAPI, APIRouter # DÃœZELTME: APIRouter'Ä± import et
from fastapi.middleware.cors import CORSMiddleware

from .core.config import settings
from .routes import experiments, pipelines, streaming

def create_app() -> FastAPI:
    app = FastAPI(title=settings.PROJECT_NAME, version="0.1.0")
    
    # CORS ayarlarÄ±nÄ± dinamik olarak belirle
    if settings.CORS_ORIGINS == "*":
        allowed_origins = ["*"]
    else:
        allowed_origins = [origin.strip() for origin in settings.CORS_ORIGINS.split(',')]

    app.add_middleware(
        CORSMiddleware,
        allow_origins=allowed_origins,
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    # DÃœZELTME: Ä°Ã§ iÃ§e FastAPI uygulamasÄ± yerine tek bir APIRouter kullanÄ±yoruz.
    # Bu, "AttributeError: 'FastAPI' object has no attribute 'default_response_class'"
    # hatasÄ±nÄ± Ã§Ã¶zer.
    api_router = APIRouter()
    api_router.include_router(experiments.router)
    api_router.include_router(pipelines.router)
    
    # Åžimdi bu birleÅŸtirilmiÅŸ router'Ä± tek bir prefix ile ana uygulamaya ekliyoruz.
    app.include_router(api_router, prefix=settings.API_V1_PREFIX)
    
    # WebSocket router'Ä± prefix dÄ±ÅŸÄ±nda, doÄŸrudan ana uygulamaya ekleniyor.
    app.include_router(streaming.router)
    
    @app.get("/", tags=["Root"])
    def read_root():
        return {"message": f"Welcome to {settings.PROJECT_NAME}"}
        
    return app

app = create_app()

def run_server():
    print(f"ðŸš€ Starting {settings.PROJECT_NAME}...")
    uvicorn.run("azuraforge_api.main:app", host="0.0.0.0", port=8000, reload=True)
========== FILE: api/src/azuraforge_api/__init__.py ==========

========== FILE: api/src/azuraforge_api/core/config.py ==========
from pydantic_settings import BaseSettings, SettingsConfigDict

class Settings(BaseSettings):
    PROJECT_NAME: str = "AzuraForge API"
    API_V1_PREFIX: str = "/api/v1"
    
    # Yeni CORS ayarÄ±
    # VirgÃ¼lle ayrÄ±lmÄ±ÅŸ URL'ler veya tÃ¼mÃ¼ne izin vermek iÃ§in "*"
    CORS_ORIGINS: str = "*" # VarsayÄ±lan olarak tÃ¼mÃ¼ne izin ver (geliÅŸtirme iÃ§in)
    
    model_config = SettingsConfigDict(env_file=".env", env_file_encoding='utf-8')

settings = Settings()

========== FILE: api/src/azuraforge_api/core/__init__.py ==========

========== FILE: api/src/azuraforge_api/routes/experiments.py ==========
# api/src/azuraforge_api/routes/experiments.py

from fastapi import APIRouter, HTTPException
from typing import List, Dict, Any
from ..services import experiment_service

router = APIRouter(tags=["Experiments"])

@router.get("/experiments", response_model=List[Dict[str, Any]])
def get_all_experiments():
    return experiment_service.list_experiments()

@router.post("/experiments", status_code=202, response_model=Dict[str, Any])
def create_new_experiment(config: Dict[str, Any]):
    return experiment_service.start_experiment(config)

@router.get("/experiments/{task_id}/status", response_model=Dict[str, Any])
def get_experiment_status(task_id: str):
    # Bu endpoint artÄ±k Ã§ok gerekli deÄŸil ama kalabilir.
    return experiment_service.get_task_status(task_id)

# YENÄ° ENDPOINT (read_experiment_report yerine)
@router.get("/experiments/{experiment_id}/details", response_model=Dict[str, Any])
def read_experiment_details(experiment_id: str):
    """
    Belirli bir deneyin tÃ¼m detaylarÄ±nÄ± (config, results, metrics, history)
     iÃ§eren JSON verisini dÃ¶ndÃ¼rÃ¼r.
    """
    try:
        return experiment_service.get_experiment_details(experiment_id)
    except HTTPException as e:
        raise e
========== FILE: api/src/azuraforge_api/routes/pipelines.py ==========
from fastapi import APIRouter, HTTPException
from typing import List, Dict, Any
from ..services import experiment_service

router = APIRouter(tags=["Pipelines"])

@router.get("/pipelines", response_model=List[Dict[str, Any]])
def get_all_available_pipelines():
    return experiment_service.get_available_pipelines()

@router.get("/pipelines/{pipeline_id}/config", response_model=Dict[str, Any])
def get_pipeline_default_config(pipeline_id: str):
    try:
        return experiment_service.get_default_pipeline_config(pipeline_id)
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
========== FILE: api/src/azuraforge_api/routes/streaming.py ==========
import asyncio
import logging
import json
import os
from fastapi import APIRouter, WebSocket, WebSocketDisconnect
import redis.asyncio as redis

router = APIRouter()

async def redis_listener(websocket: WebSocket, task_id: str):
    """Redis Pub/Sub kanalÄ±nÄ± dinler ve gelen mesajlarÄ± WebSocket'e iletir."""
    redis_url = os.environ.get("REDIS_URL", "redis://localhost:6379/0")
    r = await redis.from_url(redis_url)
    pubsub = r.pubsub()
    channel = f"task-progress:{task_id}"
    await pubsub.subscribe(channel)
    
    try:
        while True:
            # `listen` bir coroutine'dir, bu yÃ¼zden `await` edilmelidir.
            message = await pubsub.get_message(ignore_subscribe_messages=True, timeout=1.0)
            if message and message.get("type") == "message":
                # Gelen veri bytes, string'e Ã§evirip JSON olarak parse et.
                data_str = message['data'].decode('utf-8')
                progress_data = json.loads(data_str)
                # UI'Ä±n beklediÄŸi formatla gÃ¶nder
                await websocket.send_json({
                    "state": "PROGRESS",
                    "details": progress_data
                })
            # WebSocket baÄŸlantÄ±sÄ± hala aÃ§Ä±k mÄ± kontrol et
            # Bu, istemci baÄŸlantÄ±yÄ± kapattÄ±ÄŸÄ±nda dÃ¶ngÃ¼den Ã§Ä±kmayÄ± saÄŸlar.
            await asyncio.sleep(0.1) # CPU'yu yormamak iÃ§in kÄ±sa bir bekleme
    except asyncio.CancelledError:
        logging.info(f"Redis listener for task {task_id} cancelled.")
    except Exception as e:
        logging.error(f"Redis listener error for task {task_id}: {e}")
    finally:
        await pubsub.unsubscribe(channel)
        await r.close()
        logging.info(f"Redis listener for task {task_id} cleaned up.")

@router.websocket("/ws/task_status/{task_id}")
async def websocket_task_status(websocket: WebSocket, task_id: str):
    await websocket.accept()
    logging.info(f"WebSocket connection accepted for task: {task_id}")
    
    # Redis dinleyicisini bir arka plan gÃ¶revi olarak baÅŸlat
    listener_task = asyncio.create_task(redis_listener(websocket, task_id))
    
    try:
        # Ä°stemcinin baÄŸlantÄ±yÄ± kapatmasÄ±nÄ± bekle
        # Bu dÃ¶ngÃ¼, baÄŸlantÄ± aÃ§Ä±k olduÄŸu sÃ¼rece Ã§alÄ±ÅŸÄ±r.
        while True:
            await websocket.receive_text() # Bu satÄ±r aslÄ±nda istemciden mesaj beklemez,
                                           # sadece baÄŸlantÄ±nÄ±n kopup kopmadÄ±ÄŸÄ±nÄ± kontrol eder.
    except WebSocketDisconnect:
        logging.warning(f"WebSocket disconnected by client for task: {task_id}")
    finally:
        # Ä°stemci baÄŸlantÄ±yÄ± kapattÄ±ÄŸÄ±nda, arka plandaki Redis dinleyicisini iptal et
        listener_task.cancel()
        # GÃ¶revin bitmesini bekle (kaynaklarÄ±n temizlenmesi iÃ§in)
        await listener_task
        logging.info(f"Closing WebSocket connection for task {task_id}")
========== FILE: api/src/azuraforge_api/routes/__init__.py ==========

========== FILE: api/src/azuraforge_api/services/experiment_service.py ==========
# api/src/azuraforge_api/services/experiment_service.py

import json
import itertools
import uuid
from datetime import datetime
from importlib import resources
from typing import List, Dict, Any, Generator
from fastapi import HTTPException
from celery.result import AsyncResult
from sqlalchemy import desc

from azuraforge_worker.database import SessionLocal, Experiment
from azuraforge_worker import celery_app
from azuraforge_worker.tasks.training_tasks import AVAILABLE_PIPELINES_AND_CONFIGS

# --- Helper Fonksiyonlar ---
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

def _generate_config_combinations(config: Dict[str, Any]) -> Generator[Dict[str, Any], None, None]:
    varying_params = {}
    static_params = {}
    
    def _traverse_and_split(conf, path=""):
        for key, value in conf.items():
            new_path = f"{path}.{key}" if path else key
            if isinstance(value, list):
                varying_params[new_path] = value
            elif isinstance(value, dict):
                _traverse_and_split(value, new_path)
            else:
                static_params[new_path] = value

    _traverse_and_split(config)

    if not varying_params:
        yield config
        return

    param_names = list(varying_params.keys())
    param_values = list(varying_params.values())
    
    for combo in itertools.product(*param_values):
        new_config = {}
        for key_path, value in static_params.items():
            keys = key_path.split('.')
            d = new_config
            for k in keys[:-1]:
                d = d.setdefault(k, {})
            d[keys[-1]] = value
            
        for i, key_path in enumerate(param_names):
            keys = key_path.split('.')
            d = new_config
            for k in keys[:-1]:
                d = d.setdefault(k, {})
            d[keys[-1]] = combo[i]
            
        yield new_config

# --- API Servis FonksiyonlarÄ± ---

def get_available_pipelines() -> List[Dict[str, Any]]:
    official_apps_data = []
    try:
        with resources.open_text("azuraforge_applications", "official_apps.json") as f:
            official_apps_data = json.load(f)
    except (FileNotFoundError, ModuleNotFoundError):
        pass
    
    available_pipelines = [app for app in official_apps_data if app.get("id") in AVAILABLE_PIPELINES_AND_CONFIGS]
    return available_pipelines

def get_default_pipeline_config(pipeline_id: str) -> Dict[str, Any]:
    pipeline_info = AVAILABLE_PIPELINES_AND_CONFIGS.get(pipeline_id)
    if not pipeline_info:
        raise ValueError(f"Pipeline '{pipeline_id}' not found.")
    
    get_config_func = pipeline_info.get('get_config_func')
    if not get_config_func:
        return {"message": "No specific default configuration available."}
    return get_config_func()


def start_experiment(config: Dict[str, Any]) -> Dict[str, Any]:
    task_ids = []
    batch_id = str(uuid.uuid4())
    batch_name = config.pop("batch_name", f"Batch-{datetime.now().strftime('%Y-%m-%d-%H%M')}")
    
    combinations = list(_generate_config_combinations(config))
    num_combinations = len(combinations)

    for single_config in combinations:
        if num_combinations > 1:
            single_config['batch_id'] = batch_id
            single_config['batch_name'] = batch_name
        else:
            single_config['batch_id'] = None
            single_config['batch_name'] = None
            
        task = celery_app.send_task("start_training_pipeline", args=[single_config])
        task_ids.append(task.id)

    if num_combinations > 1:
        return {
            "message": f"{num_combinations} experiments submitted as a batch.",
            "batch_id": batch_id,
            "task_ids": task_ids
        }
    else:
        return {"message": "Experiment submitted to worker.", "task_id": task_ids[0]}

def list_experiments() -> List[Dict[str, Any]]:
    """VeritabanÄ±ndaki tÃ¼m deneylerin bir Ã¶zetini en yeniden eskiye doÄŸru listeler."""
    with SessionLocal() as db:
        experiments = db.query(Experiment).order_by(desc(Experiment.created_at)).all()
        
        results_summary = []
        for exp in experiments:
            # Sadece Dashboard'un liste gÃ¶rÃ¼nÃ¼mÃ¼ iÃ§in gerekli olan verileri gÃ¶nderiyoruz.
            summary = {
                "experiment_id": exp.id,
                "task_id": exp.task_id,
                "pipeline_name": exp.pipeline_name,
                "status": exp.status,
                "created_at": exp.created_at.isoformat() if exp.created_at else None,
                "completed_at": exp.completed_at.isoformat() if exp.completed_at else None,
                "failed_at": exp.failed_at.isoformat() if exp.failed_at else None,
                "batch_id": exp.batch_id,
                "batch_name": exp.batch_name,
                "config_summary": {
                    "ticker": exp.config.get("data_sourcing", {}).get("ticker", "N/A") if exp.config else "N/A",
                    "epochs": exp.config.get("training_params", {}).get("epochs", "N/A") if exp.config else "N/A",
                    "lr": exp.config.get("training_params", {}).get("lr", "N/A") if exp.config else "N/A",
                },
                "results_summary": {
                    "final_loss": exp.results.get("final_loss") if exp.results else None
                }
            }
            results_summary.append(summary)
        return results_summary

def get_experiment_details(experiment_id: str) -> Dict[str, Any]:
    """Belirli bir deneyin tÃ¼m detaylarÄ±nÄ± veritabanÄ±ndan Ã§eker."""
    with SessionLocal() as db:
        exp = db.query(Experiment).filter(Experiment.id == experiment_id).first()
        if not exp:
            raise HTTPException(status_code=404, detail=f"Experiment '{experiment_id}' not found.")
        
        return {
            "experiment_id": exp.id, "task_id": exp.task_id, "pipeline_name": exp.pipeline_name,
            "status": exp.status, "config": exp.config, "results": exp.results, "error": exp.error,
            "created_at": exp.created_at.isoformat() if exp.created_at else None,
            "completed_at": exp.completed_at.isoformat() if exp.completed_at else None,
            "failed_at": exp.failed_at.isoformat() if exp.failed_at else None,
            "batch_id": exp.batch_id,
            "batch_name": exp.batch_name,
        }

def get_task_status(task_id: str) -> Dict[str, Any]:
    task_result = AsyncResult(task_id, app=celery_app)
    return {"status": task_result.state, "details": task_result.info}
========== FILE: api/src/azuraforge_api/services/__init__.py ==========

========== FILE: api/src/azuraforge_api/tasks/training_tasks.py ==========

========== FILE: api/src/azuraforge_api/tasks/__init__.py ==========

========== FILE: api/tests/azuraforge_api/test_api_endpoints.py ==========
import pytest
from httpx import AsyncClient, ASGITransport
from unittest.mock import patch

# Test edilecek FastAPI uygulamasÄ±nÄ± import et
from azuraforge_api.main import app

# pytest'in asenkron testleri Ã§alÄ±ÅŸtÄ±rmasÄ±nÄ± saÄŸlar
pytestmark = pytest.mark.asyncio

# === DEÄžÄ°ÅžÄ°KLÄ°K BURADA ===
# API'ye yapÄ±lan tÃ¼m Ã§aÄŸrÄ±lar iÃ§in bir istemci oluÅŸturalÄ±m
# 'app' yerine ASGITransport kullanarak istemcinin doÄŸrudan uygulama ile konuÅŸmasÄ±nÄ± saÄŸlÄ±yoruz.
@pytest.fixture
async def async_client():
    transport = ASGITransport(app=app)
    async with AsyncClient(transport=transport, base_url="http://test") as client:
        yield client
# === DEÄžÄ°ÅžÄ°KLÄ°K SONU ===

async def test_read_root(async_client: AsyncClient):
    """KÃ¶k endpoint'in doÄŸru mesajÄ± dÃ¶ndÃ¼rdÃ¼ÄŸÃ¼nÃ¼ test eder."""
    response = await async_client.get("/")
    assert response.status_code == 200
    assert response.json() == {"message": "Welcome to AzuraForge API"}

@patch('azuraforge_api.services.experiment_service.get_available_pipelines')
async def test_get_all_pipelines(mock_get_pipelines, async_client: AsyncClient):
    """/pipelines endpoint'inin doÄŸru veriyi ve 200 kodunu dÃ¶ndÃ¼rdÃ¼ÄŸÃ¼nÃ¼ test eder."""
    # Servis katmanÄ±nÄ± mock'layarak veritabanÄ± veya dosya sistemi baÄŸÄ±mlÄ±lÄ±ÄŸÄ±nÄ± ortadan kaldÄ±rÄ±yoruz.
    mock_pipelines_data = [
        {"id": "stock_predictor", "name": "Hisse Senedi Fiyat Tahmini"},
        {"id": "weather_forecaster", "name": "Hava Durumu Tahmini"}
    ]
    mock_get_pipelines.return_value = mock_pipelines_data

    response = await async_client.get("/api/v1/pipelines")
    
    assert response.status_code == 200
    assert response.json() == mock_pipelines_data
    # Servis fonksiyonunun Ã§aÄŸrÄ±ldÄ±ÄŸÄ±nÄ± doÄŸrula
    mock_get_pipelines.assert_called_once()


@patch('azuraforge_api.services.experiment_service.start_experiment')
async def test_create_experiment_success(mock_start_experiment, async_client: AsyncClient):
    """Bir deney baÅŸarÄ±yla gÃ¶nderildiÄŸinde 202 kodunu ve task_id'yi dÃ¶ndÃ¼rdÃ¼ÄŸÃ¼nÃ¼ test eder."""
    test_config = {"pipeline_name": "stock_predictor", "data_sourcing": {"ticker": "GOOG"}}
    mock_start_experiment.return_value = {"message": "Experiment submitted", "task_id": "fake-task-id-123"}

    response = await async_client.post("/api/v1/experiments", json=test_config)
    
    assert response.status_code == 202 # Accepted
    assert response.json()["task_id"] == "fake-task-id-123"
    mock_start_experiment.assert_called_once_with(test_config)
========== FILE: app-stock-predictor/pyproject.toml ==========
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "azuraforge-app-stock-predictor"
version = "0.1.2" # Versiyonu artÄ±rÄ±yoruz
description = "A stock prediction pipeline application for the AzuraForge platform."
requires-python = ">=3.8"
dependencies = [
    # === DEÄžÄ°ÅžÄ°KLÄ°K BURADA: learner'Ä±n en son versiyonuna iÅŸaret ediyor ===
    "azuraforge-learner @ git+https://github.com/AzuraForge/learner.git@v0.1.4",
    "yfinance",
    "pandas",
    "scikit-learn",
    "PyYAML",
]

[project.entry-points]
"azuraforge.pipelines" = { stock_predictor = "azuraforge_stockapp.pipeline:StockPredictionPipeline" }
"azuraforge.configs" = { stock_predictor = "azuraforge_stockapp.pipeline:get_default_config" }
========== FILE: app-stock-predictor/README.md ==========
# app-stock-predictor

========== FILE: app-stock-predictor/setup.py ==========
from setuptools import setup, find_packages
setup(
    package_dir={"": "src"},
    packages=find_packages(where="src"),
    # DÃœZELTME: Paket kurulduÄŸunda .yml gibi Python dÄ±ÅŸÄ± dosyalarÄ±n da
    # kopyalanmasÄ±nÄ± saÄŸlar. Bu, API ve Worker loglarÄ±ndaki hatayÄ± Ã§Ã¶zer.
    include_package_data=True, 
    package_data={
        # "azuraforge_stockapp" paketi iÃ§indeki tÃ¼m .yml dosyalarÄ±nÄ± dahil et.
        "azuraforge_stockapp": ["config/*.yml"], 
    },
)
========== FILE: app-stock-predictor/.github/workflows/ci.yml ==========
name: AzuraForge App Stock Predictor CI

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

jobs:
  build-and-test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.10", "3.11"]

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        # BaÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± kur (learner, dolayÄ±sÄ±yla core'u da Ã§ekecektir)
        pip install git+https://github.com/AzuraForge/learner.git@v0.1.3
        # Kendisini kur
        pip install -e .

    - name: Check code format with Black
      run: |
        pip install black
        black --check .
    
    - name: Lint with flake8
      run: |
        pip install flake8
        flake8 src --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 src --count --max-complexity=10 --max-line-length=127 --statistics

    - name: Test with pytest
      run: |
        # Bu repo iÃ§in de henÃ¼z test yazÄ±lmadÄ±.
        pip install pytest
        pytest
========== FILE: app-stock-predictor/src/azuraforge_stockapp/pipeline.py ==========
# app-stock-predictor/src/azuraforge_stockapp/pipeline.py

import logging
from typing import Any, Dict, Tuple, List

import yaml
from importlib import resources
import pandas as pd
import yfinance as yf

from azuraforge_learner import Sequential, LSTM, Linear
from azuraforge_learner.pipelines import TimeSeriesPipeline

def get_default_config() -> Dict[str, Any]:
    try:
        with resources.open_text("azuraforge_stockapp.config", "stock_predictor_config.yml") as f:
            return yaml.safe_load(f)
    except Exception as e:
        # Bu log, worker'Ä±n ana sÃ¼recinde gÃ¶rÃ¼nmeli
        logging.error(f"Hisse senedi uygulamasÄ± iÃ§in varsayÄ±lan config yÃ¼klenemedi: {e}", exc_info=True)
        return {"error": f"VarsayÄ±lan konfigÃ¼rasyon yÃ¼klenemedi: {e}"}

class StockPredictionPipeline(TimeSeriesPipeline):
    # DÃœZELTME: __init__ metodunu, temel sÄ±nÄ±fÄ± Ã§aÄŸÄ±rmak ve loglamayÄ± doÄŸrulamak iÃ§in geri ekliyoruz.
    def __init__(self, config: Dict[str, Any]):
        # Bu, BasePipeline'in __init__'ini Ã§aÄŸÄ±racak ve self.logger'Ä± ayarlayacaktÄ±r.
        super().__init__(config)
        self.logger.info(f"StockPredictionPipeline (Eklenti) baÅŸarÄ±yla baÅŸlatÄ±ldÄ±.")

    def _load_data_from_source(self) -> pd.DataFrame:
        """Sadece yfinance'ten veri Ã§ekme iÅŸini yapar. Caching bu metodun dÄ±ÅŸÄ±ndadÄ±r."""
        ticker = self.config.get("data_sourcing", {}).get("ticker", "MSFT")
        self.logger.info(f"'_load_data_from_source' Ã§aÄŸrÄ±ldÄ±. Ticker: {ticker}")
        
        data = yf.download(ticker, period="max", progress=False, actions=False, auto_adjust=True)
        if data.empty:
            self.logger.error(f"'{ticker}' iÃ§in yfinance'ten boÅŸ veri dÃ¶ndÃ¼.")
            raise ValueError(f"'{ticker}' iÃ§in veri indirilemedi.")
            
        self.logger.info(f"{len(data)} satÄ±r veri indirildi.")
        return data

    def get_caching_params(self) -> Dict[str, Any]:
        """Ã–nbellek anahtarÄ± iÃ§in sadece ticker'Ä±n yeterli olduÄŸunu belirtir."""
        ticker = self.config.get("data_sourcing", {}).get("ticker", "MSFT")
        self.logger.info(f"'get_caching_params' Ã§aÄŸrÄ±ldÄ±. Ticker: {ticker}")
        return {"ticker": ticker}

    def _get_target_and_feature_cols(self) -> Tuple[str, List[str]]:
        """Bu basit model iÃ§in hedef ve Ã¶zellik aynÄ± sÃ¼tundur: 'Close'."""
        self.logger.info("'_get_target_and_feature_cols' Ã§aÄŸrÄ±ldÄ±. Hedef: Close")
        return "Close", ["Close"]

    def _create_model(self, input_shape: Tuple) -> Sequential:
        """LSTM ve bir Linear katmandan oluÅŸan modeli oluÅŸturur."""
        self.logger.info(f"'_create_model' Ã§aÄŸrÄ±ldÄ±. Girdi ÅŸekli: {input_shape}")
        input_size = input_shape[2] 
        hidden_size = self.config.get("model_params", {}).get("hidden_size", 50)
        
        model = Sequential(
            LSTM(input_size=input_size, hidden_size=hidden_size),
            Linear(hidden_size, 1)
        )
        self.logger.info("LSTM modeli baÅŸarÄ±yla oluÅŸturuldu.")
        return model
========== FILE: app-stock-predictor/src/azuraforge_stockapp/__init__.py ==========

========== FILE: app-stock-predictor/src/azuraforge_stockapp/config/stock_predictor_config.yml ==========
# app-stock-predictor/src/azuraforge_stockapp/config/stock_predictor_config.yml

pipeline_name: "stock_predictor"

data_sourcing:
  ticker: "MSFT"

# YENÄ°: Ã–zellik mÃ¼hendisliÄŸi ve dÃ¶nÃ¼ÅŸÃ¼m ayarlarÄ±
feature_engineering:
  target_col_transform: "log" # "log" veya "none" olabilir

model_params:
  sequence_length: 60
  hidden_size: 50

training_params:
  epochs: 50
  lr: 0.001
  optimizer: "adam"
  test_size: 0.2
  validate_every: 5

system:
  caching_enabled: true
  cache_max_age_hours: 24
========== FILE: app-stock-predictor/src/azuraforge_stockapp/config/__init__.py ==========

========== FILE: applications/pyproject.toml ==========
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "azuraforge-applications"
version = "0.1.0"
description = "A catalog of official applications for the AzuraForge platform."

========== FILE: applications/README.md ==========
# applications

========== FILE: applications/setup.py ==========
from setuptools import setup, find_packages

setup(
    package_dir={"": "src"},
    packages=find_packages(where="src"),
    # EN Ã–NEMLÄ° KISIM: Paket kurulduÄŸunda .json dosyasÄ±nÄ±n da kopyalanmasÄ±nÄ± saÄŸlar
    include_package_data=True, 
    package_data={
        "azuraforge_applications": ["*.json"], # "azuraforge_apps_catalog" -> "azuraforge_applications"
    },
)


========== FILE: applications/src/azuraforge_applications/official_apps.json ==========
[
  {
    "id": "stock_predictor",
    "name": "Hisse Senedi Fiyat Tahmini",
    "repository": "https://github.com/AzuraForge/app-stock-predictor",
    "description": "LSTM tabanlÄ± hisse senedi fiyat tahmini yapar."
  },
  {
    "id": "weather_forecaster",
    "name": "Hava Durumu Tahmini",
    "repository": "https://github.com/AzuraForge/app-weather-forecaster",
    "description": "Gelecekteki hava durumunu tahmin eder (HenÃ¼z GeliÅŸtirilmedi)."
  }
]

========== FILE: applications/src/azuraforge_applications/__init__.py ==========


========== FILE: core/pyproject.toml ==========
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "azuraforge-core"
version = "0.1.3"
authors = [{ name = "Azmi Sahin" }]
description = "The core automatic differentiation engine (Tensor object) for the AzuraForge ecosystem."
readme = "README.md"
requires-python = ">=3.8"
license = { text = "MIT" }
classifiers = ["Programming Language :: Python :: 3"]
dependencies = ["numpy"]

# --- YENÄ° BÃ–LÃœM ---
[project.optional-dependencies]
dev = ["pytest"]

========== FILE: core/README.md ==========
# core

========== FILE: core/setup.py ==========
from setuptools import setup, find_packages
setup(
    package_dir={"": "src"},
    packages=find_packages(where="src"),
)

========== FILE: core/.github/workflows/ci.yml ==========
name: AzuraForge Core CI

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

jobs:
  build-and-test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.10", "3.11"]

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        # Core'un sadece test baÄŸÄ±mlÄ±lÄ±klarÄ± var (pytest)
        pip install -e .[dev]

    - name: Check code format with Black
      run: |
        pip install black
        black --check .
    
    - name: Lint with flake8
      run: |
        pip install flake8
        flake8 src --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 src --count --max-complexity=10 --max-line-length=127 --statistics

    - name: Test with pytest
      run: |
        pytest
========== FILE: core/src/azuraforge_core/tensor.py ==========
import os
from typing import Callable, List, Optional, Set, Tuple, Union, Any
import numpy as np
import logging # Loglama iÃ§in import et

# === DEÄžÄ°ÅžÄ°KLÄ°K BURADA: Cihaz algÄ±landÄ±ÄŸÄ±nda logla ===
# LoglamayÄ± yapÄ±landÄ±r
logging.basicConfig(level=logging.INFO, format='%(asctime)s - CORE - %(levelname)s - %(message)s')

DEVICE = os.environ.get("AZURAFORGE_DEVICE", "cpu").lower()

xp: Any
if DEVICE == "gpu":
    try:
        import cupy
        xp = cupy
        logging.info("âœ… AzuraForge Core: CuPy (GPU) backend successfully loaded.")
    except ImportError:
        import numpy
        xp = numpy
        logging.warning("âš ï¸ AzuraForge Core: AZURAFORGE_DEVICE set to 'gpu' but CuPy not found. Falling back to NumPy (CPU).")
        DEVICE = "cpu"
else:
    import numpy
    xp = numpy
    logging.info("â„¹ï¸ AzuraForge Core: NumPy (CPU) backend is active.")
# === DEÄžÄ°ÅžÄ°KLÄ°K SONU ===

ArrayType = Any
ScalarType = Union[int, float, bool, np.number, xp.number]

def _empty_backward_op() -> None: pass

class Tensor:
    def __init__(self, data: Any, _children: Tuple["Tensor", ...] = (), _op: str = "", requires_grad: bool = False):
        if isinstance(data, Tensor): self.data = data.data.copy()
        else: 
            # Veriyi doÄŸru cihaza taÅŸÄ±
            try:
                # EÄŸer xp, cupy ise bu veriyi GPU'ya taÅŸÄ±r.
                self.data = xp.array(data, dtype=np.float64)
            except Exception as e:
                # GPU'ya taÅŸÄ±ma sÄ±rasÄ±nda hata olursa (Ã¶rn. CUDA context hatasÄ±) logla
                logging.error(f"Error transferring data to device '{DEVICE}': {e}. Falling back to CPU.")
                self.data = np.array(data, dtype=np.float64)

        self.requires_grad = requires_grad
        self.grad: Optional[ArrayType] = xp.zeros_like(self.data) if requires_grad else None
        self._backward: Callable[[], None] = _empty_backward_op
        self._prev: Set["Tensor"] = set(_children)
        self._op: str = _op

    def backward(self, grad_output: Optional[ArrayType] = None) -> None:
        if not self.requires_grad: return
        topo: List[Tensor] = []
        visited: Set[Tensor] = set()
        def build_topo(v):
            if v not in visited:
                visited.add(v); [build_topo(child) for child in v._prev]; topo.append(v)
        build_topo(self)
        for t in topo:
            if t.grad is not None:
                t.grad.fill(0.0)
        
        self.grad = xp.ones_like(self.data) if grad_output is None else xp.asarray(grad_output, dtype=np.float64).reshape(self.data.shape)
        
        for v in reversed(topo):
            v._backward()

    def to_cpu(self) -> np.ndarray:
        if hasattr(self.data, 'get'): return self.data.get()
        return np.array(self.data, copy=True)

    def __add__(self, other: Any) -> "Tensor":
        other = _ensure_tensor(other)
        out = Tensor(self.data + other.data, (self, other), "+", self.requires_grad or other.requires_grad)
        def _backward():
            if self.requires_grad and self.grad is not None: self.grad += _unbroadcast_to(self.data.shape, out.grad)
            if other.requires_grad and other.grad is not None: other.grad += _unbroadcast_to(other.data.shape, out.grad)
        out._backward = _backward
        return out

    def __mul__(self, other: Any) -> "Tensor":
        other = _ensure_tensor(other)
        out = Tensor(self.data * other.data, (self, other), "*", self.requires_grad or other.requires_grad)
        def _backward():
            if self.requires_grad and self.grad is not None: self.grad += _unbroadcast_to(self.data.shape, other.data * out.grad)
            if other.requires_grad and other.grad is not None: other.grad += _unbroadcast_to(other.data.shape, self.data * out.grad)
        out._backward = _backward
        return out

    def __pow__(self, power: float) -> "Tensor":
        out = Tensor(self.data ** power, (self,), f"**{power}", self.requires_grad)
        def _backward():
            if self.requires_grad and self.grad is not None: self.grad += (power * (self.data ** (power - 1))) * out.grad
        out._backward = _backward
        return out

    def dot(self, other: "Tensor") -> "Tensor":
        other = _ensure_tensor(other)
        out = Tensor(self.data @ other.data, (self, other), "@", self.requires_grad or other.requires_grad)
        def _backward():
            if self.requires_grad and self.grad is not None: self.grad += out.grad @ other.data.T
            if other.requires_grad and other.grad is not None: other.grad += self.data.T @ out.grad
        out._backward = _backward
        return out

    def sum(self, axis=None, keepdims=False) -> "Tensor":
        out = Tensor(xp.sum(self.data, axis=axis, keepdims=keepdims), (self,), "sum", self.requires_grad)
        def _backward(_axis=axis, _keepdims=keepdims):
            if self.requires_grad and self.grad is not None:
                grad_val = out.grad
                if _axis is not None and not _keepdims:
                    grad_val = xp.expand_dims(grad_val, axis=_axis)
                self.grad += xp.ones_like(self.data) * grad_val
        out._backward = _backward
        return out

    def mean(self, axis=None, keepdims=False) -> "Tensor":
        sum_val = self.sum(axis=axis, keepdims=keepdims)
        num_elements = float(np.prod(self.data.shape) / np.prod(sum_val.data.shape))
        return sum_val * (1.0 / num_elements)
    
    def relu(self) -> "Tensor":
        out = Tensor(xp.maximum(0, self.data), (self,), "ReLU", self.requires_grad)
        def _backward():
            if self.requires_grad and self.grad is not None: self.grad += (self.data > 0) * out.grad
        out._backward = _backward
        return out

    def sigmoid(self) -> "Tensor":
        s = 1 / (1 + xp.exp(-self.data))
        out = Tensor(s, (self,), "Sigmoid", self.requires_grad)
        def _backward():
            if self.requires_grad and self.grad is not None: self.grad += out.data * (1 - out.data) * out.grad
        out._backward = _backward
        return out
    
    def tanh(self) -> "Tensor":
        t = xp.tanh(self.data)
        out = Tensor(t, (self,), "Tanh", self.requires_grad)
        def _backward():
            if self.requires_grad and self.grad is not None:
                self.grad += (1 - t**2) * out.grad
        out._backward = _backward
        return out
        
    def __repr__(self): return f"Tensor(data={self.data}, requires_grad={self.requires_grad})"
    def __neg__(self): return self * -1
    def __sub__(self, other): return self + (-other)
    def __truediv__(self, other): return self * (_ensure_tensor(other) ** -1)
    __radd__ = __add__
    def __rmul__(self, other): return self * other
    def __rsub__(self, other): return _ensure_tensor(other) - self
    def __rtruediv__(self, other): return _ensure_tensor(other) / self

def _ensure_tensor(val: Any) -> "Tensor":
    return val if isinstance(val, Tensor) else Tensor(val)

def _unbroadcast_to(target_shape: Tuple[int, ...], grad: ArrayType) -> ArrayType:
    if target_shape == grad.shape:
        return grad
    
    ndim_diff = grad.ndim - len(target_shape)
    if ndim_diff > 0:
        grad = grad.sum(axis=tuple(range(ndim_diff)))

    axes_to_sum = []
    for i, dim in enumerate(target_shape):
        if dim == 1 and grad.shape[i] > 1:
            axes_to_sum.append(i)
    
    if axes_to_sum:
        grad = grad.sum(axis=tuple(axes_to_sum), keepdims=True)
        
    return grad
========== FILE: core/src/azuraforge_core/__init__.py ==========
from .tensor import Tensor, xp, DEVICE, ArrayType, ScalarType, _unbroadcast_to

__all__ = ["Tensor", "xp", "DEVICE", "ArrayType", "ScalarType", "_unbroadcast_to"]

========== FILE: core/tests/azuraforge_core/test_tensor.py ==========
import pytest
import numpy as np

# Test edilecek paketi import et
from azuraforge_core import Tensor

def test_tensor_creation_and_defaults():
    """Tensor nesnesinin doÄŸru ÅŸekilde ve varsayÄ±lan deÄŸerlerle oluÅŸturulduÄŸunu test eder."""
    t = Tensor([1, 2, 3])
    assert isinstance(t.data, np.ndarray)
    assert t.requires_grad is False
    assert t.grad is None

def test_tensor_requires_grad():
    """`requires_grad=True` olduÄŸunda gradyan dizisinin oluÅŸturulduÄŸunu test eder."""
    t = Tensor([1, 2], requires_grad=True)
    assert t.requires_grad is True
    assert isinstance(t.grad, np.ndarray)
    assert np.array_equal(t.grad, np.array([0.0, 0.0]))

def test_addition_backward():
    """Basit toplama iÅŸlemi iÃ§in geri yayÄ±lÄ±mÄ±n doÄŸru Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± test eder."""
    a = Tensor([1, 2, 3], requires_grad=True)
    b = Tensor(5, requires_grad=True)
    
    # c = a.sum() + b  ->  dc/da = [1, 1, 1], dc/db = 1
    c = a.sum() + b
    
    c.backward()

    assert a.grad is not None
    assert b.grad is not None
    assert np.array_equal(a.grad, [1, 1, 1])
    assert b.grad == 1.0

def test_multiplication_backward():
    """Basit Ã§arpma iÅŸlemi iÃ§in geri yayÄ±lÄ±mÄ±n doÄŸru Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± test eder."""
    x = Tensor(2.0, requires_grad=True)
    y = Tensor(3.0, requires_grad=True)
    
    z = x * y
    
    z.backward() # dz/dx = y = 3,  dz/dy = x = 2

    assert x.grad == 3.0
    assert y.grad == 2.0

def test_chained_rule_backward():
    """Zincir kuralÄ±nÄ±n birden Ã§ok iÅŸlemde doÄŸru Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± test eder."""
    x = Tensor(2.0, requires_grad=True)
    y = Tensor(3.0, requires_grad=True)

    z = x * y  # dz/dx = y, dz/dy = x
    q = z + x  # dq/dz = 1, dq/dx = 1
    
    # Zincir KuralÄ±:
    # dq/dx = (dq/dz * dz/dx) + dq/dx = (1 * y) + 1 = 3 + 1 = 4
    # dq/dy = (dq/dz * dz/dy) = 1 * x = 2
    q.backward()

    assert x.grad == 4.0
    assert y.grad == 2.0

def test_dot_product_backward():
    """Matris Ã§arpÄ±mÄ± iÃ§in geri yayÄ±lÄ±mÄ± test eder."""
    a_data = np.random.randn(2, 3)
    b_data = np.random.randn(3, 4)
    a = Tensor(a_data, requires_grad=True)
    b = Tensor(b_data, requires_grad=True)
    
    c = a.dot(b)
    
    # GradyanÄ± 1'lerden oluÅŸan bir matrisle baÅŸlat
    c.backward(np.ones_like(c.data))
    
    # Manuel olarak hesaplanan gradyanlar
    grad_a_manual = np.ones_like(c.data) @ b_data.T
    grad_b_manual = a_data.T @ np.ones_like(c.data)
    
    assert np.allclose(a.grad, grad_a_manual)
    assert np.allclose(b.grad, grad_b_manual)


def test_relu_backward():
    """ReLU aktivasyonu iÃ§in geri yayÄ±lÄ±mÄ± test eder."""
    a = Tensor([-1, 0, 5], requires_grad=True)
    r = a.relu()
    r.backward(np.array([10, 20, 30]))

    # Gradyan sadece pozitif deÄŸerler iÃ§in akar (self.data > 0)
    assert np.array_equal(a.grad, [0, 0, 30])

========== FILE: dashboard/eslint.config.js ==========
import js from '@eslint/js'
import globals from 'globals'
import reactHooks from 'eslint-plugin-react-hooks'
import reactRefresh from 'eslint-plugin-react-refresh'
import { defineConfig, globalIgnores } from 'eslint/config'

export default defineConfig([
  globalIgnores(['dist']),
  {
    files: ['**/*.{js,jsx}'],
    extends: [
      js.configs.recommended,
      reactHooks.configs['recommended-latest'],
      reactRefresh.configs.vite,
    ],
    languageOptions: {
      ecmaVersion: 2020,
      globals: globals.browser,
      parserOptions: {
        ecmaVersion: 'latest',
        ecmaFeatures: { jsx: true },
        sourceType: 'module',
      },
    },
    rules: {
      'no-unused-vars': ['error', { varsIgnorePattern: '^[A-Z_]' }],
    },
  },
])

========== FILE: dashboard/index.html ==========
<!doctype html>
<html lang="tr">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/favicon.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>AzuraForge | MLOps Dashboard</title>
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.jsx"></script>
  </body>
</html>
========== FILE: dashboard/package.json ==========
{
  "name": "dashboard",
  "private": true,
  "version": "0.0.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "vite build",
    "lint": "eslint .",
    "preview": "vite preview"
  },
  "dependencies": {
    "@fontsource/inter": "^5.2.6",
    "axios": "^1.10.0",
    "chart.js": "^4.5.0",
    "chartjs-adapter-date-fns": "^3.0.0",
    "chartjs-plugin-annotation": "^3.1.0",
    "chartjs-plugin-zoom": "^2.2.0",
    "date-fns": "^4.1.0",
    "prop-types": "^15.8.1",
    "react": "^19.1.0",
    "react-chartjs-2": "^5.3.0",
    "react-dom": "^19.1.0",
    "react-markdown": "^10.1.0",
    "react-router-dom": "^7.6.3",
    "react-toastify": "^11.0.5",
    "remark-gfm": "^4.0.1"
  },
  "devDependencies": {
    "@eslint/js": "^9.30.0",
    "@types/react": "^19.1.8",
    "@types/react-dom": "^19.1.6",
    "@vitejs/plugin-react": "^4.6.0",
    "eslint": "^9.30.0",
    "eslint-plugin-react-hooks": "^5.2.0",
    "eslint-plugin-react-refresh": "^0.4.20",
    "globals": "^16.2.0",
    "npm-check-updates": "^18.0.1",
    "vite": "^7.0.0"
  }
}

========== FILE: dashboard/README.md ==========
# AzuraForge Dashboard

Bu proje, AzuraForge platformunun kullanÄ±cÄ± arayÃ¼zÃ¼dÃ¼r. React ve Vite kullanÄ±larak geliÅŸtirilmiÅŸtir.

## âœ¨ Ana Yetenekler

*   Sistemde mevcut olan tÃ¼m AI pipeline'larÄ±nÄ± listeleme ve yeni deneyler baÅŸlatma.
*   GeÃ§miÅŸte Ã§alÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ tÃ¼m deneylerin sonuÃ§larÄ±nÄ± gÃ¶rÃ¼ntÃ¼leme ve filtreleme.
*   SeÃ§ilen deneyleri tek bir ekranda karÅŸÄ±laÅŸtÄ±rarak performanslarÄ±nÄ± analiz etme.
*   Devam eden bir deneyi, anlÄ±k kayÄ±p ve tahmin grafikleriyle **canlÄ± olarak takip etme**.
*   Tamamlanan deneyler iÃ§in oluÅŸturulmuÅŸ interaktif raporlarÄ± gÃ¶rÃ¼ntÃ¼leme.

## ðŸš€ Yerel GeliÅŸtirme OrtamÄ±

Bu projeyi yerel ortamda Ã§alÄ±ÅŸtÄ±rmak iÃ§in, ana `platform` reposundaki **[GeliÅŸtirme Rehberi](../../platform/docs/DEVELOPMENT_GUIDE.md)**'ni takip edin.

Proje baÄŸÄ±mlÄ±lÄ±klarÄ± kurulduktan sonra, aÅŸaÄŸÄ±daki komutlarla geliÅŸtirme sunucusunu baÅŸlatabilir ve testleri Ã§alÄ±ÅŸtÄ±rabilirsiniz.

**GeliÅŸtirme Sunucusunu BaÅŸlatma:**
```bash
# dashboard/ kÃ¶k dizinindeyken
npm run dev
```
Uygulama `http://localhost:5173` adresinde eriÅŸilebilir olacaktÄ±r.

**Lint KontrolÃ¼:**
```bash
npm run lint
```

**Not:** Dashboard'un tam olarak Ã§alÄ±ÅŸabilmesi iÃ§in `api` servisinin `http://localhost:8000` adresinde Ã§alÄ±ÅŸÄ±yor olmasÄ± gerekmektedir.
```

========== FILE: dashboard/vite.config.js ==========
import { defineConfig } from 'vite'
import react from '@vitejs/plugin-react'

// https://vite.dev/config/
export default defineConfig({
  plugins: [react()],
})

========== FILE: dashboard/src/App.css ==========
/* ==========================================================================
   1. KÃ–K DEÄžÄ°ÅžKENLER (TasarÄ±m Sistemi)
   ========================================================================== */
:root {
  /* Koyu Tema (VarsayÄ±lan) */
  --primary-color: #42b983; --primary-color-dark: #369c70; --secondary-color: #3b82f6;
  --text-color: #e2e8f0; --text-color-darker: #94a3b8; --text-inverse: #ffffff;
  --bg-color: #0f172a; --content-bg: #1e293b; --border-color: #334155; --hover-bg: #2a3a52;
  --success-color: #22c55e; --error-color: #ef4444; --warning-color: #f59e0b; --info-color: #3b82f6;
  --font-sans: 'Inter', system-ui, sans-serif; --font-mono: ui-monospace, Menlo, monospace;
  --shadow-md: 0 4px 6px -1px rgb(0 0 0 / 0.1), 0 2px 4px -2px rgb(0 0 0 / 0.1);
  --shadow-lg: 0 10px 15px -3px rgb(0 0 0 / 0.1), 0 4px 6px -2px rgb(0 0 0 / 0.1);
  --border-radius: 12px;
}
body.light-theme {
  /* AÃ§Ä±k Tema */
  --text-color: #1e293b; --text-color-darker: #475569; --bg-color: #f8fafc;
  --content-bg: #ffffff; --border-color: #e2e8f0; --hover-bg: #f1f5f9;
  --success-color: #16a34a; --error-color: #dc2626; --warning-color: #d97706; --info-color: #2563eb;
}
/* ==========================================================================
   2. TEMEL RESET VE GLOBAL STÄ°LLER
   ========================================================================== */
*, *::before, *::after { box-sizing: border-box; }
body {
  margin: 0; font-family: var(--font-sans); background-color: var(--bg-color); color: var(--text-color);
  font-synthesis: none; text-rendering: optimizeLegibility; -webkit-font-smoothing: antialiased;
  transition: background-color 0.3s, color 0.3s;
}
#root, .app-layout { display: flex; width: 100vw; height: 100vh; overflow: hidden; }
/* ==========================================================================
   3. ANA YAPI (Layout)
   ========================================================================== */
.sidebar {
  width: 260px; background-color: var(--content-bg); border-right: 1px solid var(--border-color);
  padding: 20px; display: flex; flex-direction: column; flex-shrink: 0;
  transition: background-color 0.3s, border-color 0.3s, width 0.3s ease; z-index: 2000;
}
.sidebar nav ul { list-style: none; padding: 0; margin: 0; }
.sidebar nav a {
  display: flex; align-items: center; gap: 15px; padding: 12px 15px; text-decoration: none;
  color: var(--text-color-darker); border-radius: 8px; margin-bottom: 8px;
  transition: background-color 0.2s ease, color 0.2s ease; font-weight: 500;
}
.sidebar nav a:hover { background-color: var(--hover-bg); color: var(--text-color); }
.sidebar nav a.active { background-color: var(--primary-color); color: var(--text-inverse); font-weight: 600; }
.main-content { flex-grow: 1; padding: 30px; overflow-y: auto; position: relative; }
.page-header { margin-bottom: 30px; }
.page-header h1 {
  font-size: 2.2em; font-weight: 700; color: var(--text-color); margin: 0 0 5px 0;
  display: flex; align-items: center; gap: 15px; transition: color 0.3s;
}
.page-header p { color: var(--text-color-darker); font-size: 1.1em; margin: 0; }
/* ==========================================================================
   4. LOGO VE TEMA DEÄžÄ°ÅžTÄ°RME
   ========================================================================== */
.logo-container {
  display: flex; align-items: center; justify-content: flex-start; gap: 12px;
  margin-bottom: 40px; padding: 0 10px;
}
.logo-container h1 {
  color: var(--text-color); font-size: 1.6em; font-weight: 600; margin: 0;
  letter-spacing: 0.5px; transition: color 0.3s;
}
.theme-toggle-button {
  background-color: var(--bg-color); color: var(--text-color-darker); border: 1px solid var(--border-color);
  border-radius: 8px; padding: 8px; cursor: pointer; display: flex; align-items: center;
  justify-content: center; margin-top: 20px; transition: all 0.2s ease;
}
.theme-toggle-button:hover { border-color: var(--primary-color); color: var(--primary-color); }
/* ==========================================================================
   5. BÄ°LEÅžENLER (Components)
   ========================================================================== */
.card, .table-container, .form-container, .pipeline-details {
  background-color: var(--content-bg); border: 1px solid var(--border-color);
  border-radius: var(--border-radius); box-shadow: var(--shadow-md);
  transition: background-color 0.3s, border-color 0.3s;
}
.table-container { padding: 0; }
.card { padding: 25px; }
.button-primary {
  background-color: var(--primary-color); color: var(--text-inverse); padding: 10px 20px; border: none;
  border-radius: 8px; cursor: pointer; font-size: 1em; font-weight: 600;
  transition: background-color 0.2s ease, transform 0.2s ease; display: inline-flex;
  align-items: center; gap: 8px;
}
.button-primary:hover:not(:disabled) { background-color: var(--primary-color-dark); transform: translateY(-2px); }
.button-primary:disabled { background-color: var(--border-color); cursor: not-allowed; opacity: 0.6; transform: none; }
.status-badge {
  display: inline-flex; align-items: center; gap: 6px; padding: 4px 12px; border-radius: 9999px;
  font-size: 0.8em; font-weight: 600; text-transform: uppercase; justify-content: center;
}
.status-badge::before { content: ''; display: inline-block; width: 8px; height: 8px; border-radius: 50%; }
.status-badge.status-started, .status-badge.status-progress, .status-badge.status-connecting { background-color: rgba(59, 130, 246, 0.2); color: #60a5fa; }
.status-badge.status-started::before, .status-badge.status-progress::before, .status-badge.status-connecting::before { background-color: var(--info-color); animation: pulse 2s infinite; }
@keyframes pulse { 0%, 100% { opacity: 1; } 50% { opacity: 0.5; } }
.status-badge.status-success { background-color: rgba(34, 197, 94, 0.2); color: #4ade80; }
.status-badge.status-success::before { background-color: var(--success-color); }
.status-badge.status-failure, .status-badge.status-error { background-color: rgba(239, 68, 68, 0.2); color: #f87171; }
.status-badge.status-failure::before, .status-badge.status-error::before { background-color: var(--error-color); }
.status-badge.status-pending, .status-badge.status-unknown, .status-badge.status-disconnected { background-color: rgba(148, 163, 184, 0.2); color: var(--text-color-darker); }
.status-badge.status-pending::before, .status-badge.status-unknown::before, .status-badge.status-disconnected::before { background-color: var(--text-color-darker); }
.form-group { margin-bottom: 20px; }
.form-group label { display: block; margin-bottom: 8px; font-weight: 500; color: var(--text-color-darker); }
.form-group input, .form-group select {
  width: 100%; padding: 12px; background-color: var(--bg-color); border: 1px solid var(--border-color);
  border-radius: 8px; color: var(--text-color); font-size: 1em; font-family: var(--font-sans);
  transition: background-color 0.3s, border-color 0.3s, color 0.3s;
}
.form-group input:focus, .form-group select:focus {
  outline: none; border-color: var(--primary-color); box-shadow: 0 0 0 2px color-mix(in srgb, var(--primary-color) 30%, transparent);
}
table { width: 100%; border-collapse: collapse; }
table th, table td { padding: 12px 20px; text-align: left; border-bottom: 1px solid var(--border-color); vertical-align: middle; transition: border-color 0.3s; }
table th {
  background-color: var(--bg-color); color: var(--text-color-darker); font-weight: 600;
  text-transform: uppercase; font-size: 0.8em; letter-spacing: 0.5px;
  transition: background-color 0.3s, color 0.3s;
}
table tbody tr:hover { background-color: var(--hover-bg); }
table tbody tr.selected-row { background-color: color-mix(in srgb, var(--primary-color) 10%, transparent); border-left: 3px solid var(--primary-color); }
.actions-cell { position: relative; text-align: right !important; }
.actions-button {
  background: none; border: none; font-size: 24px; line-height: 1; color: var(--text-color-darker);
  cursor: pointer; border-radius: 4px; padding: 0 8px;
}
.actions-button:hover { background-color: var(--border-color); color: var(--text-color); }
.actions-menu {
  position: absolute; right: 20px; top: 50px; background-color: var(--hover-bg); border: 1px solid var(--border-color);
  border-radius: 8px; box-shadow: var(--shadow-lg); z-index: 100; display: flex;
  flex-direction: column; padding: 8px; min-width: 180px;
}
.actions-menu button {
  background: none; border: none; color: var(--text-color); padding: 10px 15px; text-align: left;
  cursor: pointer; border-radius: 6px; display: flex; align-items: center; gap: 10px; font-size: 0.9em;
}
.actions-menu button:hover { background-color: var(--secondary-color); color: var(--text-inverse); }
/* ==========================================================================
   6. Ã–ZEL PANELLER VE MODALLAR
   ========================================================================== */
.live-tracker-pane {
  position: sticky; top: -30px; z-index: 1000; margin: -30px -30px 30px -30px;
  padding: 20px 30px; background: color-mix(in srgb, var(--content-bg) 70%, transparent);
  backdrop-filter: blur(10px); -webkit-backdrop-filter: blur(10px);
  border-bottom: 1px solid var(--border-color); box-shadow: var(--shadow-lg);
}
.comparison-modal-overlay {
  position: fixed; top: 0; left: 0; right: 0; bottom: 0;
  background-color: color-mix(in srgb, var(--bg-color) 70%, transparent); backdrop-filter: blur(8px);
  z-index: 5000; display: flex; align-items: center; justify-content: center;
}
.comparison-modal-content {
  background-color: var(--bg-color); border: 1px solid var(--border-color); border-radius: 16px;
  width: 90%; max-width: 1200px; height: 90vh; box-shadow: var(--shadow-lg);
  display: flex; flex-direction: column;
}
.comparison-header {
  display: flex; justify-content: space-between; align-items: center; padding: 20px 30px;
  border-bottom: 1px solid var(--border-color); flex-shrink: 0;
}
.comparison-header h2 { margin: 0; font-size: 1.5em; }
.close-button {
  background: none; border: none; font-size: 24px; color: var(--text-color-darker);
  cursor: pointer; transition: color 0.2s;
}
.close-button:hover { color: var(--text-color); }
.comparison-body {
  padding: 30px; flex-grow: 1; overflow-y: auto; display: flex;
  flex-direction: column; gap: 30px;
}
.comparison-chart-container { height: 400px; min-height: 300px; width: 100%; position: relative; }
.chart-instructions {
  position: absolute; bottom: 5px; right: 10px; font-size: 0.75em;
  color: var(--text-color-darker); background-color: color-mix(in srgb, var(--content-bg) 80%, transparent);
  padding: 2px 8px; border-radius: 4px; opacity: 0.7;
}
.color-indicator {
  display: inline-block; width: 12px; height: 12px; border-radius: 50%;
  margin-right: 10px; vertical-align: middle;
}
/* ==========================================================================
   7. ÃœÃ‡ÃœNCÃœ PARTÄ° KÃœTÃœPHANE STÄ°LLERÄ°
   ========================================================================== */
.Toastify__toast {
  background-color: var(--content-bg) !important; color: var(--text-color) !important;
  border: 1px solid var(--border-color) !important; border-radius: 8px !important;
  font-family: var(--font-sans) !important;
}
.Toastify__progress-bar { background: var(--primary-color) !important; }
.Toastify__close-button { color: var(--text-color) !important; }

/* ==========================================================================
   8. YENÄ° DENEY SAYFASI Ã–ZEL STÄ°LLERÄ°
   ========================================================================== */

.new-experiment-layout {
  display: flex;
  flex-direction: column;
  height: 100%; /* Ana konteynerin tÃ¼m yÃ¼ksekliÄŸi kaplamasÄ± iÃ§in */
}

.new-experiment-form {
  flex-grow: 1; /* Kalan tÃ¼m alanÄ± kapla */
  display: flex;
  flex-direction: column;
  overflow: hidden; /* Ä°Ã§erik taÅŸmasÄ±nÄ± engelle */
  border-radius: var(--border-radius);
  background-color: var(--content-bg);
  border: 1px solid var(--border-color);
}

.form-main-content {
  flex-grow: 1;
  overflow-y: auto; /* Sadece bu alan kaydÄ±rÄ±labilir olacak */
  padding: 25px;
  display: flex;
  flex-direction: column;
  gap: 25px;
}

.form-action-bar {
  flex-shrink: 0; /* Bu panelin kÃ¼Ã§Ã¼lmesini engelle */
  padding: 15px 25px;
  background-color: var(--bg-color);
  border-top: 1px solid var(--border-color);
  display: flex;
  justify-content: space-between;
  align-items: center;
  transition: background-color 0.3s, border-color 0.3s;
}

.pipeline-info {
  display: flex;
  align-items: center;
  gap: 10px;
  color: var(--text-color-darker);
}
.pipeline-info span {
  font-weight: 600;
  color: var(--text-color);
}

/* Form iÃ§indeki iÃ§ iÃ§e gruplar iÃ§in daha iyi stiller */
.form-fieldset {
  border: 1px solid var(--border-color);
  border-radius: 8px;
  padding: 20px;
  margin: 0 0 20px 0;
}
.form-fieldset legend {
  padding: 0 10px;
  font-weight: 600;
  color: var(--text-color);
}

/* ... mevcut CSS kodunuz ... */

.actions-menu button, .actions-menu-button {
  background: none; border: none; color: var(--text-color); padding: 10px 15px; text-align: left;
  cursor: pointer; border-radius: 6px; display: flex; align-items: center; gap: 10px; font-size: 0.9em;
  width: 100%; text-decoration: none;
}
.actions-menu button:hover, .actions-menu-button:hover { background-color: var(--secondary-color); color: var(--text-inverse); }
========== FILE: dashboard/src/App.jsx ==========
// dashboard/src/App.jsx

import { useState, useContext } from 'react';
import { Routes, Route, Link, useNavigate, useLocation } from 'react-router-dom';
import { ToastContainer } from 'react-toastify';
import 'react-toastify/dist/ReactToastify.css';

import './App.css'; 
import { ThemeContext } from './context/ThemeContext';
import NewExperiment from './pages/NewExperiment';
import DashboardOverview from './pages/DashboardOverview';
import ReportViewer from './pages/ReportViewer'; // YENÄ°
import LiveTrackerPane from './components/LiveTrackerPane';
import Logo from './components/Logo';
import ThemeToggle from './components/ThemeToggle';

function App() {
  const [trackingTaskId, setTrackingTaskId] = useState(null);
  const navigate = useNavigate();
  const location = useLocation();
  const { theme } = useContext(ThemeContext);

  const handleExperimentStarted = (taskId) => {
    if (taskId) setTrackingTaskId(taskId);
  };
  
  const handleCloseTracker = () => {
    setTrackingTaskId(null);
  };

  const isActive = (path) => location.pathname.startsWith(path) && path !== '/' || location.pathname === path;

  return (
    <div className="app-layout">
      <ToastContainer position="bottom-right" autoClose={5000} theme={theme} />
      <aside className="sidebar">
        <Logo />
        <nav style={{ flexGrow: 1 }}>
          <ul>
            <li><Link to="/" className={location.pathname === '/' ? 'active' : ''}>
                <span role="img" aria-label="dashboard">ðŸ“Š</span><span>Genel BakÄ±ÅŸ</span>
            </Link></li>
            <li><Link to="/new-experiment" className={isActive('/new-experiment') ? 'active' : ''}>
                <span role="img" aria-label="rocket">ðŸš€</span><span>Yeni Deney</span>
            </Link></li>
          </ul>
        </nav>
        <ThemeToggle />
      </aside>
      <main className="main-content">
        {trackingTaskId && <LiveTrackerPane taskId={trackingTaskId} onClose={handleCloseTracker} />}
        <Routes>
          <Route path="/" element={<DashboardOverview setTrackingTaskId={setTrackingTaskId} />} />
          <Route path="/new-experiment" element={<NewExperiment onExperimentStarted={handleExperimentStarted} />} />
          {/* YENÄ° ROUTE */}
          <Route path="/reports/:experimentId" element={<ReportViewer />} />
        </Routes>
      </main>
    </div>
  );
}

export default App;
========== FILE: dashboard/src/index.css ==========
/*
  Bu dosya, gelecekte Ã§ok temel, uygulama geneli reset kurallarÄ± iÃ§in kullanÄ±labilir.
  Åžimdilik, projenin tÃ¼m Ã¶zel stil mantÄ±ÄŸÄ± App.css dosyasÄ±nda merkezileÅŸtirilmiÅŸtir.
*/
========== FILE: dashboard/src/main.jsx ==========
import React from 'react';
import { createRoot } from 'react-dom/client';
import { BrowserRouter } from 'react-router-dom';
import { ThemeProvider } from './context/ThemeContext';

// Fontsource
import '@fontsource/inter/400.css';
import '@fontsource/inter/500.css';
import '@fontsource/inter/600.css';
import '@fontsource/inter/700.css';

// Stil dosyalarÄ±
import './index.css'; 
import './App.css';
import App from './App.jsx';

createRoot(document.getElementById('root')).render(
  // StrictMode'u bilinÃ§li olarak kapalÄ± tutuyoruz
  // <React.StrictMode>
    <ThemeProvider>
      <BrowserRouter> 
        <App />
      </BrowserRouter>
    </ThemeProvider>
  // </React.StrictMode>
);
========== FILE: dashboard/src/components/ComparisonView.jsx ==========
// Bu dosyanÄ±n iÃ§eriÄŸi Ã¶nceki cevaptaki ile aynÄ±, tam halini tekrar veriyorum
import PropTypes from 'prop-types';
import { Line } from 'react-chartjs-2';
import { Chart as ChartJS, CategoryScale, LinearScale, PointElement, LineElement, Title, Tooltip, Legend } from 'chart.js';
import zoomPlugin from 'chartjs-plugin-zoom';

ChartJS.register(CategoryScale, LinearScale, PointElement, LineElement, Title, Tooltip, Legend, zoomPlugin);

const chartColors = ['#42b983', '#3b82f6', '#ef4444', '#f59e0b', '#8b5cf6', '#ec4899'];

function ComparisonView({ experiments, onClose }) {
  const chartData = {
    labels: Array.from({ length: Math.max(...experiments.map(e => e.results?.loss?.length || 0)) }, (_, i) => `E${i + 1}`),
    datasets: experiments.map((exp, i) => ({
      label: `${exp.config.data_sourcing.ticker} (${exp.experiment_id.slice(-6)})`,
      data: exp.results?.loss || [],
      borderColor: chartColors[i % chartColors.length],
      backgroundColor: `${chartColors[i % chartColors.length]}33`,
      tension: 0.1, fill: false, borderWidth: 2, pointRadius: 1, pointHoverRadius: 5,
    })),
  };

  const chartOptions = {
      responsive: true, maintainAspectRatio: false,
      interaction: { mode: 'index', intersect: false, },
      plugins: { 
        legend: { position: 'top', labels: { font: { size: 14 } } },
        tooltip: {
          backgroundColor: 'var(--content-bg)',
          borderColor: 'var(--border-color)',
          borderWidth: 1,
        },
        zoom: {
          pan: { enabled: true, mode: 'xy', modifierKey: 'alt', },
          zoom: { wheel: { enabled: true }, pinch: { enabled: true }, mode: 'xy' }
        }
      },
      scales: {
          y: { title: { display: true, text: 'KayÄ±p DeÄŸeri (Loss)' }, beginAtZero: false, }, 
          x: { title: { display: true, text: 'Epoch' }, grid: { display: false } } 
      }
  };

  return (
    <div className="comparison-modal-overlay" onClick={onClose}>
      <div className="comparison-modal-content" onClick={e => e.stopPropagation()}>
        <div className="comparison-header">
          <h2>Deney KarÅŸÄ±laÅŸtÄ±rmasÄ± ({experiments.length} adet)</h2>
          <button className="close-button" onClick={onClose}>Ã—</button>
        </div>
        <div className="comparison-body">
          <div className="comparison-chart-container">
            <Line data={chartData} options={chartOptions} />
            <p className="chart-instructions">YakÄ±nlaÅŸtÄ±rmak iÃ§in fare tekerleÄŸini kullanÄ±n. SÄ±fÄ±rlamak iÃ§in Ã§ift tÄ±klayÄ±n. KaydÄ±rmak iÃ§in <strong>Alt + SÃ¼rÃ¼kle</strong>.</p>
          </div>
          <h4 className="section-title" style={{ marginTop: 0 }}>Ã–zet Tablosu</h4>
          <div className="table-container">
            <table>
              <thead><tr><th>Deney ID</th><th>Ticker</th><th>Epochs</th><th>LR</th><th>Final KayÄ±p</th></tr></thead>
              <tbody>
                {experiments.map((exp, i) => (
                  <tr key={exp.experiment_id}>
                    <td><span className="color-indicator" style={{backgroundColor: chartColors[i % chartColors.length]}}></span>{exp.experiment_id.slice(0, 18)}...</td>
                    <td>{exp.config.data_sourcing.ticker}</td>
                    <td>{exp.config.training_params.epochs}</td>
                    <td>{exp.config.training_params.lr}</td>
                    <td>{exp.results.final_loss?.toFixed(6) ?? 'N/A'}</td>
                  </tr>
                ))}
              </tbody>
            </table>
          </div>
        </div>
      </div>
    </div>
  );
}
ComparisonView.propTypes = { experiments: PropTypes.array.isRequired, onClose: PropTypes.func.isRequired, };
export default ComparisonView;
========== FILE: dashboard/src/components/ExperimentRow.jsx ==========
import { useState } from 'react';
import PropTypes from 'prop-types';
import { toast } from 'react-toastify';
import { Link } from 'react-router-dom';

const Icon = ({ path, className }) => <svg className={className} width="16" height="16" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d={path} /></svg>;
Icon.propTypes = { path: PropTypes.string.isRequired, className: PropTypes.string };

const ICONS = {
  rerun: "M12 4V1L8 5l4 4V6c3.31 0 6 2.69 6 6s-2.69 6-6 6-6-2.69-6-6H4c0 4.42 3.58 8 8 8s8-3.58 8-8-3.58-8-8-8z",
  copy: "M16 1H4c-1.1 0-2 .9-2 2v14h2V3h12V1zm3 4H8c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h11c1.1 0 2-.9 2-2V7c0-1.1-.9-2-2-2zm0 16H8V7h11v14z",
  satellite: "M6.34 5.34L4.93 3.93l-1.41 1.41 1.41 1.41C3.89 7.79 3.5 9.05 3.5 10.41c0 .46.06.91.16 1.34l-2.48 1.43c-.22.13-.34.38-.34.65v1.14c0 .27.11.52.34.65l2.48 1.43c-.1.43-.16.88-.16 1.34 0 2.21 1.79 4 4 4s4-1.79 4-4c0-1.37-.69-2.63-1.76-3.34l1.41-1.41-1.41-1.41-1.41 1.41C9.11 6.1 9.5 4.95 9.5 3.59c0-.46-.06-.91-.16-1.34l2.48-1.43c.22-.13.34-.38.34-.65V-.98c0-.27-.11-.52-.34-.65L9.34.8c.1-.43.16-.88.16-1.34 0-2.21-1.79-4-4-4s-4 1.79-4 4c0 1.37.69 2.63 1.76 3.34zm.24 9.35l-1.24-.71c.12-.52.19-1.06.19-1.61s-.07-1.09-.19-1.61l1.24-.71C7.58 10.9 8.5 12.05 8.5 13.41s-.92 2.51-1.92 3.28zM12 17.5c-2.21 0-4-1.79-4-4s1.79-4 4-4 4 1.79 4 4-1.79 4-4 4z",
  report: "M20 2H4c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h16c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z"
};

function ExperimentRow({ experiment, isSelected, onSelect, setTrackingTaskId }) {
  const [actionsOpen, setActionsOpen] = useState(false);
  const { 
    experiment_id, status, task_id, pipeline_name,
    created_at, completed_at, failed_at,
    config_summary, results_summary
  } = experiment;

  const handleCopyId = () => {
    navigator.clipboard.writeText(experiment_id);
    toast.success('Deney ID panoya kopyalandÄ±!');
    setActionsOpen(false);
  };

  const isRunning = ['STARTED', 'PROGRESS', 'PENDING'].includes(status);
  const finalLoss = results_summary?.final_loss;
  const displayLoss = (finalLoss !== null && finalLoss !== undefined) ? finalLoss.toFixed(6) : 'N/A';
  const startTime = created_at ? new Date(created_at).toLocaleString() : 'N/A';
  const endTime = completed_at || failed_at ? new Date(completed_at || failed_at).toLocaleString() : 'N/A';

  return (
    <tr className={isSelected ? 'selected-row' : ''}>
      <td><input type="checkbox" checked={isSelected} onChange={onSelect} title="KarÅŸÄ±laÅŸtÄ±rmak iÃ§in seÃ§"/></td>
      <td><span className={`status-badge status-${status?.toLowerCase() || 'unknown'}`}>{status || 'Bilinmiyor'}</span></td>
      <td><div className="detail-cell"><strong>{pipeline_name || 'N/A'}</strong><span className="exp-id">{experiment_id}</span></div></td>
      <td><div className="detail-cell"><span>Ticker: <strong>{config_summary?.ticker || 'N/A'}</strong></span><span>Epochs: <strong>{config_summary?.epochs || 'N/A'}</strong></span></div></td>
      <td><div className="detail-cell"><span>Final KayÄ±p: <strong>{displayLoss}</strong></span></div></td>
      <td><div className="detail-cell"><span>BaÅŸlangÄ±Ã§: {startTime}</span><span>BitiÅŸ: {endTime}</span></div></td>
      <td className="actions-cell">
        <button className="actions-button" onClick={() => setActionsOpen(!actionsOpen)}>â‹®</button>
        {actionsOpen && (
          <div className="actions-menu" onMouseLeave={() => setActionsOpen(false)}>
            {status === 'SUCCESS' && (
              <Link to={`/reports/${experiment_id}`} className="actions-menu-button">
                  <Icon path={ICONS.report} /> Raporu GÃ¶rÃ¼ntÃ¼le
              </Link>
            )}
            {isRunning && <button onClick={() => { setTrackingTaskId(task_id); setActionsOpen(false); }}><Icon path={ICONS.satellite} /> CanlÄ± Ä°zle</button>}
            <button onClick={handleCopyId}><Icon path={ICONS.copy} /> ID'yi Kopyala</button>
          </div>
        )}
      </td>
    </tr>
  );
}

ExperimentRow.propTypes = {
  experiment: PropTypes.object.isRequired,
  isSelected: PropTypes.bool.isRequired,
  onSelect: PropTypes.func.isRequired,
  setTrackingTaskId: PropTypes.func.isRequired,
};

export default ExperimentRow;
========== FILE: dashboard/src/components/ExperimentsList.jsx ==========
import PropTypes from 'prop-types';
import ExperimentRow from './ExperimentRow';

function ExperimentsList({ experiments, selectedIds, onSelect, setTrackingTaskId }) {
  if (!experiments || experiments.length === 0) {
    return <p style={{textAlign: 'center', padding: '20px'}}>Filtrelerinize uyan bir deney bulunamadÄ±.</p>;
  }
  return (
    <div className="table-container">
      <table>
        <thead>
          <tr>
            <th style={{width: '40px'}}></th>
            <th>Durum</th>
            <th>Deney DetaylarÄ±</th>
            <th>Parametreler</th>
            <th>SonuÃ§lar</th>
            <th>Zamanlama</th>
            <th style={{width: '50px'}}>Aksiyon</th>
          </tr>
        </thead>
        <tbody>
          {experiments.map((exp) => (
            <ExperimentRow 
              key={exp.experiment_id} 
              experiment={exp} 
              isSelected={selectedIds.has(exp.experiment_id)}
              onSelect={() => onSelect(exp.experiment_id)}
              setTrackingTaskId={setTrackingTaskId}
            />
          ))}
        </tbody>
      </table>
    </div>
  );
}
ExperimentsList.propTypes = { 
  experiments: PropTypes.array.isRequired, 
  selectedIds: PropTypes.object.isRequired, 
  onSelect: PropTypes.func.isRequired, 
  setTrackingTaskId: PropTypes.func.isRequired 
};
export default ExperimentsList;
========== FILE: dashboard/src/components/LiveTrackerPane.jsx ==========
// dashboard/src/components/LiveTrackerPane.jsx

import { useState, useEffect, useMemo } from 'react';
import PropTypes from 'prop-types';
import { Line } from 'react-chartjs-2';
import { Chart as ChartJS, CategoryScale, LinearScale, PointElement, LineElement, Title, Tooltip, Legend } from 'chart.js';
import { getCssVar } from '../utils/cssUtils';

ChartJS.register(CategoryScale, LinearScale, PointElement, LineElement, Title, Tooltip, Legend);

const initialStatus = { 
  state: 'CONNECTING', 
  details: { status_text: 'Worker\'a baÄŸlanÄ±lÄ±yor...' }
};

function LiveTrackerPane({ taskId, onClose }) {
  const [liveData, setLiveData] = useState({ 
    status: initialStatus,
    chart: { labels: [], datasets: [{ data: [] }] }
  });
  
  const chartOptions = useMemo(() => ({
    responsive: true, maintainAspectRatio: false,
    animation: { duration: 300, easing: 'linear' },
    plugins: { 
      legend: { display: false }, 
      tooltip: {
        enabled: true, backgroundColor: getCssVar('--content-bg'),
        titleColor: getCssVar('--text-color'), bodyColor: getCssVar('--text-color'),
        borderColor: getCssVar('--border-color'), borderWidth: 1, padding: 10,
        displayColors: false,
        callbacks: {
          title: (ctx) => ctx[0].label,
          label: (ctx) => `KayÄ±p: ${ctx.parsed.y.toFixed(6)}`,
        }
      },
    },
    scales: { 
      y: { 
        grid: { color: getCssVar('--border-color'), borderDash: [2, 4], drawTicks: false },
        ticks: { padding: 10, maxTicksLimit: 5, font: { size: 12 }, color: getCssVar('--text-color-darker') },
      }, 
      x: {
        grid: { display: false },
        ticks: { padding: 10, maxRotation: 0, autoSkip: true, maxTicksLimit: 7, font: { size: 12 }, color: getCssVar('--text-color-darker') },
      } 
    }
  }), []);

  useEffect(() => {
    if (!taskId) return;

    const initialChartWithColors = {
      labels: [],
      datasets: [{
        label: 'EÄŸitim KaybÄ±', data: [], fill: true,
        borderColor: getCssVar('--primary-color'),
        backgroundColor: `color-mix(in srgb, ${getCssVar('--primary-color')} 20%, transparent)`,
        tension: 0.4, borderWidth: 2,
        pointRadius: (ctx) => ctx.dataIndex === ctx.dataset.data.length - 1 ? 6 : 0,
        pointBorderColor: getCssVar('--text-inverse'),
        pointBackgroundColor: getCssVar('--primary-color'),
        pointHoverRadius: 8,
      }]
    };
    
    setLiveData({ status: initialStatus, chart: initialChartWithColors });

    const newSocket = new WebSocket(`ws://localhost:8000/ws/task_status/${taskId}`);
    
    newSocket.onmessage = (event) => {
      const data = JSON.parse(event.data);
      setLiveData(prev => {
        let newChart = prev.chart;
        if (data.state === 'PROGRESS' && data.details?.loss !== undefined) {
          const epochLabel = `E${data.details.epoch}`;
          if (!prev.chart.labels.includes(epochLabel)) {
            const newLabels = [...prev.chart.labels, epochLabel].slice(-30);
            const newLossData = [...prev.chart.datasets[0].data, data.details.loss].slice(-30);
            newChart = { ...prev.chart, labels: newLabels, datasets: [{ ...prev.chart.datasets[0], data: newLossData }] };
          }
        } else if (data.result?.results?.loss) {
          const finalLossHistory = data.result.results.loss;
          newChart = {
            ...prev.chart,
            labels: Array.from({ length: finalLossHistory.length }, (_, i) => `E${i + 1}`),
            datasets: [{ ...prev.chart.datasets[0], data: finalLossHistory }]
          };
        }
        return { status: data, chart: newChart };
      });
    };
    
    newSocket.onerror = () => setLiveData(prev => ({ ...prev, status: { state: 'ERROR', details: { status_text: 'WebSocket baÄŸlantÄ± hatasÄ±!' } }}));
    newSocket.onclose = () => setLiveData(prev => (['SUCCESS', 'FAILURE', 'ERROR'].includes(prev.status.state)) ? prev : { ...prev, status: { ...prev.status, state: 'DISCONNECTED' }});
    
    return () => { if (newSocket.readyState === 1) newSocket.close(1000, "Component unmounting"); };
  }, [taskId]);
  
  const { state, details, result } = liveData.status;
  const pipeline_name = details?.pipeline_name || result?.config?.pipeline_name || "Bilinmiyor";
  const { total_epochs, epoch, status_text } = details || {};
  const progressPercent = (state === 'SUCCESS' || state === 'FAILURE') ? 100 : (total_epochs && epoch ? (epoch / total_epochs) * 100 : 0);
  
  return (
    <div className="live-tracker-pane">
      <button className="close-button" onClick={onClose}>Ã—</button>
      <div style={{display: 'flex', justifyContent: 'space-between', alignItems: 'center', marginBottom: '15px'}}>
        <h4><span role="img" aria-label="satellite">ðŸ›°ï¸</span> CanlÄ± Takip: {pipeline_name}</h4>
        <span className={`status-badge status-${state?.toLowerCase()}`}>{state}</span>
      </div>
      <div style={{display: 'flex', gap: '20px', alignItems: 'center'}}>
        <div style={{flex: 1}}>
            <div style={{ display: 'flex', justifyContent: 'space-between', alignItems: 'baseline', marginBottom: '5px' }}>
                <p style={{ margin: 0, color: 'var(--text-color-darker)', fontSize: '0.9em' }}>{status_text || state}</p>
                {epoch && total_epochs && (
                    <span style={{ fontWeight: 'bold', fontFamily: 'var(--font-mono)' }}>{epoch} / {total_epochs}</span>
                )}
            </div>
            <progress value={progressPercent} max="100" style={{width: '100%', height: '10px'}}></progress>
        </div>
        <div style={{flex: 2, height: '100px', position: 'relative'}}>
          {liveData.chart.labels.length > 0 ? (
            <Line data={liveData.chart} options={chartOptions} />
          ) : (
            <div style={{textAlign: 'center', color: 'var(--text-color-darker)'}}>KayÄ±p verisi bekleniyor...</div>
          )}
        </div>
      </div>
      {state === 'FAILURE' && result?.error && <p style={{marginTop: '15px', color: 'var(--error-color)'}}>{result.error.message}</p>}
    </div>
  );
}

LiveTrackerPane.propTypes = { 
  taskId: PropTypes.string.isRequired, 
  onClose: PropTypes.func.isRequired, 
};

export default LiveTrackerPane;
========== FILE: dashboard/src/components/Logo.jsx ==========
import PropTypes from 'prop-types';

function Logo({ size = 36 }) {
  return (
    <div className="logo-container">
      <svg width={size} height={size} viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" style={{ flexShrink: 0 }}>
        <defs>
          <linearGradient id="azura-stream-gradient-component" x1="15" y1="50" x2="85" y2="50" gradientUnits="userSpaceOnUse">
            <stop offset="0%" stopColor="var(--secondary-color)"/>
            <stop offset="100%" stopColor="var(--primary-color)"/>
          </linearGradient>
        </defs>
        <path d="M50 10 L10 90 H32 L42 70 H58 L68 90 H90 Z" fill="var(--content-bg)" stroke="var(--text-color-darker)" strokeWidth="4" strokeLinejoin="round"/>
        <path d="M15 55 C 35 45, 65 45, 85 55" stroke="url(#azura-stream-gradient-component)" strokeWidth="8" strokeLinecap="round" fill="none"/>
      </svg>
      <h1>AzuraForge</h1>
    </div>
  );
}
Logo.propTypes = { size: PropTypes.number, };
export default Logo;
========== FILE: dashboard/src/components/ThemeToggle.jsx ==========
import { useContext } from 'react';
import { ThemeContext } from '../context/ThemeContext';

const SunIcon = () => <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round"><circle cx="12" cy="12" r="5"></circle><line x1="12" y1="1" x2="12" y2="3"></line><line x1="12" y1="21" x2="12" y2="23"></line><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line><line x1="1" y1="12" x2="3" y2="12"></line><line x1="21" y1="12" x2="23" y2="12"></line><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line></svg>;
const MoonIcon = () => <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path></svg>;

function ThemeToggle() {
  const { theme, toggleTheme } = useContext(ThemeContext);

  return (
    <button onClick={toggleTheme} className="theme-toggle-button" title="TemayÄ± DeÄŸiÅŸtir">
      {theme === 'light' ? <MoonIcon /> : <SunIcon />}
    </button>
  );
}

// DÃœZELTME: Eksik olan 'export default' satÄ±rÄ± eklendi.
export default ThemeToggle;
========== FILE: dashboard/src/context/ThemeContext.jsx ==========
import { createContext, useState, useEffect, useMemo } from 'react';
import PropTypes from 'prop-types';

export const ThemeContext = createContext();

export function ThemeProvider({ children }) {
  const [theme, setTheme] = useState(() => {
    const savedTheme = localStorage.getItem('theme');
    const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
    return savedTheme || (prefersDark ? 'dark' : 'light');
  });

  useEffect(() => {
    const body = document.body;
    // body'nin class'Ä±nÄ± mevcut temaya gÃ¶re ayarla
    body.classList.toggle('light-theme', theme === 'light');
    // SeÃ§imi tarayÄ±cÄ± hafÄ±zasÄ±na kaydet
    localStorage.setItem('theme', theme);
  }, [theme]);

  const toggleTheme = () => {
    setTheme(prevTheme => (prevTheme === 'light' ? 'dark' : 'light'));
  };

  // Sadece theme ve toggleTheme'i dÄ±ÅŸarÄ±ya veriyoruz.
  const value = useMemo(() => ({ theme, toggleTheme }), [theme]);

  return (
    <ThemeContext.Provider value={value}>
      {children}
    </ThemeContext.Provider>
  );
}

ThemeProvider.propTypes = {
  children: PropTypes.node.isRequired,
};
========== FILE: dashboard/src/pages/DashboardOverview.jsx ==========
import { useState, useEffect, useMemo } from 'react';
import PropTypes from 'prop-types';
import ExperimentsList from '../components/ExperimentsList';
import ComparisonView from '../components/ComparisonView';
import { fetchExperiments, fetchExperimentDetails } from '../services/api';

function DashboardOverview({ setTrackingTaskId }) {
  const [experiments, setExperiments] = useState([]);
  const [loading, setLoading] = useState(true);
  const [error, setError] = useState(null);
  
  const [searchTerm, setSearchTerm] = useState('');
  const [filterStatus, setFilterStatus] = useState('ALL');
  
  const [selectedForComparison, setSelectedForComparison] = useState(new Set());
  const [comparisonData, setComparisonData] = useState(null);

  const getExperiments = async (showLoadingIndicator = false) => {
    if (showLoadingIndicator) setLoading(true);
    try {
      const response = await fetchExperiments();
      setExperiments(response.data);
      setError(null);
    } catch (err) {
      setError('API sunucusuna baÄŸlanÄ±lamadÄ± veya veri Ã§ekilemedi. Servislerin Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun.');
      console.error(err);
    } finally {
      if (showLoadingIndicator) setLoading(false);
    }
  };

  useEffect(() => {
    getExperiments(true);
    const intervalId = setInterval(() => getExperiments(false), 5000);
    return () => clearInterval(intervalId);
  }, []);

  const allStatuses = useMemo(() => {
    const statuses = new Set(experiments.map(exp => exp.status));
    return ['ALL', ...Array.from(statuses).sort()];
  }, [experiments]);

  const filteredExperiments = useMemo(() => {
    return experiments.filter(exp => {
      const statusMatch = filterStatus === 'ALL' || exp.status === filterStatus;
      if (!statusMatch) return false;

      if (searchTerm) {
        const lowerCaseSearchTerm = searchTerm.toLowerCase();
        const searchFields = [
          exp.experiment_id,
          exp.pipeline_name,
          exp.config_summary?.ticker,
          exp.batch_name,
        ];
        return searchFields.some(field => field?.toLowerCase().includes(lowerCaseSearchTerm));
      }

      return true;
    });
  }, [experiments, filterStatus, searchTerm]);
  
  const handleComparisonSelect = (experimentId) => {
    setSelectedForComparison(prev => {
      const newSelection = new Set(prev);
      if (newSelection.has(experimentId)) {
        newSelection.delete(experimentId);
      } else {
        newSelection.add(experimentId);
      }
      return newSelection;
    });
  };

  const handleStartComparison = async () => {
    const idsToCompare = Array.from(selectedForComparison);
    if (idsToCompare.length < 2) return;
    
    setComparisonData([]); // Show loading state in modal
    try {
      const promises = idsToCompare.map(id => fetchExperimentDetails(id));
      const responses = await Promise.all(promises);
      const dataToCompare = responses.map(res => res.data);
      setComparisonData(dataToCompare);
    } catch (error) {
      console.error("KarÅŸÄ±laÅŸtÄ±rma verileri Ã§ekilemedi:", error);
      setComparisonData(null); // Hide modal on error
    }
  };

  if (loading) return <p style={{textAlign: 'center', padding: '40px'}}>Deney verileri yÃ¼kleniyor...</p>;
  if (error) return <p style={{textAlign: 'center', padding: '40px', color: 'var(--error-color)'}}>{error}</p>;

  return (
    <div className="dashboard-overview">
      {comparisonData && <ComparisonView experiments={comparisonData} onClose={() => setComparisonData(null)}/>}
      
      <div className="page-header">
        <h1>Genel BakÄ±ÅŸ</h1>
        <p>TÃ¼m deneylerinizi tek bir yerden yÃ¶netin, takip edin ve karÅŸÄ±laÅŸtÄ±rÄ±n.</p>
      </div>
      
      <div className="card" style={{ marginBottom: '25px' }}>
        <div style={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center', flexWrap: 'wrap', gap: '20px' }}>
          <div style={{ display: 'flex', gap: '20px', flexWrap: 'wrap', alignItems: 'flex-end' }}>
            <div className="form-group" style={{ marginBottom: 0 }}>
              <label htmlFor="search-term">Arama</label>
              <input type="text" id="search-term" placeholder="ID, Pipeline, Sembol ara..." value={searchTerm} onChange={(e) => setSearchTerm(e.target.value)} />
            </div>
            <div className="form-group" style={{ marginBottom: 0 }}>
              <label htmlFor="filter-status">Durum</label>
              <select id="filter-status" value={filterStatus} onChange={(e) => setFilterStatus(e.target.value)}>
                {allStatuses.map(s => <option key={s} value={s}>{s === 'ALL' ? 'TÃ¼mÃ¼' : s}</option>)}
              </select>
            </div>
          </div>
          <button 
            className="button-primary" 
            onClick={handleStartComparison} 
            disabled={selectedForComparison.size < 2} 
            title={selectedForComparison.size < 2 ? 'KarÅŸÄ±laÅŸtÄ±rmak iÃ§in en az 2 deney seÃ§in' : ''}
          >
            <span role="img" aria-label="scales">âš–ï¸</span> SeÃ§ilenleri KarÅŸÄ±laÅŸtÄ±r ({selectedForComparison.size})
          </button>
        </div>
      </div>
      
      <ExperimentsList 
        experiments={filteredExperiments} 
        selectedIds={selectedForComparison} 
        onSelect={handleComparisonSelect} 
        setTrackingTaskId={setTrackingTaskId}
      />
    </div>
  );
}

DashboardOverview.propTypes = { 
  setTrackingTaskId: PropTypes.func.isRequired, 
};

export default DashboardOverview;
========== FILE: dashboard/src/pages/NewExperiment.jsx ==========
import { useState, useEffect } from 'react';
import { useNavigate } from 'react-router-dom';
import PropTypes from 'prop-types';
import { toast } from 'react-toastify';
import { startNewExperiment, fetchAvailablePipelines, fetchPipelineDefaultConfig } from '../services/api';

// Bu bileÅŸen artÄ±k bu dosyanÄ±n iÃ§inde yerel olarak kalabilir.
const renderConfigForm = (config, setConfig) => {
  const handleChange = (e, keyPath) => {
    const { value, type } = e.target;
    const newConfig = JSON.parse(JSON.stringify(config));
    let current = newConfig;
    for (let i = 0; i < keyPath.length - 1; i++) {
      current = current[keyPath[i]] = current[keyPath[i]] || {};
    }
    current[keyPath[keyPath.length - 1]] = type === 'number' ? parseFloat(value) : value;
    setConfig(newConfig);
  };

  const traverseConfig = (obj, path = []) => Object.entries(obj).map(([key, value]) => {
    const currentPath = [...path, key];
    const fieldId = currentPath.join('-');
    const labelText = key.replace(/_/g, ' ').replace(/\b\w/g, l => l.toUpperCase());

    if (typeof value === 'object' && value !== null && !Array.isArray(value)) {
      return (
        <fieldset key={fieldId} className="form-fieldset">
          <legend>{labelText}</legend>
          {traverseConfig(value, currentPath)}
        </fieldset>
      );
    }
    const inputType = typeof value === 'number' ? 'number' : 'text';
    return (
      <div key={fieldId} className="form-group">
        <label htmlFor={fieldId}>{labelText}:</label>
        <input type={inputType} id={fieldId} value={value ?? ''} onChange={(e) => handleChange(e, currentPath)} step={inputType === 'number' ? 'any' : undefined} />
      </div>
    );
  });

  return traverseConfig(config);
};

function NewExperiment({ onExperimentStarted }) {
  const [pipelines, setPipelines] = useState([]);
  const [selectedPipelineId, setSelectedPipelineId] = useState('');
  const [currentConfig, setCurrentConfig] = useState(null);
  const [isLoading, setIsLoading] = useState(true);
  const [isSubmitting, setIsSubmitting] = useState(false);
  const navigate = useNavigate();

  useEffect(() => {
    const loadPipelines = async () => {
      setIsLoading(true);
      try {
        const response = await fetchAvailablePipelines();
        if (response.data && response.data.length > 0) {
          setPipelines(response.data);
          setSelectedPipelineId(response.data[0].id);
        }
      } catch (error) { toast.error('Pipeline listesi yÃ¼klenemedi.'); } 
      finally { setIsLoading(false); }
    };
    loadPipelines();
  }, []);

  useEffect(() => {
    const loadPipelineConfig = async (pipelineId) => {
      if (!pipelineId) return;
      setIsLoading(true);
      setCurrentConfig(null);
      try {
        const { data } = await fetchPipelineDefaultConfig(pipelineId);
        if (data && !data.error) setCurrentConfig(data);
      } catch (error) { toast.error(`KonfigÃ¼rasyon yÃ¼klenemedi: ${error.message}`); } 
      finally { setIsLoading(false); }
    };
    loadPipelineConfig(selectedPipelineId);
  }, [selectedPipelineId]);

  const handleSubmit = async (e) => {
    e.preventDefault();
    setIsSubmitting(true);
    const configToSend = { ...currentConfig, pipeline_name: selectedPipelineId };
    try {
      const { data } = await startNewExperiment(configToSend);
      toast.success(`GÃ¶rev baÅŸarÄ±yla gÃ¶nderildi! ID: ${data.task_id}`);
      if (onExperimentStarted) onExperimentStarted(data.task_id);
      navigate('/');
    } catch (err) {
      toast.error('Deney baÅŸlatÄ±lamadÄ±. API/Worker loglarÄ±nÄ± kontrol edin.');
    } finally {
      setIsSubmitting(false);
    }
  };
  
  const selectedPipelineDetails = pipelines.find(p => p.id === selectedPipelineId);

  return (
    // YENÄ° YAPI: Form, baÅŸlÄ±k ve aksiyon paneli olarak ayrÄ±ldÄ±
    <div className="new-experiment-layout">
      <div className="page-header">
        <h1>Yeni Deney BaÅŸlat</h1>
        <p>Mevcut bir AI pipeline'Ä± seÃ§erek yeni bir eÄŸitim sÃ¼reci baÅŸlatÄ±n.</p>
      </div>
      
      <form onSubmit={handleSubmit} className="new-experiment-form">
        <div className="form-main-content"> {/* KaydÄ±rÄ±labilir alan */}
          <div className="card">
            <div className="form-group">
              <label htmlFor="pipeline-select">Ã‡alÄ±ÅŸtÄ±rÄ±lacak Pipeline Eklentisi</label>
              <select id="pipeline-select" value={selectedPipelineId} onChange={(e) => setSelectedPipelineId(e.target.value)} disabled={isLoading || isSubmitting}>
                {pipelines.map(p => <option key={p.id} value={p.id}>{p.name} ({p.id})</option>)}
              </select>
            </div>
          </div>
          
          <div className="card">
            <h3>Deney Parametreleri</h3>
            {isLoading ? <p>Parametreler yÃ¼kleniyor...</p> : (
              currentConfig ? renderConfigForm(currentConfig, setCurrentConfig) : <p>Bu pipeline iÃ§in dÃ¼zenlenebilir konfigÃ¼rasyon bulunamadÄ±.</p>
            )}
          </div>
        </div>

        {/* YENÄ° YAPI: YapÄ±ÅŸkan Aksiyon Paneli */}
        <div className="form-action-bar">
          <div className="pipeline-info">
            <strong>Pipeline:</strong>
            <span>{selectedPipelineDetails?.name || '...'}</span>
          </div>
          <button type="submit" disabled={isLoading || isSubmitting} className="button-primary">
            {isSubmitting ? 'BaÅŸlatÄ±lÄ±yor...' : 'EÄŸitimi BaÅŸlat'}
          </button>
        </div>
      </form>
    </div>
  );
}

NewExperiment.propTypes = { onExperimentStarted: PropTypes.func.isRequired };
export default NewExperiment;
========== FILE: dashboard/src/pages/ReportViewer.jsx ==========
// dashboard/src/pages/ReportViewer.jsx

import { useState, useEffect } from 'react';
import { useParams, Link } from 'react-router-dom';
import { Line } from 'react-chartjs-2';
// DÃœZELTME: Filler eklentisini import et
import { Chart as ChartJS, CategoryScale, LinearScale, PointElement, LineElement, TimeScale, Title, Tooltip, Legend, Filler } from 'chart.js';
import 'chartjs-adapter-date-fns';
import { fetchExperimentDetails } from '../services/api';

// DÃœZELTME: Filler'Ä± kaydet
ChartJS.register(CategoryScale, LinearScale, PointElement, LineElement, TimeScale, Title, Tooltip, Legend, Filler);

const chartOptions = (title) => ({
    responsive: true,
    maintainAspectRatio: false,
    plugins: {
        legend: { position: 'top' },
        title: { display: true, text: title, font: { size: 16 } }
    },
    interaction: {
        intersect: false,
        mode: 'index',
    },
});

function ReportViewer() {
    const { experimentId } = useParams();
    const [details, setDetails] = useState(null);
    const [loading, setLoading] = useState(true);
    const [error, setError] = useState(null);

    useEffect(() => {
        const getDetails = async () => {
            setLoading(true);
            try {
                const response = await fetchExperimentDetails(experimentId);
                setDetails(response.data);
                setError(null);
            } catch (err) {
                setError(`Rapor detaylarÄ± yÃ¼klenemedi: ${err.response?.data?.detail || err.message}`);
            } finally {
                setLoading(false);
            }
        };
        getDetails();
    }, [experimentId]);

    if (loading) return <div className="card"><p>Rapor YÃ¼kleniyor...</p></div>;
    if (error) return <div className="card" style={{borderColor: 'var(--error-color)'}}><p>{error}</p><Link to="/">Geri DÃ¶n</Link></div>;
    if (!details) return <div className="card"><p>Deney detayÄ± bulunamadÄ±.</p></div>;

    const { config, results } = details;
    // DÃœZELTME: Verinin varlÄ±ÄŸÄ±nÄ± gÃ¼venli bir ÅŸekilde kontrol et
    const lossHistory = results?.history?.loss || [];
    const predictionData = {
        labels: results?.time_index || [],
        datasets: [
            {
                label: 'GerÃ§ek DeÄŸerler',
                data: results?.y_true || [],
                borderColor: 'rgb(54, 162, 235)',
                backgroundColor: 'rgba(54, 162, 235, 0.2)',
                pointRadius: 1,
                fill: 'origin'
            },
            {
                label: 'Tahmin Edilen DeÄŸerler',
                data: results?.y_pred || [],
                borderColor: 'rgb(255, 99, 132)',
                borderDash: [5, 5],
                pointRadius: 1,
            }
        ]
    };
    
    const lossData = {
        labels: lossHistory.map((_, i) => `Epoch ${i + 1}`),
        datasets: [{
            label: 'EÄŸitim KaybÄ±',
            data: lossHistory,
            borderColor: 'rgb(75, 192, 192)',
            tension: 0.1
        }]
    };

    return (
        <div className="report-viewer">
            <div className="page-header" style={{display: 'flex', justifyContent: 'space-between', alignItems: 'center'}}>
                <div>
                    <h1>{config?.pipeline_name} Deney Raporu</h1>
                    <p style={{fontFamily: 'var(--font-mono)', color: 'var(--text-color-darker)'}}>{experimentId}</p>
                </div>
                <Link to="/" className="button-primary" style={{textDecoration: 'none'}}>â† Genel BakÄ±ÅŸ'a DÃ¶n</Link>
            </div>

            <div className="card" style={{marginBottom: '20px'}}>
                <h2>Performans Ã–zeti</h2>
                <div style={{display: 'flex', gap: '20px', flexWrap: 'wrap'}}>
                    <p><strong>RÂ² Skoru:</strong> {results?.metrics?.r2_score?.toFixed(4) ?? 'N/A'}</p>
                    <p><strong>MAE:</strong> {results?.metrics?.mae?.toFixed(4) ?? 'N/A'}</p>
                    <p><strong>Final KayÄ±p:</strong> {results?.final_loss?.toFixed(6) ?? 'N/A'}</p>
                </div>
            </div>

            <div className="card" style={{height: '500px', marginBottom: '20px'}}>
                 <Line options={{...chartOptions('Tahmin vs GerÃ§ek DeÄŸerler'), scales: {x: {type: 'time', time: {unit: 'year'}}}}} data={predictionData} />
            </div>

            <div className="card" style={{height: '400px', marginBottom: '20px'}}>
                <Line options={chartOptions('Model Ã–ÄŸrenme EÄŸrisi')} data={lossData} />
            </div>
            
            <div className="card">
                <h2>Deney KonfigÃ¼rasyonu</h2>
                <pre style={{backgroundColor: 'var(--bg-color)', padding: '15px', borderRadius: '8px', whiteSpace: 'pre-wrap', maxHeight: '400px', overflowY: 'auto'}}>
                    <code>{JSON.stringify(config, null, 2)}</code>
                </pre>
            </div>
        </div>
    );
}

export default ReportViewer;
========== FILE: dashboard/src/services/api.js ==========
// dashboard/src/services/api.js

import axios from 'axios';

export const API_BASE_URL = 'http://localhost:8000/api/v1';

const apiClient = axios.create({
  baseURL: API_BASE_URL,
  headers: { 'Content-Type': 'application/json' },
});

export const fetchExperiments = () => apiClient.get('/experiments');
export const startNewExperiment = (config) => apiClient.post('/experiments', config);
export const fetchAvailablePipelines = () => apiClient.get('/pipelines'); 
export const fetchPipelineDefaultConfig = (pipelineId) => apiClient.get(`/pipelines/${pipelineId}/config`);

// YENÄ° FONKSÄ°YON (fetchExperimentReport yerine)
// ArtÄ±k JSON dÃ¶ndÃ¼ÄŸÃ¼ iÃ§in responseType belirtmeye gerek yok.
export const fetchExperimentDetails = (experimentId) => {
  return apiClient.get(`/experiments/${experimentId}/details`);
};
========== FILE: dashboard/src/utils/cssUtils.js ==========
/**
 * Belirtilen CSS deÄŸiÅŸkeninin hesaplanmÄ±ÅŸ deÄŸerini dÃ¶ndÃ¼rÃ¼r.
 * @param {string} varName - '--primary-color' gibi CSS deÄŸiÅŸken adÄ±.
 * @returns {string} - DeÄŸiÅŸkenin renk deÄŸeri (Ã¶rn. '#42b983').
 */
export const getCssVar = (varName) => {
  if (typeof window === 'undefined') return '';
  return getComputedStyle(document.documentElement).getPropertyValue(varName).trim();
};
========== FILE: docs/ARCHITECTURE.md ==========
# ðŸ—ï¸ AzuraForge Mimarisi

Bu belge, AzuraForge platformunu oluÅŸturan servislerin ve bileÅŸenlerin birbirleriyle nasÄ±l etkileÅŸime girdiÄŸini, Ã¶zellikle de **asenkron ve olay gÃ¼dÃ¼mlÃ¼ yapÄ±nÄ±n** nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± detaylandÄ±rmaktadÄ±r.

## 1. Temel BileÅŸenler ve SorumluluklarÄ±

Platform, her biri belirli bir gÃ¶reve odaklanmÄ±ÅŸ baÄŸÄ±msÄ±z servislerden oluÅŸur:

*   **Dashboard (ArayÃ¼z KatmanÄ±):**
    *   KullanÄ±cÄ±nÄ±n etkileÅŸime girdiÄŸi React tabanlÄ± web uygulamasÄ±.
    *   Deneyleri baÅŸlatÄ±r, canlÄ± ilerlemeyi gÃ¶sterir, raporlarÄ± gÃ¶rÃ¼ntÃ¼ler.
    *   Sadece `API` servisi ile konuÅŸur.

*   **API (Ä°letiÅŸim ve AÄŸ GeÃ§idi KatmanÄ±):**
    *   Platformun dÄ±ÅŸ dÃ¼nyaya aÃ§Ä±lan kapÄ±sÄ±dÄ±r.
    *   Gelen istekleri doÄŸrular ve gÃ¶revleri `Celery` kuyruÄŸuna (Redis) iletir.
    *   `Dashboard`'dan gelen canlÄ± takip istekleri iÃ§in `WebSocket` baÄŸlantÄ±larÄ±nÄ± yÃ¶netir.
    *   Redis Pub/Sub kanallarÄ±na **abone (subscribe)** olarak `Worker`'dan gelen olaylarÄ± dinler.

*   **Worker (Ä°ÅŸleme KatmanÄ±):**
    *   AÄŸÄ±r hesaplama yÃ¼kÃ¼nÃ¼ Ã¼stlenir (model eÄŸitimi, rapor oluÅŸturma vb.).
    *   `Celery` kuyruÄŸundan gÃ¶revleri alÄ±r ve iÅŸler.
    *   EÄŸitim sÄ±rasÄ±nda ilerleme bilgilerini (`loss`, `epoch` vb.) Redis Pub/Sub kanallarÄ±na **yayÄ±nlar (publish)**.
    *   `Learner` ve `Core` kÃ¼tÃ¼phanelerini kullanarak AI modellerini Ã§alÄ±ÅŸtÄ±rÄ±r.

*   **Redis (MesajlaÅŸma ve Ã–nbellek KatmanÄ±):**
    *   Platformun merkezi sinir sistemidir.
    *   **Celery Broker & Backend:** `API` ve `Worker` arasÄ±ndaki gÃ¶rev kuyruÄŸu ve sonuÃ§ deposu olarak hizmet eder.
    *   **Pub/Sub Sunucusu:** `Worker` ile `API` arasÄ±nda gerÃ§ek zamanlÄ±, bloklamayan iletiÅŸim iÃ§in kullanÄ±lÄ±r.

## 2. Bir Deneyin YaÅŸam DÃ¶ngÃ¼sÃ¼: Olay GÃ¼dÃ¼mlÃ¼ AkÄ±ÅŸ

AÅŸaÄŸÄ±daki ÅŸema, kullanÄ±cÄ± bir deneyi baÅŸlattÄ±ÄŸÄ± andan itibaren sistemde gerÃ§ekleÅŸen olaylar zincirini gÃ¶stermektedir.

```mermaid
sequenceDiagram
    participant D as Dashboard
    participant A as API Server
    participant R as Redis
    participant W as Worker

    D->>A: 1. POST /experiments (config ile)
    A->>R: 2. send_task('start_training', config)
    A-->>D: 3. { task_id: 'xyz' }

    D->>A: 4. WebSocket /ws/task_status/xyz
    Note over A: WebSocket baÄŸlantÄ±sÄ± aÃ§Ä±lÄ±r.
    A->>R: 5. SUBSCRIBE 'task-progress:xyz'
    Note over A: API artÄ±k bu kanalÄ± dinliyor.

    R->>W: 6. GÃ¶revi (task_id: 'xyz') teslim eder.
    Note over W: Worker, Pipeline'Ä± baÅŸlatÄ±r ve<br/>RedisProgressCallback'i ekler.
    
    loop EÄŸitim DÃ¶ngÃ¼sÃ¼ (Her Epoch Sonu)
        W->>W: Learner, on_epoch_end olayÄ±nÄ± tetikler.
        W->>R: 7. PUBLISH 'task-progress:xyz'<br/>{ epoch: n, loss: 0.123, ... }
    end

    R-->>A: 8. Kanalda yeni mesaj var!
    A-->>D: 9. WebSocket ile veriyi anÄ±nda iletir.
    Note over D: LiveTrackerPane gÃ¼ncellenir.
    
    Note over W: EÄŸitim biter.
    W->>R: 10. GÃ¶rev sonucunu (results.json) yazar.
    W-->>A: (Opsiyonel) GÃ¶rev durumu 'SUCCESS' olur.
```

### AkÄ±ÅŸÄ±n AdÄ±m AdÄ±m AÃ§Ä±klamasÄ±:

1.  **Deney BaÅŸlatma:** `Dashboard`, `API`'ye deney konfigÃ¼rasyonunu iÃ§eren bir HTTP POST isteÄŸi gÃ¶nderir.
2.  **GÃ¶rev KuyruÄŸa Atma:** `API`, bu isteÄŸi alÄ±r ve `Celery`'nin `send_task` metoduyla gÃ¶revi Redis'teki kuyruÄŸa bÄ±rakÄ±r.
3.  **AnÄ±nda Geri DÃ¶nÃ¼ÅŸ:** `API`, gÃ¶revin iÅŸlenmesini beklemeden, `Dashboard`'a anÄ±nda bir `task_id` dÃ¶ndÃ¼rÃ¼r. ArayÃ¼z "donmaz".
4.  **CanlÄ± Takip BaÄŸlantÄ±sÄ±:** `Dashboard`, aldÄ±ÄŸÄ± `task_id` ile `API`'nin WebSocket endpoint'ine baÄŸlanÄ±r.
5.  **Kanala Abone Olma:** `API`, bu `task_id`'ye Ã¶zel bir Redis Pub/Sub kanalÄ±na (`task-progress:xyz`) abone olur ve sessizce beklemeye baÅŸlar.
6.  **GÃ¶revi Alma:** `Worker`, Redis kuyruÄŸundaki gÃ¶revi alÄ±r ve `start_training_pipeline` gÃ¶revini Ã§alÄ±ÅŸtÄ±rmaya baÅŸlar.
7.  **Ä°lerleme YayÄ±nlama:** EÄŸitim sÄ±rasÄ±nda, `Learner`'daki `RedisProgressCallback`, her epoch sonunda ilerleme verisini (kayÄ±p, epoch vb.) ilgili Redis kanalÄ±na yayÄ±nlar.
8.  **MesajÄ± Yakalama:** `API`, abone olduÄŸu kanalda bir mesaj belirdiÄŸini anÄ±nda fark eder.
9.  **AnÄ±nda Ä°letim:** `API`, bu mesajÄ± alÄ±r ve olduÄŸu gibi WebSocket Ã¼zerinden `Dashboard`'a iletir. `Dashboard`'daki ilgili bileÅŸen (grafik, ilerleme Ã§ubuÄŸu) kendini gÃ¼nceller.
10. **GÃ¶revin TamamlanmasÄ±:** EÄŸitim bittiÄŸinde, `Worker` nihai sonuÃ§larÄ± (`results.json`) yazar ve Celery gÃ¶revini `SUCCESS` olarak iÅŸaretler.

Bu mimari, hesaplama (`Worker`) ve iletiÅŸim (`API`) katmanlarÄ±nÄ± birbirinden tamamen ayÄ±rarak platforma **saÄŸlamlÄ±k, Ã¶lÃ§eklenebilirlik ve gerÃ§ek zamanlÄ±lÄ±k** kazandÄ±rÄ±r.


========== FILE: docs/CONTRIBUTING.md ==========
========== FILE: docs/CONTRIBUTING.md ==========
# ðŸ¤ AzuraForge Platformuna KatkÄ±da Bulunma Rehberi

AzuraForge projesine gÃ¶sterdiÄŸiniz ilgi ve katkÄ±larÄ±nÄ±z iÃ§in teÅŸekkÃ¼r ederiz! Bu proje, modern yapay zeka sistemlerinin nasÄ±l inÅŸa edilmesi gerektiÄŸine dair bir vizyonu hayata geÃ§irmeyi amaÃ§lamaktadÄ±r. Bu rehber, kod tabanÄ±nÄ±n tutarlÄ±, okunabilir, sÃ¼rdÃ¼rÃ¼lebilir ve yÃ¼ksek kalitede kalmasÄ±nÄ± saÄŸlamak iÃ§in benimsediÄŸimiz Ã§alÄ±ÅŸma prensiplerini ve standartlarÄ±nÄ± aÃ§Ä±klamaktadÄ±r.

## ðŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

EÄŸer henÃ¼z geliÅŸtirme ortamÄ±nÄ±zÄ± kurmadÄ±ysanÄ±z, lÃ¼tfen platformun ana [GeliÅŸtirme Rehberi](./DEVELOPMENT_GUIDE.md) belgesindeki "GeliÅŸtirme OrtamÄ± Kurulumu" bÃ¶lÃ¼mÃ¼nÃ¼ takip edin.

## ðŸ› ï¸ Kodlama StandartlarÄ±

Projeye eklenen her kodun aÅŸaÄŸÄ±daki standartlarÄ± karÅŸÄ±lamasÄ± beklenmektedir. Bu standartlarÄ±n birÃ§oÄŸu, ilgili reponun kÃ¶k dizinindeki `pre-commit` hook'larÄ± ile otomatik olarak kontrol edilir.

1.  **Stil KÄ±lavuzu (PEP8 & Black):**
    *   TÃ¼m Python kodlarÄ±, PEP8 stil kurallarÄ±na uymalÄ±dÄ±r.
    *   Kodunuzu `black` ile otomatik formatlayÄ±n.
    *   **Kontrol:** `black .` komutunu Ã§alÄ±ÅŸtÄ±rÄ±n.
    *   **Otomasyon:** `pre-commit` hook'larÄ± bu formatlamayÄ± zorunlu kÄ±lar.

2.  **Linting (`flake8`):**
    *   TÃ¼m Python kodlarÄ±, `flake8` denetiminden hatasÄ±z geÃ§melidir.
    *   **Kontrol:** Ä°lgili repo iÃ§inde `flake8 src` komutunu Ã§alÄ±ÅŸtÄ±rÄ±n.
    *   **Otomasyon:** `pre-commit` hook'larÄ± linting'i zorunlu kÄ±lar.

3.  **Tip Ä°puÃ§larÄ± (Type Hinting & Mypy):**
    *   TÃ¼m fonksiyon ve metod imzalarÄ±, parametreler ve dÃ¶nÃ¼ÅŸ deÄŸerleri iÃ§in tip ipuÃ§larÄ± (`typing` modÃ¼lÃ¼ kullanÄ±larak) iÃ§ermelidir.
    *   **Kontrol:** Ä°lgili repo iÃ§inde `mypy src` komutunu Ã§alÄ±ÅŸtÄ±rÄ±n.
    *   **Otomasyon:** `pre-commit` hook'larÄ± statik tip denetimini zorunlu kÄ±lar.

4.  **DokÃ¼mantasyon (Docstrings):**
    *   TÃ¼m public modÃ¼ller, sÄ±nÄ±flar ve fonksiyonlar, ne iÅŸe yaradÄ±klarÄ±nÄ±, aldÄ±klarÄ± argÃ¼manlarÄ± (`Args:`) ve ne dÃ¶ndÃ¼rdÃ¼klerini (`Returns:`) aÃ§Ä±klayan Google-style docstring'ler iÃ§ermelidir.

5.  **Testler (`pytest`):**
    *   Eklenen her yeni Ã¶zellik veya fonksiyon iÃ§in ilgili birim testleri (`unit tests`) `tests/` klasÃ¶rÃ¼ne eklenmelidir.
    *   YapÄ±lan bir hata dÃ¼zeltmesi (bug fix) iÃ§in, o hatanÄ±n tekrar oluÅŸmasÄ±nÄ± engelleyecek bir regresyon testi yazÄ±lmalÄ±dÄ±r.
    *   **Kontrol:** Ä°lgili reponun kÃ¶k dizinindeyken `pytest` komutunu Ã§alÄ±ÅŸtÄ±rÄ±n.

## ðŸ“ Commit MesajlarÄ±

Commit mesajlarÄ±, yapÄ±lan deÄŸiÅŸikliÄŸi net bir ÅŸekilde aÃ§Ä±klamalÄ±dÄ±r ve [Conventional Commits](https://www.conventionalcommits.org/en/v1.0.0/) standardÄ±na uymalÄ±dÄ±r. Bu, otomatik versiyonlama ve deÄŸiÅŸiklik gÃ¼nlÃ¼ÄŸÃ¼ oluÅŸturmak iÃ§in hayati Ã¶nem taÅŸÄ±r.

**Format:**
```
<tip>(<kapsam>): <aÃ§Ä±klama>

[opsiyonel gÃ¶vde]
```

**Ã–rnek Tipler:**
*   `feat`: Yeni bir Ã¶zellik ekler (Minor versiyon artÄ±rÄ±mÄ±).
*   `fix`: Bir hata dÃ¼zeltmesi (Patch versiyon artÄ±rÄ±mÄ±).
*   `docs`: Sadece dokÃ¼mantasyon deÄŸiÅŸiklikleri.
*   `style`: Kod formatÄ±, eksik noktalÄ± virgÃ¼l gibi stil dÃ¼zeltmeleri.
*   `refactor`: Kodu yeniden yapÄ±landÄ±rma, davranÄ±ÅŸ deÄŸiÅŸikliÄŸi yok.
*   `perf`: Performans iyileÅŸtirmesi yapan kod deÄŸiÅŸikliÄŸi.
*   `test`: Eksik testlerin eklenmesi veya mevcut testlerin dÃ¼zeltilmesi.
*   `build`: Build sistemi veya dÄ±ÅŸ baÄŸÄ±mlÄ±lÄ±k deÄŸiÅŸiklikleri.
*   `ci`: CI/CD yapÄ±landÄ±rma dosyalarÄ± ve script'leri deÄŸiÅŸiklikleri.

**Ã–rnekler:**
*   `feat(learner): Add LSTM backward pass implementation`
*   `fix(api): Handle null values from experiment results`
*   `docs(platform): Update development guide with Pub/Sub architecture`
*   `refactor(worker): Extract Redis publishing logic into a callback`

## ðŸ”„ Pull Request (PR) SÃ¼reci

1.  **Branch OluÅŸturma:** `main` branch'inden kendi feature branch'inizi (`feat/yeni-ozellik` veya `fix/hata-adi` gibi) oluÅŸturun.
2.  **DeÄŸiÅŸikliklerinizi YapÄ±n:** YukarÄ±daki standartlara uyduÄŸunuzdan emin olun.
3.  **Test Edin:** Yerel testlerinizi (`pytest`) Ã§alÄ±ÅŸtÄ±rÄ±n ve geÃ§tiÄŸinden emin olun.
4.  **Commit ve Push:** DeÄŸiÅŸikliklerinizi anlamlÄ± commit mesajlarÄ±yla branch'inize `push` edin.
5.  **Pull Request AÃ§Ä±n:** GitHub Ã¼zerinden `main` branch'ine bir "Pull Request" (PR) aÃ§Ä±n.
6.  **CI Kontrolleri:** PR'Ä±nÄ±zÄ±n CI kontrollerinden (testler, linting) baÅŸarÄ±yla geÃ§tiÄŸinden emin olun.
7.  **Kod Ä°ncelemesi:** Kodunuz incelenecek ve gerekli geri bildirimler saÄŸlanacaktÄ±r.

Bu standartlara uyarak, AzuraForge platformunun uzun vadede saÄŸlÄ±klÄ±, sÃ¼rdÃ¼rÃ¼lebilir ve yÃ¼ksek kalitede kalmasÄ±na yardÄ±mcÄ± olursunuz.


## ðŸ“¦ Versiyonlama ve BaÄŸÄ±mlÄ±lÄ±k YÃ¶netimi

Platformun kararlÄ±lÄ±ÄŸÄ±nÄ± saÄŸlamak iÃ§in tÃ¼m Python paketlerimiz Anlamsal Versiyonlama (Semantic Versioning) ve Git etiketlerini kullanÄ±r. BaÄŸÄ±mlÄ±lÄ±klar asla `@main` branch'ine iÅŸaret etmemelidir.

### Bir KÃ¼tÃ¼phanede DeÄŸiÅŸiklik YapÄ±ldÄ±ÄŸÄ±nda Ä°zlenecek AdÄ±mlar:

Bir kÃ¼tÃ¼phanede (Ã¶rn: `learner`) bir hata dÃ¼zeltmesi veya yeni bir Ã¶zellik eklendiÄŸinde, aÅŸaÄŸÄ±daki adÄ±mlar izlenmelidir:

1.  **DeÄŸiÅŸiklikleri TamamlayÄ±n:** Gerekli kod deÄŸiÅŸikliklerini yapÄ±n, testleri gÃ¼ncelleyin ve `main` branch'ine birleÅŸtirin.

2.  **Versiyonu YÃ¼kseltin:** `pyproject.toml` dosyasÄ±ndaki `version` numarasÄ±nÄ± anlamsal versiyonlama kurallarÄ±na gÃ¶re artÄ±rÄ±n.
    *   `fix` (hata dÃ¼zeltmesi): `0.1.3` -> `0.1.4` (Patch artÄ±ÅŸÄ±)
    *   `feat` (yeni Ã¶zellik): `0.1.3` -> `0.2.0` (Minor artÄ±ÅŸ)

3.  **Yeni Versiyonu Etiketleyin:** Yeni versiyon numarasÄ±nÄ± bir Git etiketi olarak oluÅŸturun ve GitHub'a gÃ¶nderin.
    ```bash
    # learner/ dizinindeyken
    git tag v0.2.0
    git push origin v0.2.0
    ```

4.  **BaÄŸÄ±mlÄ± RepolarÄ± GÃ¼ncelleyin:** `learner` kÃ¼tÃ¼phanesini kullanan tÃ¼m diÄŸer repolarÄ±n (`api`, `app-stock-predictor` vb.) `pyproject.toml` dosyalarÄ±ndaki ilgili satÄ±rÄ± yeni versiyon etiketiyle (`...@v0.2.0`) gÃ¼ncelleyin.
========== FILE: docs/DEVELOPMENT_GUIDE.md ==========
========== FILE: docs/DEVELOPMENT_GUIDE.md ==========
# ðŸ› ï¸ AzuraForge Platform GeliÅŸtirme Rehberi

Bu belge, AzuraForge platformunda geliÅŸtirme yapmak isteyenler iÃ§in adÄ±m adÄ±m kurulum, Ã§alÄ±ÅŸma prensipleri ve katkÄ±da bulunma yÃ¶nergelerini iÃ§erir.

## ðŸŽ¯ Temel Felsefemiz

AzuraForge'da geliÅŸtirme yaparken, iki temel prensibi aklÄ±mÄ±zda tutarÄ±z:

1.  **BaÄŸÄ±msÄ±z Paketler:** Her repo (`core`, `learner`, `api` vb.), kendi baÅŸÄ±na yaÅŸayan, kurulabilir ve test edilebilir baÄŸÄ±msÄ±z bir Python/JavaScript paketidir.
2.  **DÃ¼zenlenebilir Kurulum:** Repolar arasÄ± baÄŸÄ±mlÄ±lÄ±klar, yerel geliÅŸtirmeyi hÄ±zlandÄ±rmak iÃ§in `pip install -e` (editable) komutuyla kurulur. Bu sayede bir kÃ¼tÃ¼phanede yaptÄ±ÄŸÄ±nÄ±z deÄŸiÅŸiklik, diÄŸerlerine anÄ±nda yansÄ±r.


Her repomuz, kendi baÅŸÄ±na yaÅŸayan, kurulabilir ve test edilebilir baÄŸÄ±msÄ±z bir Python/JavaScript paketidir. Repolar arasÄ± baÄŸÄ±mlÄ±lÄ±klar, Git adresleri (`@git+https://...`) Ã¼zerinden kurulur.

## ðŸ“¦ Proje RepolarÄ±na Genel BakÄ±ÅŸ

AzuraForge platformu, aÅŸaÄŸÄ±daki baÄŸÄ±msÄ±z GitHub depolarÄ±ndan oluÅŸur. GeliÅŸtirme yaparken bu repolarÄ±n bir kÄ±smÄ±nÄ± veya tamamÄ±nÄ± yerel makinenizde klonlamanÄ±z gerekecektir.

*   **`core`**: Temel otomatik tÃ¼rev motoru.
*   **`learner`**: `core` Ã¼zerinde yÃ¼ksek seviyeli Ã¶ÄŸrenme kÃ¼tÃ¼phanesi.
*   **`app-stock-predictor`**: Bir uygulama eklentisi Ã¶rneÄŸi.
*   **`applications`**: Resmi uygulama katalogu.
*   **`api`**: RESTful API ve WebSocket sunucusu (Redis Pub/Sub dinleyicisi).
*   **`worker`**: Arka plan gÃ¶revlerini iÅŸleyen Celery worker (Redis Pub/Sub yayÄ±ncÄ±sÄ±).
*   **`dashboard`**: React tabanlÄ± web kullanÄ±cÄ± arayÃ¼zÃ¼.
*   **`platform`**: TÃ¼m servisleri bir araya getiren ana orkestrasyon deposu (bu repo).

## âš™ï¸ GeliÅŸtirme OrtamÄ± Kurulumu

Bu adÄ±mlar, platformun tÃ¼m parÃ§alarÄ±nÄ± yerel geliÅŸtirme iÃ§in hazÄ±r hale getirir.

1.  **Gerekli AraÃ§lar:** Git, Python 3.8+, Node.js & npm, Docker Desktop.

2.  **RepolarÄ± Klonlama:**
    TÃ¼m ilgili repolarÄ± aynÄ± seviyede bir klasÃ¶re klonlayÄ±n:
    ```bash
    mkdir azuraforge-dev
    cd azuraforge-dev

    git clone https://github.com/AzuraForge/platform.git
    git clone https://github.com/AzuraForge/core.git
    git clone https://github.com/AzuraForge/learner.git
    git clone https://github.com/AzuraForge/applications.git
    git clone https://github.com/AzuraForge/app-stock-predictor.git
    git clone https://github.com/AzuraForge/api.git
    git clone https://github.com/AzuraForge/worker.git
    git clone https://github.com/AzuraForge/dashboard.git
    ```

3.  **Sanal Ortam ve BaÄŸÄ±mlÄ±lÄ±klar (Python):**

    **`.env` DosyasÄ±nÄ± OluÅŸturma:**
    Platformu Ã§alÄ±ÅŸtÄ±rmadan Ã¶nce, ana `platform` dizininde bir `.env` dosyasÄ± oluÅŸturun. Bu dosya, servislerin ortak dizinlere eriÅŸimini saÄŸlar.
    ```
    # .env
    REDIS_URL=redis://redis:6379/0
    REPORTS_DIR=./reports
    CACHE_DIR=./.cache
    ```

    Yerel geliÅŸtirme iÃ§in, **`platform` projesinin** kÃ¶k dizininde tek bir sanal ortam oluÅŸturup tÃ¼m Python baÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± oraya kurmak en pratik yoldur.

    ```bash
    cd platform # Ana `platform` reposunun iÃ§ine gir
    python -m venv .venv
    # Windows: .\.venv\Scripts\activate | Linux/macOS: source ./.venv/bin/activate
    ```
    # TÃ¼m Python repolarÄ±nÄ± "dÃ¼zenlenebilir" modda kur
    ```bash
    pip install -e ../core 
    pip install -e ../learner
    pip install -e ../applications
    pip install -e ../app-stock-predictor
    pip install -e ../api
    pip install -e ../worker
    ```

4.  **JavaScript BaÄŸÄ±mlÄ±lÄ±klarÄ± (Dashboard):**
    ```bash
    cd ../dashboard # `dashboard` reposunun iÃ§ine gir
    npm install
    ```

5.  **Redis Kurulumu (Docker ile):**
    ```bash
    docker run -d -p 6379:6379 --name azuraforge_redis redis
    ```

## â–¶ï¸ Servisleri Ã‡alÄ±ÅŸtÄ±rma (Yerel GeliÅŸtirme)

Sanal ortamÄ±nÄ±z aktifken ve Redis Ã§alÄ±ÅŸÄ±rken, her servisi ayrÄ± bir terminalde baÅŸlatÄ±n.

1.  **API Sunucusu (`api` reposundan):**
    ```bash
    cd ../api # veya bulunduÄŸunuz yere gÃ¶re ayarlayÄ±n
    # Gerekirse sanal ortamÄ± aktive et
    start-api
    ```

2.  **Worker Servisi (`worker` reposundan):**
    ```bash
    cd ../worker
    # Gerekirse sanal ortamÄ± aktive et
    start-worker
    ```

3.  **Dashboard (`dashboard` reposundan):**
    ```bash
    cd ../dashboard
    npm run dev
    ```

##  ðŸ”„ Ä°teratif GeliÅŸtirme AkÄ±ÅŸÄ±

Ã‡oÄŸu zaman, kodda kÃ¼Ã§Ã¼k deÄŸiÅŸiklikler yapÄ±p bunlarÄ± hÄ±zla test etmek istersiniz.

1.  **KÃ¼tÃ¼phanede DeÄŸiÅŸiklik (Ã¶rn: `core/src/azuraforge_core/tensor.py`):**
    *   DeÄŸiÅŸikliÄŸi yapÄ±n ve kaydedin.
    *   Bu deÄŸiÅŸikliÄŸin diÄŸer kÃ¼tÃ¼phanelerde anÄ±nda etkili olmasÄ± iÃ§in **ekstra bir `pip install` komutuna GEREK YOKTUR**, Ã§Ã¼nkÃ¼ `-e` ile kurulduklarÄ± iÃ§in doÄŸrudan kaynak dosyayÄ± kullanÄ±rlar.
    *   `core` projesine geri dÃ¶nÃ¼p birim testlerini (`pytest`) koÅŸarak deÄŸiÅŸikliÄŸi doÄŸrulayÄ±n.
    *   DeÄŸiÅŸikliÄŸi `commit`'leyin ve `push`'layÄ±n.

2.  **Uygulama/Servis DeÄŸiÅŸikliÄŸi (Ã¶rn: `app-stock-predictor/src/azuraforge_stockapp/pipeline.py`):**
    *   DeÄŸiÅŸikliÄŸi yapÄ±n ve kaydedin.
    *   `api` veya `worker` servisleri otomatik olarak `reload` (yeniden yÃ¼kleme) yapacaktÄ±r (eÄŸer `uvicorn --reload` ile Ã§alÄ±ÅŸÄ±yorlarsa). DeÄŸiÅŸikliÄŸin etkisini gÃ¶rmek iÃ§in genellikle ilgili servisi (API veya Worker) yeniden baÅŸlatmak yeterlidir.
    *   DeÄŸiÅŸikliÄŸi `commit`'leyin ve `push`'layÄ±n.

3.  **Yeni Bir BaÄŸÄ±mlÄ±lÄ±k EklendiÄŸinde (`pyproject.toml` deÄŸiÅŸtiÄŸinde):**
    *   Bir reponun (Ã¶rn: `learner`) `pyproject.toml` dosyasÄ±na yeni bir baÄŸÄ±mlÄ±lÄ±k (Ã¶rn: `pandas`) eklediyseniz, bu deÄŸiÅŸikliÄŸin diÄŸer repolar tarafÄ±ndan tanÄ±nmasÄ± iÃ§in **baÄŸÄ±mlÄ±lÄ±k zincirini yeniden kurmanÄ±z gerekir.**
    *   `platform` klasÃ¶rÃ¼ndeki ana sanal ortamÄ±nÄ±zÄ± aktive edin.
    *   `pip install -e ../learner` komutunu tekrar Ã§alÄ±ÅŸtÄ±rÄ±n. `pip`, sadece eksik olan yeni baÄŸÄ±mlÄ±lÄ±klarÄ± (`pandas`) ekleyecektir.

##  CanlÄ± Takip Mimarisi NasÄ±l Ã‡alÄ±ÅŸÄ±r?

1.  `Dashboard`, `API`'ye bir `/experiments` POST isteÄŸi atar.
2.  `API`, gÃ¶revi `Celery` kuyruÄŸuna bÄ±rakÄ±r ve `Dashboard`'a bir `task_id` dÃ¶ner.
3.  `Dashboard`, bu `task_id` ile `API`'nin `/ws/task_status/{task_id}` WebSocket endpoint'ine baÄŸlanÄ±r.
4.  `API`, bu baÄŸlantÄ± iÃ§in bir Redis istemcisi oluÅŸturur ve `task-progress:{task_id}` kanalÄ±na **abone (subscribe)** olur.
5.  `Worker`, gÃ¶revi kuyruktan alÄ±r ve `Learner`'Ä±, iÃ§ine `RedisProgressCallback` enjekte edilmiÅŸ ÅŸekilde Ã§alÄ±ÅŸtÄ±rÄ±r.
6.  `Learner`, her epoch sonunda `on_epoch_end` olayÄ±nÄ± yayÄ±nlar.
7.  `RedisProgressCallback`, bu olayÄ± yakalar ve ilerleme verisini (epoch, loss) Redis'teki `task-progress:{task_id}` kanalÄ±na **yayÄ±nlar (publish)**.
8.  `API`, abone olduÄŸu kanalda yeni bir mesaj duyar, onu alÄ±r ve WebSocket Ã¼zerinden anÄ±nda `Dashboard`'a iletir.
9.  `Dashboard`'daki `LiveTrackerPane` bileÅŸeni, gelen bu veriyle kendini gÃ¼nceller.

Bu yapÄ±, `Worker`'Ä±n CPU kullanÄ±mÄ± ne kadar yoÄŸun olursa olsun, raporlama ve arayÃ¼z gÃ¼ncellemesinin bloklanmadan, anlÄ±k olarak gerÃ§ekleÅŸmesini saÄŸlar.

##  Platform Mimarisi NasÄ±l Ã‡alÄ±ÅŸÄ±r?

### Standart Bir Deney AkÄ±ÅŸÄ±

1.  **BaÅŸlatma (`Dashboard` -> `API` -> `Worker`):**
    *   `Dashboard`, kullanÄ±cÄ±dan aldÄ±ÄŸÄ± konfigÃ¼rasyon ile `API`'nin `/experiments` endpoint'ine bir `POST` isteÄŸi atar.
    *   `API`, gÃ¶revi `Celery` kuyruÄŸuna bÄ±rakÄ±r ve bir `task_id` dÃ¶ner.
    *   `Worker`, gÃ¶revi kuyruktan alÄ±r ve ilgili `Pipeline` eklentisinin bir Ã¶rneÄŸini oluÅŸturur.

2.  **EÄŸitim ve CanlÄ± Takip (`Worker` -> `Learner` -> `API` -> `Dashboard`):**
    *   `Worker`, eklentinin standart `run` metodunu Ã§aÄŸÄ±rÄ±r. Bu metot, `azuraforge-learner` iÃ§indeki `TimeSeriesPipeline`'den gelir.
    *   `run` metodu, `LivePredictionCallback` ve `RedisProgressCallback` gibi Ã¶zel `Callback`'ler oluÅŸturur.
    *   `Learner`'Ä±n `fit` metodu Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r. Her epoch sonunda:
        *   `LivePredictionCallback`, doÄŸrulama seti Ã¼zerinde tahmin yapar.
        *   `RedisProgressCallback`, hem kayÄ±p bilgisini hem de canlÄ± tahmin verisini birleÅŸtirerek Redis Pub/Sub kanalÄ±na yayÄ±nlar.
    *   `API`, bu kanala abone olduÄŸu iÃ§in mesajÄ± anÄ±nda alÄ±r ve `WebSocket` Ã¼zerinden `Dashboard`'a iletir.
    *   `LiveTrackerPane`, gelen bu zengin veriyle kendini (kayÄ±p grafiÄŸi, tahmin grafiÄŸi, ilerleme Ã§ubuÄŸu) gÃ¼nceller.

3.  **Tamamlama ve Raporlama (`Worker` -> `Learner`):**
    *   EÄŸitim bittiÄŸinde, `TimeSeriesPipeline`'in `run` metodu, son deÄŸerlendirmeyi yapar ve `azuraforge_learner.reporting` iÃ§indeki `generate_regression_report` fonksiyonunu Ã§aÄŸÄ±rÄ±r.
    *   Bu fonksiyon, `/reports` dizini altÄ±na, grafikleri iÃ§eren bir `report.md` dosyasÄ± oluÅŸturur.
    *   `run` metodu, deneyin tÃ¼m sonuÃ§larÄ±nÄ± (`history`, `metrics`, ham veriler) iÃ§eren bir JSON objesini `worker` gÃ¶revine dÃ¶ndÃ¼rÃ¼r.
    *   `Worker`, bu sonucu `results.json` dosyasÄ±na yazar ve gÃ¶revi `SUCCESS` olarak iÅŸaretler.

## ðŸ¤ KatkÄ±da Bulunma

Bu proje bir aÃ§Ä±k kaynak projesi olarak geliÅŸtirilmektedir. KatkÄ±da bulunmak iÃ§in lÃ¼tfen `platform/docs/CONTRIBUTING.md` dosyasÄ±nÄ± inceleyin.
========== FILE: docs/PROJECT_JOURNEY.md ==========
# ðŸ—ºï¸ Proje YolculuÄŸu: AzuraForge'un GeliÅŸim Hikayesi ve Gelecek Vizyonu

Bu belge, AzuraForge platformunun baÅŸlangÄ±cÄ±ndan mevcut durumuna kadar olan geliÅŸim sÃ¼recini, karÅŸÄ±laÅŸÄ±lan zorluklarÄ±, bulunan Ã§Ã¶zÃ¼mleri ve projenin **kendi kendini anlayan, sÄ±fÄ±rdan inÅŸa edilmiÅŸ, eklenti tabanlÄ± ve evrensel bir yapay zeka geliÅŸtirme platformuna** dÃ¶nÃ¼ÅŸme vizyonunu Ã¶zetlemektedir.

## ðŸš€ Proje Vizyonu ve Felsefesi

AzuraForge, basit bir araÃ§ seti olmanÄ±n Ã¶tesinde, bir felsefeyi temsil eder: **Ferrari motorunu (kanÄ±tlanmÄ±ÅŸ, sÄ±fÄ±rdan inÅŸa edilmiÅŸ AI gÃ¼cÃ¼) alÄ±p, modÃ¼ler ve geniÅŸletilebilir bir uzay gemisi ÅŸasisine (daÄŸÄ±tÄ±k MLOps mimarisi) monte etmek.** Bu uzay gemisi, yeni eklentilerle (`app-xx`) sÃ¼rekli geliÅŸtirilerek farklÄ± gÃ¶revleri yerine getirebilen, akÄ±llÄ± ve kendi kendini yÃ¶netebilen bir yapÄ±ya dÃ¶nÃ¼ÅŸÃ¼r.

Bu vizyonu gerÃ§ekleÅŸtirmek iÃ§in iki temel anayasal prensibe baÄŸlÄ±yÄ±z:

1.  **Ã‡ekirdek BaÄŸÄ±msÄ±zlÄ±ÄŸÄ± ve Derin AnlayÄ±ÅŸ ("Smart Learner" Ruhu):**
    Platformun kalbindeki (`azuraforge-core`, `azuraforge-learner`) algoritmalar ve yapÄ±lar, dÄ±ÅŸ kÃ¼tÃ¼phanelere minimal baÄŸÄ±mlÄ±lÄ±kla, temel prensipleri anlaÅŸÄ±larak sÄ±fÄ±rdan inÅŸa edilir. Bu bize tam kontrol, esneklik, ÅŸeffaflÄ±k ve derinlemesine bir "know-how" saÄŸlar.

2.  **ModÃ¼ler ve Ã–lÃ§eklenebilir Ekosistem ("AzuraForge" Mimarisi):**
    Ã‡ekirdeÄŸin saf gÃ¼cÃ¼; daÄŸÄ±tÄ±k, asenkron, olay gÃ¼dÃ¼mlÃ¼ ve eklenti tabanlÄ± bir mimariyle sunulur. Bu sayede platform; saÄŸlam, esnek, bÃ¼yÃ¼meye aÃ§Ä±k ve modern mÃ¼hendislik standartlarÄ±na uygun kalÄ±r.

---

## âœ… Tamamlanan Fazlar ve Elde Edilen BaÅŸarÄ±lar

### Faz 0: Fikir ve Prototip ("Smart Learner" Projesi)
- **DÃ¼ÅŸÃ¼nce:** Mevcut ML araÃ§larÄ±nÄ±n karmaÅŸÄ±klÄ±ÄŸÄ±na, "kara kutu" yapÄ±sÄ±na ve aÅŸÄ±rÄ± baÄŸÄ±mlÄ±lÄ±klarÄ±na bir tepki olarak, sÄ±fÄ±rdan bir derin Ã¶ÄŸrenme motoru (`mininn`) inÅŸa etme fikri doÄŸdu.
- **KanÄ±t:** Monolitik bir prototip olan "Smart Learner" projesi geliÅŸtirildi. SÄ±fÄ±rdan yazÄ±lan `LSTM` mimarisi, hava durumu tahmininde **RÂ² > 0.98**, hisse senedi fiyat tahmininde ise **RÂ² â‰ˆ 0.73** gibi somut ve etkileyici baÅŸarÄ±lar elde etti.
- **Ã–ÄŸrenilen Ders:** Monolitik yapÄ± hÄ±zlÄ± prototipleme saÄŸlasa da, Ã¶lÃ§eklenebilirlik, canlÄ± takip ve modÃ¼ler geniÅŸleme iÃ§in yetersizdi. Daha bÃ¼yÃ¼k bir vizyon iÃ§in paradigma deÄŸiÅŸimi gerekiyordu.

### Faz 1-3: Mikroservis Mimarisine GeÃ§iÅŸ ve Temel YapÄ±nÄ±n Ä°nÅŸasÄ±
- **Karar:** Uzun vadeli sÃ¼rdÃ¼rÃ¼lebilirlik iÃ§in platform, baÄŸÄ±msÄ±z repolara (`azuraforge-core`, `learner`, `api`, `worker`, `dashboard` vb.) sahip bir mikroservis mimarisine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼.
- **YapÄ±:** `docker-compose` ile orkestrasyon, `pip install -e` ile yerel geliÅŸtirme ve `git+https` ile repo'lar arasÄ± baÄŸÄ±mlÄ±lÄ±klar kuruldu.
- **Ä°lk BaÅŸarÄ±:** `Dashboard` -> `API` -> `Worker` -> `Uygulama` -> `Learner` -> `Core` ÅŸeklindeki temel gÃ¶rev akÄ±ÅŸÄ± baÅŸarÄ±yla Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±.

### Faz 4-8: GerÃ§ek ZamanlÄ± AkÄ±ÅŸÄ±n SaÄŸlanmasÄ± (Pub/Sub Mimarisi - DÃ¶nÃ¼m NoktasÄ±)
- **Sorun:** CPU-yoÄŸun eÄŸitim gÃ¶revleri, Worker'Ä±n durum gÃ¼ncelleme mesajlarÄ±nÄ± gÃ¶ndermesini engelleyerek arayÃ¼zÃ¼n "donmasÄ±na" neden oluyordu.
- **Ã‡Ã¶zÃ¼m:** Hesaplama ve raporlama gÃ¶revlerini tamamen ayÄ±rmak iÃ§in Redis Pub/Sub modeline geÃ§ildi.
    - `Learner` sadece olay (`on_epoch_end`) yayÄ±nlayan saf bir bileÅŸen haline getirildi.
    - `Worker`, `RedisProgressCallback` aracÄ±lÄ±ÄŸÄ±yla bu olaylarÄ± dinleyip bir Redis kanalÄ±na yayÄ±nlar hale geldi.
    - `API`, bu kanala abone olup, gelen her mesajÄ± WebSocket Ã¼zerinden anÄ±nda `Dashboard`'a iletir hale geldi.
- **BAÅžARI:** Bu mimari deÄŸiÅŸiklik sayesinde, anlÄ±k ve akÄ±cÄ± bir canlÄ± takip deneyimi mÃ¼mkÃ¼n oldu.

### Faz 9-17: Mimari Olgunluk ve GeliÅŸmiÅŸ Yetenekler ("Checkpoint Echo")
- **Standardizasyon (`BasePipeline`):** Uygulama eklentisi geliÅŸtirmeyi standartlaÅŸtÄ±ran `TimeSeriesPipeline` soyut sÄ±nÄ±fÄ± oluÅŸturuldu.
- **Verimlilik (Caching):** Harici API Ã§aÄŸrÄ±larÄ±nÄ± Ã¶nbelleÄŸe alan merkezi bir caching mekanizmasÄ± eklendi.
- **AkÄ±llÄ± Ã–n Ä°ÅŸleme:** `TimeSeriesPipeline`'a, hedef deÄŸiÅŸkene otomatik logaritmik dÃ¶nÃ¼ÅŸÃ¼m ve ters dÃ¶nÃ¼ÅŸÃ¼m uygulama yeteneÄŸi kazandÄ±rÄ±ldÄ±.
- **Dinamik Raporlama:** Raporlama, statik Markdown dosyalarÄ±ndan, `results.json` verisini kullanan, `Chart.js` ile Ã§izilmiÅŸ **interaktif ve canlÄ± grafikler** sunan bir yapÄ±ya dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼.
- **BAÅžARI:** Platform, kararlÄ±, canlÄ± takip yetenekli, dinamik raporlama sunan, verimli bir Ã¶nbellekleme mekanizmasÄ±na ve geliÅŸmiÅŸ Ã¶n iÅŸleme yeteneklerine sahip "Checkpoint Echo" kilometre taÅŸÄ±na ulaÅŸtÄ±.

---

## ðŸ—ºï¸ Gelecek Vizyonu ve Stratejik Yol HaritasÄ±

Bu saÄŸlam temel Ã¼zerine inÅŸa edilecek adÄ±mlar, AzuraForge'u daha da zenginleÅŸtirmeyi ve kapsamÄ±nÄ± geniÅŸletmeyi hedefleyecektir.

### **Faz 0: BÃ¼yÃ¼k BirleÅŸme (The Grand Unification) - Mevcut GÃ¼Ã§leri Konsolide Etme**
*Bu faz, "Smart Learner" prototipinin kanÄ±tlanmÄ±ÅŸ baÅŸarÄ±larÄ±nÄ± ve olgunlaÅŸmÄ±ÅŸ kodunu AzuraForge ekosistemine tam olarak entegre etmeyi hedefler.*

-   **1. KanÄ±tlanmÄ±ÅŸ Pipeline'larÄ± Eklenti Haline Getirme:**
    -   `weather_forecaster` (RÂ² > 0.98) ve `stock_predictor` (RÂ² â‰ˆ 0.73) pipeline'larÄ±nÄ±, AzuraForge standartlarÄ±na uygun, baÄŸÄ±msÄ±z `app-weather-forecaster` ve `app-stock-predictor` eklentileri olarak hayata geÃ§irmek.
-   **2. Motor ve Ã–ÄŸreniciyi YÃ¼kseltme:**
    -   "Smart Learner"daki daha olgun `Tensor` ve `LSTM` implementasyonlarÄ±nÄ±, birim testleriyle birlikte `azuraforge-core` ve `azuraforge-learner` paketlerine taÅŸÄ±mak.
-   **3. Hikayeyi BirleÅŸtirme:**
    -   Projenin tÃ¼m evrim hikayesini, bu belgede olduÄŸu gibi tek ve tutarlÄ± bir anlatÄ±da birleÅŸtirmek.

### **Faz 1: Deneyimi DerinleÅŸtirme ve ZenginleÅŸtirme**
*Bu faz, platformu daha profesyonel ve gÃ¼Ã§lÃ¼ kÄ±lacak temel MLOps yeteneklerini eklemeyi hedefler.*

-   **1. GeliÅŸmiÅŸ Model YÃ¶netimi ve Sunumu:**
    -   `ModelCheckpoint` callback'ini entegre ederek en iyi modelleri kalÄ±cÄ± olarak kaydetmek.
    -   `API`'ye `/models` ve `/models/{model_id}/predict` gibi yeni endpoint'ler ekleyerek, kaydedilmiÅŸ modellere eriÅŸim ve onlar Ã¼zerinden tahmin yapma imkanÄ± saÄŸlamak.
    -   `Dashboard`'a bir "Model KayÄ±t Defteri" sayfasÄ± eklemek.
-   **2. "Hiper SÃ¼rÃ¼cÃ¼" - Otomatik Hiperparametre Optimizasyonu:**
    -   `Dashboard` Ã¼zerinden parametre aralÄ±klarÄ± tanÄ±mlayarak hiperparametre optimizasyon gÃ¶revleri baÅŸlatma arayÃ¼zÃ¼ geliÅŸtirmek.
    -   `Worker`'Ä±n bu gÃ¶revleri yÃ¼zlerce alt gÃ¶reve bÃ¶lerek paralel Ã§alÄ±ÅŸtÄ±rmasÄ±nÄ± saÄŸlamak.
    -   SonuÃ§larÄ± `Dashboard`'da interaktif bir tablo veya Ä±sÄ± haritasÄ± ile gÃ¶rselleÅŸtirmek.
-   **3. GPU DesteÄŸinin Aktivasyonu (`CuPy`):**
    -   `docker-compose.yml` dosyasÄ±na GPU desteÄŸi eklemek ve platformun uyumlu donanÄ±mlarda `CuPy` ile hÄ±zlandÄ±rÄ±lmasÄ±nÄ± saÄŸlamak.

### **Faz 2: Evreni GeniÅŸletme (Yeni Veri Modaliteleri ve Yetenekler)**
*Bu faz, platformun farklÄ± veri tÃ¼rleriyle Ã§alÄ±ÅŸabilme yeteneÄŸini kanÄ±tlamayÄ± hedefler.*

-   **1. GÃ¶rÃ¼ntÃ¼ Ä°ÅŸleme ModÃ¼lÃ¼ (`app-image-classifier`):**
    -   `azuraforge-core`'a `Conv2D`, `MaxPool2D`, `Flatten` katmanlarÄ±nÄ± sÄ±fÄ±rdan eklemek.
    -   `azuraforge-learner`'a `BaseImagePipeline` soyut sÄ±nÄ±fÄ±nÄ± ve sÄ±nÄ±flandÄ±rma raporlamasÄ± (`Confusion Matrix` vb.) eklemek.
    -   `app-image-classifier` eklentisini geliÅŸtirmek.
-   **2. Ãœretken Yapay Zeka ModÃ¼lÃ¼ (`app-gan-generator`):**
    -   `azuraforge-core` ile basit bir GAN veya VAE mimarisi implemente etmek ve temel rakamlar Ã¼reten bir eklenti geliÅŸtirmek.
-   **3. DoÄŸal Dil Ä°ÅŸleme / Ses ModÃ¼lÃ¼ (Projenin KÃ¶kenine DÃ¶nÃ¼ÅŸ):**
    -   `azuraforge-core`'a `Embedding` katmanÄ± ve temel bir `Attention` mekanizmasÄ± eklemek.
    -   Metin sÄ±nÄ±flandÄ±rma veya ses tanÄ±ma gibi gÃ¶revler iÃ§in temel pipeline'larÄ± ve eklentileri oluÅŸturarak daha karmaÅŸÄ±k hedeflere (Ã¶rn: TTS) zemin hazÄ±rlamak.

### **Faz 3: GeleceÄŸi Kucaklamak (2025 ve Ã–tesi Trendleri)**
*Bu faz, platformu endÃ¼stri standardÄ± ve gerÃ§ekten "akÄ±llÄ±" bir sisteme dÃ¶nÃ¼ÅŸtÃ¼rmeyi hedefler.*

-   **1. Model BirleÅŸtirme ve Transfer Ã–ÄŸrenmesi:**
    -   Platforma, sÄ±fÄ±rdan eÄŸitmek yerine, daha Ã¶nce eÄŸitilmiÅŸ bir modeli (model kayÄ±t defterinden) yÃ¼kleyip yeni bir veri setiyle **ince ayar (fine-tuning)** yapabilme yeteneÄŸi kazandÄ±rmak.
-   **2. AÃ§Ä±klanabilir Yapay Zeka (XAI) Entegrasyonu:**
    -   Raporlara, modelin bir tahmini "neden" yaptÄ±ÄŸÄ±nÄ± basitÃ§e aÃ§Ä±klayan (Ã¶rn: SHAP, LIME entegrasyonu ile) bir bÃ¶lÃ¼m eklemek.
-   **3. Otomatik MLOps (AutoML-light):**
    -   Hiperparametre optimizasyonu sonuÃ§larÄ±nÄ± analiz edip, bir sonraki deney iÃ§in en olasÄ± parametre setini kullanÄ±cÄ±ya Ã¶neren bir "AkÄ±llÄ± Asistan" Ã¶zelliÄŸi geliÅŸtirmek.
-   **4. Ã‡oklu-Modalite (Multi-Modality):**
    -   Tek bir pipeline'Ä±n hem metin hem de gÃ¶rÃ¼ntÃ¼ gibi farklÄ± tÃ¼rde girdileri aynÄ± anda alarak tahmin yapabildiÄŸi mimarileri desteklemek.
========== FILE: docs/ROADMAP.md ==========
# ðŸ—ºï¸ AzuraForge Stratejik Yol HaritasÄ±

Bu belge, AzuraForge platformunun stratejik hedeflerini, tamamlanan kilometre taÅŸlarÄ±nÄ± ve gelecekteki geliÅŸtirme fazlarÄ±nÄ± Ã¶zetlemektedir. Bu yol haritasÄ±, projenin nereye gittiÄŸini gÃ¶steren canlÄ± bir dokÃ¼mandÄ±r.

**Daha detaylÄ± proje geÃ§miÅŸi ve evrimi iÃ§in [Proje YolculuÄŸu](./PROJECT_JOURNEY.md) belgesini inceleyebilirsiniz.**

---

### **ðŸ“ MEVCUT DURUM: Faz 0 - "Checkpoint Echo" (Fonksiyonel MVP)**

Platform, temel MLOps yeteneklerine sahip, Ã§alÄ±ÅŸan ve kararlÄ± bir MVP (Minimum Viable Product) aÅŸamasÄ±ndadÄ±r.

*   **Tamamlananlar:**
    *   SÄ±fÄ±rdan inÅŸa edilmiÅŸ `core` ve `learner` motorlarÄ±.
    *   Olay gÃ¼dÃ¼mlÃ¼ mimari ile asenkron gÃ¶rev iÅŸleme.
    *   `docker-compose` ile tam orkestrasyon.
    *   CanlÄ± deney takibi (WebSocket & Redis Pub/Sub).
    *   Dinamik ve interaktif raporlama arayÃ¼zÃ¼.
    *   GeniÅŸletilebilir eklenti sistemi (`entry_points`).

---

### **âž¡ï¸ FAZ 1: TEMELÄ° SAÄžLAMLAÅžTIRMA (Foundation Hardening)**

*   **Hedef:** Projeyi Ã¼retim kalitesine, endÃ¼stri standardÄ± geliÅŸtirme pratiklerine ve yÃ¼ksek gÃ¼venilirliÄŸe taÅŸÄ±mak.
*   **Durum:** `ðŸŸ¢ Aktif`
*   **Ana BaÅŸlÄ±klar:**
    *   `[âœ”ï¸]` KapsamlÄ± DokÃ¼mantasyon (`VISION.md`, `ROADMAP.md`, `ARCHITECTURE.md`).
    *   `[â³]` Anlamsal Versiyonlama ve Git Etiketleme Stratejisi.
    *   `[â³]` Test KapsamÄ±nÄ±n ArtÄ±rÄ±lmasÄ± (Unit & Integration Tests).
    *   `[â³]` SÃ¼rekli Entegrasyon (CI) Pipeline'larÄ± Kurulumu (GitHub Actions).
    *   `[â¬œ]` Deney Verilerinin Dosya Sisteminden VeritabanÄ±na (PostgreSQL) TaÅŸÄ±nmasÄ±.

---

### **âž¡ï¸ FAZ 2: DENEYÄ°MÄ° DERÄ°NLEÅžTÄ°RME (MLOps Capability Expansion)**

*   **Hedef:** Platformu, temel bir araÃ§tan daha profesyonel ve gÃ¼Ã§lÃ¼ bir MLOps Ã§Ã¶zÃ¼mÃ¼ne dÃ¶nÃ¼ÅŸtÃ¼rmek.
*   **Durum:** `â¬œ PlanlanÄ±yor`
*   **Ana BaÅŸlÄ±klar:**
    *   `[â¬œ]` **Model KayÄ±t Defteri (Model Registry):** EÄŸitilen en iyi modellerin kalÄ±cÄ± olarak saklanmasÄ± ve yÃ¶netilmesi.
    *   `[â¬œ]` **Model Sunumu (Model Serving):** KayÄ±tlÄ± modeller Ã¼zerinden tahmin yapmak iÃ§in API endpoint'leri (`/models/{id}/predict`).
    *   `[â¬œ]` **Hiperparametre Optimizasyonu:** `Dashboard` Ã¼zerinden optimizasyon gÃ¶revleri baÅŸlatma ve sonuÃ§larÄ± gÃ¶rselleÅŸtirme.
    *   `[â¬œ]` **Kimlik DoÄŸrulama ve Yetkilendirme:** Ã‡ok kullanÄ±cÄ±lÄ± ortamlar iÃ§in gÃ¼venlik katmanÄ± (JWT).
    *   `[â¬œ]` **GPU DesteÄŸi:** Uyumlu donanÄ±mlarda `CuPy` ile eÄŸitimi hÄ±zlandÄ±rma.

---

### **âž¡ï¸ FAZ 3: EVRENÄ° GENÄ°ÅžLETME (New Data Modalities)**

*   **Hedef:** Platformun yeteneklerini zaman serilerinin Ã¶tesine taÅŸÄ±yarak farklÄ± veri tÃ¼rleri ile Ã§alÄ±ÅŸabildiÄŸini kanÄ±tlamak.
*   **Durum:** `â¬œ PlanlanÄ±yor`
*   **Ana BaÅŸlÄ±klar:**
    *   `[â¬œ]` **GÃ¶rÃ¼ntÃ¼ Ä°ÅŸleme DesteÄŸi:** `Conv2D`, `MaxPool2D` katmanlarÄ± ve `ImageClassificationPipeline` eklentisi.
    *   `[â¬œ]` **DoÄŸal Dil Ä°ÅŸleme Temelleri:** `Embedding`, `Attention` katmanlarÄ± ve metin sÄ±nÄ±flandÄ±rma pipeline'Ä±.
    *   `[â¬œ]` **AÃ§Ä±klanabilir Yapay Zeka (XAI):** Raporlara, modelin tahminlerini "neden" yaptÄ±ÄŸÄ±nÄ± aÃ§Ä±klayan SHAP/LIME gibi gÃ¶rseller eklemek.

---
`[âœ”ï¸] TamamlandÄ±` `[ðŸŸ¢ Aktif]` `[â³ Devam Ediyor]` `[â¬œ PlanlanÄ±yor]`

========== FILE: docs/VISION.md ==========
# ðŸ“œ AzuraForge Vizyonu ve Felsefesi

AzuraForge, sadece bir yazÄ±lÄ±m projesi deÄŸil, modern yapay zeka sistemlerinin nasÄ±l inÅŸa edilmesi ve anlaÅŸÄ±lmasÄ± gerektiÄŸine dair bir manifestodur. Vizyonumuz, **ÅŸeffaf, modÃ¼ler ve derinlemesine anlaÅŸÄ±lmÄ±ÅŸ bir AI motorunu, saÄŸlam ve Ã¶lÃ§eklenebilir bir MLOps ÅŸasisi Ã¼zerine yerleÅŸtirerek**, hem Ã¶ÄŸrenme aracÄ± hem de gÃ¼Ã§lÃ¼ bir Ã¼retim platformu olarak hizmet edebilen bir ekosistem yaratmaktÄ±r.

## ðŸŽ¯ Temel Felsefe: Ferrari Motoru ve Uzay Gemisi Åžasisi

Bu vizyonu, basit bir metaforla Ã¶zetliyoruz:

*   **Ferrari Motoru:** Platformun kalbindeki (`core`, `learner`) AI motoru, kanÄ±tlanmÄ±ÅŸ temel algoritmalardan oluÅŸur. Bu motor, dÄ±ÅŸ kÃ¼tÃ¼phanelere minimal baÄŸÄ±mlÄ±lÄ±kla, temel prensipleri anlaÅŸÄ±larak sÄ±fÄ±rdan inÅŸa edilmiÅŸtir. Bu bize tam kontrol, esneklik, ÅŸeffaflÄ±k ve en Ã¶nemlisi, bir "kara kutu" ile Ã§alÄ±ÅŸmak yerine sistemin ruhunu anlama imkanÄ± verir.

*   **Uzay Gemisi Åžasisi:** Bu saf gÃ¼Ã§, modern mÃ¼hendislik pratikleriyle tasarlanmÄ±ÅŸ, daÄŸÄ±tÄ±k ve Ã¶lÃ§eklenebilir bir MLOps mimarisiyle sunulur. Bu ÅŸasi, Ferrari motorunun gÃ¼cÃ¼nÃ¼ gÃ¼venli, verimli ve yÃ¶netilebilir bir ÅŸekilde kullanÄ±labilir hale getirir.

## â­ "The AzuraForge Way": DÃ¶rt Anayasal Prensip

Platforma yapÄ±lan her katkÄ± ve alÄ±nan her karar, bu dÃ¶rt temel prensibe uygun olmalÄ±dÄ±r.

1.  **Ã–nce Kalite, Sonra HÄ±z (Quality First, Velocity Second):**
    HÄ±zlÄ± prototipleme dÃ¶nemi baÅŸarÄ±yla tamamlanmÄ±ÅŸtÄ±r. ArtÄ±k her yeni Ã¶zellik, testlerle gÃ¼vence altÄ±na alÄ±nmÄ±ÅŸ, dokÃ¼mante edilmiÅŸ ve standartlara uygun olmalÄ±dÄ±r. SÃ¼rdÃ¼rÃ¼lebilir hÄ±z, ancak saÄŸlam bir temel Ã¼zerine inÅŸa edilebilir.

2.  **ÅžeffaflÄ±k ve Sahiplenme (Transparency and Ownership):**
    Kod "kara kutu" olamaz. AlÄ±nan Ã¶nemli mimari kararlarÄ±n "nedenleri" (`ARCHITECTURE.md` gibi belgelerle) aÃ§Ä±kÃ§a belgelenir. Her bileÅŸenin (`api`, `worker` vb.) net bir sorumluluÄŸu ve amacÄ± vardÄ±r.

3.  **Pragmatik MÃ¼kemmeliyetÃ§ilik (Pragmatic Perfectionism):**
    En iyi mÃ¼hendislik pratiklerini (olay gÃ¼dÃ¼mlÃ¼ mimari, eklenti sistemi, CI/CD) hedefleriz, ancak bunlarÄ± projenin mevcut ihtiyaÃ§larÄ±na ve hedeflerine hizmet edecek ÅŸekilde pragmatik bir yaklaÅŸÄ±mla uygularÄ±z. MÃ¼kemmellik, karmaÅŸÄ±klÄ±k demek deÄŸildir.

4.  **KullanÄ±cÄ± OdaklÄ± DeÄŸer (User-Centric Value):**
    GeliÅŸtirdiÄŸimiz her Ã¶zellik, son kullanÄ±cÄ±ya (bu durumda platformu kullanan AI geliÅŸtiricisi/araÅŸtÄ±rmacÄ±sÄ±) somut bir deÄŸer katmalÄ±dÄ±r. CanlÄ± deney takibi, interaktif raporlama ve deney karÅŸÄ±laÅŸtÄ±rma gibi Ã¶zellikler bu prensibin en gÃ¼zel Ã¶rnekleridir.

Bu vizyon ve prensipler, AzuraForge'un gelecekteki geliÅŸimine rehberlik edecek olan kutup yÄ±ldÄ±zÄ±mÄ±zdÄ±r.

========== FILE: learner/pyproject.toml ==========
# learner/pyproject.toml

[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "azuraforge-learner"
version = "0.1.4" # Yeni baÄŸÄ±mlÄ±lÄ±k iÃ§in sÃ¼rÃ¼m artÄ±rÄ±mÄ±
authors = [{ name = "Azmi Sahin" }]
description = "High-level deep learning library for model training and management, using the AzuraForge Core engine."
readme = "README.md"
requires-python = ">=3.8"
license = { text = "MIT" }
dependencies = [
    "azuraforge-core @ git+https://github.com/AzuraForge/core.git@v0.1.3",
    "scikit-learn",
    "numpy",
    "redis",
    "matplotlib",
    "pandas",
    "pyarrow"
]

[project.optional-dependencies]
dev = ["pytest"]
========== FILE: learner/README.md ==========
# AzuraForge Learner ðŸ§ 

**AzuraForge Learner**, `azuraforge-core` motorunu kullanarak modelleri kolayca oluÅŸturmak, eÄŸitmek ve yÃ¶netmek iÃ§in tasarlanmÄ±ÅŸ yÃ¼ksek seviyeli bir kÃ¼tÃ¼phanedir.

## Kurulum

```bash
pip install azuraforge-learner@git+https://github.com/AzuraForge/learner.git
```

========== FILE: learner/setup.py ==========
from setuptools import setup, find_packages
setup(
    package_dir={"": "src"},
    packages=find_packages(where="src"),
)

========== FILE: learner/.github/workflows/ci.yml ==========
name: AzuraForge Learner CI

# Bu workflow'un ne zaman Ã§alÄ±ÅŸacaÄŸÄ±nÄ± belirler
on:
  push:
    branches: [ "main" ] # Sadece main branch'ine yapÄ±lan push'larda
  pull_request:
    branches: [ "main" ] # Main branch'ine aÃ§Ä±lan her PR'da

jobs:
  build-and-test:
    runs-on: ubuntu-latest # Workflow'un Ã§alÄ±ÅŸacaÄŸÄ± sanal makine (Linux)
    strategy:
      matrix:
        python-version: ["3.10", "3.11"] # FarklÄ± Python versiyonlarÄ±nda test et

    steps:
    # 1. AdÄ±m: Repoyu sanal makineye klonla
    - uses: actions/checkout@v4

    # 2. AdÄ±m: Belirtilen Python versiyonunu kur
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}

    # 3. AdÄ±m: BaÄŸÄ±mlÄ±lÄ±klarÄ± kur (pip ve proje baÄŸÄ±mlÄ±lÄ±klarÄ±)
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        # Core kÃ¼tÃ¼phanesini doÄŸrudan Git'ten kur
        pip install git+https://github.com/AzuraForge/core.git@v0.1.2
        # Learner'Ä±n kendisini ve test baÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± kur
        pip install -e .[dev]

    # 4. AdÄ±m: Kod stilini kontrol et (Black)
    - name: Check code format with Black
      run: |
        pip install black
        black --check .

    # 5. AdÄ±m: Linting kontrolÃ¼ yap (flake8)
    - name: Lint with flake8
      run: |
        pip install flake8
        # --count -> hata sayÄ±sÄ±nÄ± gÃ¶ster, --select -> kontrol edilecek hatalar, --ignore -> gÃ¶z ardÄ± edilecekler
        # --max-line-length -> satÄ±r uzunluÄŸu limiti
        flake8 src --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 src --count --max-complexity=10 --max-line-length=127 --statistics

    # 6. AdÄ±m: Testleri Ã§alÄ±ÅŸtÄ±r (pytest)
    - name: Test with pytest
      run: |
        pytest
========== FILE: learner/src/azuraforge_learner/caching.py ==========
# learner/src/azuraforge_learner/caching.py

import logging
import os
import hashlib
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, Optional
import pandas as pd

def get_cache_filepath(cache_dir: str, context: str, params: Dict[str, Any]) -> str:
    """
    Verilen parametrelere gÃ¶re deterministik bir Ã¶nbellek dosya yolu oluÅŸturur.
    Dosya adÄ±, parametrelerin sÄ±ralÄ± bir karmasÄ±ndan (hash) tÃ¼retilir.
    
    Args:
        cache_dir (str): Ã–nbellek dosyalarÄ±nÄ±n saklanacaÄŸÄ± ana dizin.
        context (str): Ã–nbelleÄŸin ait olduÄŸu baÄŸlam (Ã¶rn: 'stock_predictor').
        params (Dict[str, Any]): Dosya adÄ±nÄ± oluÅŸturmak iÃ§in kullanÄ±lacak parametreler.
        
    Returns:
        str: OluÅŸturulan tam dosya yolu.
    """
    # Parametreleri anahtarlarÄ±na gÃ¶re sÄ±ralayarak tutarlÄ± bir string oluÅŸtur
    param_str = str(sorted(params.items()))
    # Bu string'in hash'ini alarak benzersiz ve dosya sistemi iÃ§in gÃ¼venli bir kimlik oluÅŸtur
    param_hash = hashlib.md5(param_str.encode()).hexdigest()
    filename = f"{context}_{param_hash}.parquet"
    
    # BaÄŸlama Ã¶zel bir alt klasÃ¶r oluÅŸturarak karÄ±ÅŸÄ±klÄ±ÄŸÄ± Ã¶nle
    full_cache_dir = os.path.join(cache_dir, context)
    os.makedirs(full_cache_dir, exist_ok=True)
    
    return os.path.join(full_cache_dir, filename)

def load_from_cache(filepath: str, max_age_hours: int) -> Optional[pd.DataFrame]:
    """
    Veriyi Ã¶nbellekten yÃ¼kler. EÄŸer dosya yoksa veya belirtilen sÃ¼reden eskiyse
    None dÃ¶ner.
    
    Args:
        filepath (str): Ã–nbellek dosyasÄ±nÄ±n yolu.
        max_age_hours (int): Ã–nbelleÄŸin saat cinsinden maksimum geÃ§erlilik sÃ¼resi.
        
    Returns:
        Optional[pd.DataFrame]: GeÃ§erli Ã¶nbellek verisi varsa DataFrame, yoksa None.
    """
    if not os.path.exists(filepath):
        return None
        
    try:
        # DosyanÄ±n son deÄŸiÅŸtirilme zamanÄ±nÄ± al (UTC olarak)
        mod_time = datetime.fromtimestamp(os.path.getmtime(filepath), tz=timezone.utc)
        # EÄŸer dosyanÄ±n yaÅŸÄ±, izin verilen maksimum yaÅŸtan kÃ¼Ã§Ã¼kse, geÃ§erlidir
        if (datetime.now(timezone.utc) - mod_time) < timedelta(hours=max_age_hours):
            logging.info(f"GeÃ§erli Ã¶nbellek bulundu, buradan okunuyor: {filepath}")
            return pd.read_parquet(filepath)
        else:
            logging.info(f"Ã–nbellek sÃ¼resi dolmuÅŸ: {filepath}")
            os.remove(filepath) # SÃ¼resi dolmuÅŸ dosyayÄ± temizle
            return None
    except Exception as e:
        logging.error(f"Ã–nbellekten okuma hatasÄ± {filepath}: {e}")
        return None

def save_to_cache(df: pd.DataFrame, filepath: str) -> None:
    """
    Verilen DataFrame'i belirtilen yola Parquet formatÄ±nda kaydeder.
    
    Args:
        df (pd.DataFrame): Kaydedilecek veri.
        filepath (str): Kaydedilecek dosyanÄ±n tam yolu.
    """
    try:
        # DosyanÄ±n kaydedileceÄŸi dizinin var olduÄŸundan emin ol
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        df.to_parquet(filepath)
        logging.info(f"Veri Ã¶nbelleÄŸe kaydedildi: {filepath}")
    except Exception as e:
        logging.error(f"Ã–nbelleÄŸe yazma hatasÄ± {filepath}: {e}")
========== FILE: learner/src/azuraforge_learner/callbacks.py ==========
# learner/src/azuraforge_learner/callbacks.py

import os
import numpy as np
from typing import TYPE_CHECKING, Optional, Any
from .events import Event # Event'i de import edelim

# DÃ¶ngÃ¼sel importu Ã¶nlemek iÃ§in, sadece tip kontrolÃ¼ sÄ±rasÄ±nda Learner'Ä± import et
if TYPE_CHECKING:
    from .learner import Learner

class Callback:
    """
    TÃ¼m callback'lerin temel sÄ±nÄ±fÄ±.
    Kendisini Ã§alÄ±ÅŸtÄ±ran Learner'a bir referans tutar.
    """
    def __init__(self):
        self.learner: Optional['Learner'] = None

    def set_learner(self, learner: 'Learner'):
        """Bu metod, Learner tarafÄ±ndan Ã§aÄŸrÄ±larak referansÄ± ayarlar."""
        self.learner = learner

    def __call__(self, event: Event):
        """
        Gelen olaya gÃ¶re ilgili metodu (Ã¶rn: on_epoch_end) Ã§aÄŸÄ±rÄ±r.
        """
        method = getattr(self, f"on_{event.name}", None)
        if method:
            method(event)

    # Olay metotlarÄ±
    def on_train_begin(self, event: Event) -> None: pass
    def on_train_end(self, event: Event) -> None: pass
    def on_epoch_begin(self, event: Event) -> None: pass
    def on_epoch_end(self, event: Event) -> None: pass
    def on_batch_begin(self, event: Event) -> None: pass
    def on_batch_end(self, event: Event) -> None: pass


# Ã–NEMLÄ°: ModelCheckpoint ve EarlyStopping sÄ±nÄ±flarÄ±nÄ± koruyoruz ve
# yeni temel sÄ±nÄ±ftan miras almalarÄ±nÄ± saÄŸlÄ±yoruz.
class ModelCheckpoint(Callback):
    """Her epoch sonunda performansÄ± izler ve sadece en iyi modeli kaydeder."""
    def __init__(self, filepath: str, monitor: str = "val_loss", mode: str = "min", verbose: int = 1):
        super().__init__() # Temel sÄ±nÄ±fÄ±n init'ini Ã§aÄŸÄ±r
        self.filepath = filepath
        self.monitor = monitor
        self.mode = mode
        self.verbose = verbose
        self.best = np.inf if mode == "min" else -np.inf

        dir_path = os.path.dirname(self.filepath)
        if dir_path:
            os.makedirs(dir_path, exist_ok=True)

    def on_epoch_end(self, event: Event):
        current_val = event.payload.get(self.monitor)
        if current_val is None:
            if event.payload.get("epoch") == 0 and self.verbose > 0:
                print(f"ModelCheckpoint Warning: Can't find metric '{self.monitor}' to save model.")
            return

        is_better = (self.mode == "min" and current_val < self.best) or \
                    (self.mode == "max" and current_val > self.best)

        if is_better:
            if self.verbose > 0:
                print(f"ModelCheckpoint: {self.monitor} improved from {self.best:.6f} to {current_val:.6f}. Saving model...")
            self.best = current_val
            if self.learner and hasattr(self.learner, 'save_model'): # Learner'da save_model metodu varsa
                 self.learner.save_model(self.filepath)


class EarlyStopping(Callback):
    """Performans belirli bir epoch sayÄ±sÄ± boyunca iyileÅŸmediÄŸinde eÄŸitimi durdurur."""
    def __init__(self, monitor: str = "val_loss", patience: int = 10, mode: str = "min", verbose: int = 1):
        super().__init__() # Temel sÄ±nÄ±fÄ±n init'ini Ã§aÄŸÄ±r
        self.monitor = monitor
        self.patience = patience
        self.mode = mode
        self.verbose = verbose
        self.wait = 0
        self.best = np.inf if mode == "min" else -np.inf

    def on_train_begin(self, event: Event):
        self.wait = 0
        self.best = np.inf if self.mode == "min" else -np.inf

    def on_epoch_end(self, event: Event):
        current_val = event.payload.get(self.monitor)
        if current_val is None:
            return
            
        is_better = (self.mode == "min" and current_val < self.best) or \
                    (self.mode == "max" and current_val > self.best)

        if is_better:
            self.best = current_val
            self.wait = 0
        else:
            self.wait += 1
            if self.wait >= self.patience:
                if self.verbose > 0:
                    print(f"EarlyStopping: Stopping training. {self.monitor} did not improve for {self.patience} epochs.")
                if self.learner:
                    self.learner.stop_training = True
========== FILE: learner/src/azuraforge_learner/events.py ==========
from dataclasses import dataclass, field
from typing import Dict, Any, Literal, TYPE_CHECKING

if TYPE_CHECKING:
    from .learner import Learner

EventName = Literal["train_begin", "train_end", "epoch_begin", "epoch_end"]

@dataclass
class Event:
    name: EventName
    learner: 'Learner'
    payload: Dict[str, Any] = field(default_factory=dict)

========== FILE: learner/src/azuraforge_learner/layers.py ==========
from typing import List, Tuple, Optional
import numpy as np
from azuraforge_core import Tensor, xp, ArrayType

class Layer:
    def forward(self, x: Tensor) -> Tensor: raise NotImplementedError
    def parameters(self) -> List[Tensor]: return []
    def __call__(self, x: Tensor) -> Tensor: return self.forward(x)

class Linear(Layer):
    def __init__(self, input_dim: int, output_dim: int):
        limit = np.sqrt(2.0 / input_dim)
        self.weights = Tensor(xp.random.randn(input_dim, output_dim) * limit, requires_grad=True)
        self.bias = Tensor(xp.zeros(output_dim), requires_grad=True)
    def forward(self, x: Tensor) -> Tensor:
        return x.dot(self.weights) + self.bias
    def parameters(self) -> List[Tensor]:
        return [self.weights, self.bias]

class ReLU(Layer):
    def forward(self, x: Tensor) -> Tensor:
        return x.relu()

class Sigmoid(Layer):
    def forward(self, x: Tensor) -> Tensor:
        return x.sigmoid()

# DÃœZELTME: LSTM katmanÄ± tam backward pass ile yeniden yazÄ±ldÄ±.
class LSTM(Layer):
    def __init__(self, input_size: int, hidden_size: int):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        H = hidden_size
        D = input_size
        
        limit = np.sqrt(1.0 / H)
        self.W_x = Tensor(xp.random.randn(D, H * 4) * limit, requires_grad=True)
        self.W_h = Tensor(xp.random.randn(H, H * 4) * limit, requires_grad=True)
        self.b = Tensor(xp.zeros(H * 4), requires_grad=True)
        
        self.cache: Optional[Tuple] = None

    def parameters(self) -> List[Tensor]:
        return [self.W_x, self.W_h, self.b]

    def forward(self, x: Tensor) -> Tensor:
        N, T, D = x.data.shape
        H = self.hidden_size

        h_prev = xp.zeros((N, H))
        c_prev = xp.zeros((N, H))
        
        h_all = xp.zeros((N, T, H))
        c_all = xp.zeros((N, T, H))
        gates_all = xp.zeros((N, T, 4 * H))
        i_all = xp.zeros((N, T, H))
        f_all = xp.zeros((N, T, H))
        o_all = xp.zeros((N, T, H))
        g_all = xp.zeros((N, T, H))

        for t in range(T):
            x_t = x.data[:, t, :]
            gates = x_t @ self.W_x.data + h_prev @ self.W_h.data + self.b.data
            
            i = 1 / (1 + xp.exp(-gates[:, :H]))
            f = 1 / (1 + xp.exp(-gates[:, H:2*H]))
            o = 1 / (1 + xp.exp(-gates[:, 2*H:3*H]))
            g = xp.tanh(gates[:, 3*H:])
            
            c_next = f * c_prev + i * g
            h_next = o * xp.tanh(c_next)

            h_prev, c_prev = h_next, c_next
            
            h_all[:, t, :] = h_next
            c_all[:, t, :] = c_next
            gates_all[:, t, :] = gates
            i_all[:, t, :] = i
            f_all[:, t, :] = f
            o_all[:, t, :] = o
            g_all[:, t, :] = g

        # Ã‡Ä±ktÄ± olarak tÃ¼m zaman adÄ±mlarÄ±ndaki gizli durumlarÄ± dÃ¶ndÃ¼r
        out = Tensor(h_all, _children=(x, self.W_x, self.W_h, self.b), _op="lstm", requires_grad=x.requires_grad)
        
        # Geriye yayÄ±lÄ±m iÃ§in gerekli tÃ¼m ara deÄŸerleri sakla
        self.cache = (x.data, h_all, c_all, i_all, f_all, o_all, g_all)

        def _backward():
            if not out.requires_grad or out.grad is None: return
            assert self.cache is not None, "Cache is not set"
            
            x_data, h_data, c_data, i_data, f_data, o_data, g_data = self.cache
            _N, _T, _D = x_data.shape
            _H = self.hidden_size
            
            # BaÅŸlangÄ±Ã§ gradyanlarÄ±
            dx = xp.zeros_like(x_data)
            dW_x = xp.zeros_like(self.W_x.data)
            dW_h = xp.zeros_like(self.W_h.data)
            db = xp.zeros_like(self.b.data)
            
            dh_next = xp.zeros((_N, _H))
            dc_next = xp.zeros((_N, _H))

            for t in reversed(range(_T)):
                dh = out.grad[:, t, :] + dh_next
                
                # Geriye yayÄ±lÄ±m adÄ±mlarÄ±
                dc = dc_next + dh * o_data[:, t, :] * (1 - xp.tanh(c_data[:, t, :])**2)
                
                di = dc * g_data[:, t, :]
                df = dc * (c_data[:, t-1, :] if t > 0 else 0)
                do = dh * xp.tanh(c_data[:, t, :])
                dg = dc * i_data[:, t, :]
                
                d_gates_i = di * i_data[:, t, :] * (1 - i_data[:, t, :])
                d_gates_f = df * f_data[:, t, :] * (1 - f_data[:, t, :])
                d_gates_o = do * o_data[:, t, :] * (1 - o_data[:, t, :])
                d_gates_g = dg * (1 - g_data[:, t, :]**2)
                
                dgates = xp.concatenate((d_gates_i, d_gates_f, d_gates_o, d_gates_g), axis=1)

                # GradyanlarÄ± biriktir
                x_t = x_data[:, t, :]
                h_prev = h_data[:, t-1, :] if t > 0 else xp.zeros((_N, _H))
                
                dx[:, t, :] = dgates @ self.W_x.data.T
                dh_next = dgates @ self.W_h.data.T
                dc_next = dc * f_data[:, t, :]
                
                dW_x += x_t.T @ dgates
                dW_h += h_prev.T @ dgates
                db += xp.sum(dgates, axis=0)

            # Hesaplanan gradyanlarÄ± tensÃ¶rlere ata
            if x.requires_grad and x.grad is not None: x.grad += dx
            if self.W_x.requires_grad and self.W_x.grad is not None: self.W_x.grad += dW_x
            if self.W_h.requires_grad and self.W_h.grad is not None: self.W_h.grad += dW_h
            if self.b.requires_grad and self.b.grad is not None: self.b.grad += db

        out._backward = _backward
        # Sadece son gizli durumu dÃ¶ndÃ¼rerek uyumluluÄŸu koru
        return Tensor(h_all[:, -1, :], _children=(out,), _op="lstm_last_step")


    def forward_old(self, x: Tensor) -> Tensor:
        # Eski forward metodu referans iÃ§in burada bÄ±rakÄ±labilir
        # ...
        pass
========== FILE: learner/src/azuraforge_learner/learner.py ==========
# learner/src/azuraforge_learner/learner.py

import time
from typing import Any, Dict, List, Optional
import numpy as np

from azuraforge_core import Tensor
from .events import Event
from .models import Sequential
from .losses import Loss
from .optimizers import Optimizer
from .callbacks import Callback

class Learner:
    def __init__(self, model: Sequential, criterion: Loss, optimizer: Optimizer, callbacks: Optional[List[Callback]] = None):
        self.model = model
        self.criterion = criterion
        self.optimizer = optimizer
        self.callbacks = callbacks or []
        
        # KRÄ°TÄ°K DÃœZELTME: TÃ¼m callback'lere bu learner Ã¶rneÄŸini tanÄ±t.
        # Bu, callback'lerin `self.learner` Ã¼zerinden `predict` gibi metotlara eriÅŸmesini saÄŸlar.
        for cb in self.callbacks:
            cb.set_learner(self)
                 
        self.history: Dict[str, List[float]] = {}
        self.stop_training: bool = False

    def _publish(self, event_name: str, payload: Optional[Dict[str, Any]] = None):
        """OlayÄ± tÃ¼m callback'lere yayÄ±nlar."""
        event = Event(name=event_name, learner=self, payload=payload or {})
        for cb in self.callbacks:
            cb(event)

    def fit(self, X_train: np.ndarray, y_train: np.ndarray, epochs: int, pipeline_name: str = "Bilinmiyor"):
        self.history = {"loss": []}
        X_train_t, y_train_t = Tensor(X_train), Tensor(y_train)
        
        self._publish("train_begin", payload={"total_epochs": epochs, "status_text": "EÄŸitim baÅŸlÄ±yor...", "pipeline_name": pipeline_name})
        
        for epoch in range(epochs):
            if self.stop_training:
                break
            
            self._publish("epoch_begin", payload={"epoch": epoch, "total_epochs": epochs, "pipeline_name": pipeline_name})
            
            y_pred = self.model(X_train_t)
            loss = self.criterion(y_pred, y_train_t)
            
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
            
            current_loss = loss.to_cpu().item() if hasattr(loss, 'to_cpu') else float(loss.data)
            
            epoch_logs = {
                "epoch": epoch + 1, "total_epochs": epochs, "loss": current_loss,
                "status_text": f"Epoch {epoch + 1}/{epochs} tamamlandÄ±, KayÄ±p: {current_loss:.6f}",
                "pipeline_name": pipeline_name
            }
            
            self.history["loss"].append(current_loss)
            self._publish("epoch_end", payload=epoch_logs)
            
        self._publish("train_end", payload={"status_text": "EÄŸitim tamamlandÄ±.", "pipeline_name": pipeline_name})
        return self.history
        
    def predict(self, X_test: np.ndarray) -> np.ndarray:
        if not isinstance(X_test, np.ndarray):
            raise TypeError("Girdi (X_test) bir NumPy dizisi olmalÄ±dÄ±r.")
        
        input_tensor = Tensor(X_test)
        predictions_tensor = self.model(input_tensor)
        return predictions_tensor.to_cpu()

    def evaluate(self, X_val: np.ndarray, y_val: np.ndarray) -> Dict[str, float]:
        from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
        
        y_val_t = Tensor(y_val)
        y_pred_t = self.model(Tensor(X_val))
        
        val_loss = self.criterion(y_pred_t, y_val_t).to_cpu().item()
        y_pred_np = y_pred_t.to_cpu()
        
        y_val_np = y_val if isinstance(y_val, np.ndarray) else np.array(y_val)
        
        val_r2 = r2_score(y_val_np, y_pred_np)
        val_mae = mean_absolute_error(y_val_np, y_pred_np)
        val_rmse = np.sqrt(mean_squared_error(y_val_np, y_pred_np))

        return {"val_loss": val_loss, "val_r2": val_r2, "val_mae": val_mae, "val_rmse": val_rmse}
========== FILE: learner/src/azuraforge_learner/losses.py ==========
from azuraforge_core import Tensor

class Loss:
    def __call__(self, y_pred: Tensor, y_true: Tensor) -> Tensor: raise NotImplementedError

class MSELoss(Loss):
    def __call__(self, y_pred: Tensor, y_true: Tensor) -> Tensor:
        return ((y_pred - y_true) ** 2).mean()

========== FILE: learner/src/azuraforge_learner/models.py ==========
from typing import List
from .layers import Layer
from azuraforge_core import Tensor

class Sequential(Layer):
    def __init__(self, *layers: Layer):
        self.layers = list(layers)
    def forward(self, x: Tensor) -> Tensor:
        for layer in self.layers:
            x = layer(x)
        return x
    def parameters(self) -> List[Tensor]:
        return [p for layer in self.layers for p in layer.parameters()]

========== FILE: learner/src/azuraforge_learner/optimizers.py ==========
from typing import List
from azuraforge_core import Tensor

class Optimizer:
    def __init__(self, params: List[Tensor], lr: float):
        self.params = [p for p in params if p.requires_grad]
        self.lr = lr
    def step(self) -> None: raise NotImplementedError
    def zero_grad(self) -> None:
        for p in self.params:
            if p.grad is not None: p.grad.fill(0.0)

class SGD(Optimizer):
    def step(self) -> None:
        for p in self.params:
            if p.grad is not None: p.data -= self.lr * p.grad

# YENÄ°: Adam Optimizer eklendi
class Adam(Optimizer):
    def __init__(self, params: List[Tensor], lr: float = 0.001, beta1: float = 0.9, beta2: float = 0.999, epsilon: float = 1e-8):
        super().__init__(params, lr)
        from azuraforge_core import xp # xp'yi burada import ediyoruz
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.m = {id(p): xp.zeros_like(p.data) for p in self.params}
        self.v = {id(p): xp.zeros_like(p.data) for p in self.params}
        self.t = 0

    def step(self) -> None:
        self.t += 1
        from azuraforge_core import xp # xp'yi burada import ediyoruz
        for p in self.params:
            if p.grad is not None:
                param_id = id(p)
                self.m[param_id] = self.beta1 * self.m[param_id] + (1 - self.beta1) * p.grad
                self.v[param_id] = self.beta2 * self.v[param_id] + (1 - self.beta2) * (p.grad**2)

                m_hat = self.m[param_id] / (1 - self.beta1**self.t)
                v_hat = self.v[param_id] / (1 - self.beta2**self.t)

                update_val = self.lr * m_hat / (xp.sqrt(v_hat) + self.epsilon)
                p.data -= update_val
========== FILE: learner/src/azuraforge_learner/pipelines.py ==========
# learner/src/azuraforge_learner/pipelines.py

import logging
import os
from abc import ABC, abstractmethod
from typing import Dict, Any, Tuple, Optional, List

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

from .learner import Learner, Callback
from .models import Sequential
from .reporting import generate_regression_report
from .optimizers import Adam, SGD
from .losses import MSELoss
from .events import Event
from .caching import get_cache_filepath, load_from_cache, save_to_cache

def _create_sequences(data: np.ndarray, seq_length: int) -> Tuple[np.ndarray, np.ndarray]:
    xs, ys = [], []
    for i in range(len(data) - seq_length):
        x = data[i:(i + seq_length)]
        y = data[i + seq_length]
        xs.append(x)
        ys.append(y)
    # DÃœZELTME: y'nin her zaman (samples, features) ÅŸeklinde olmasÄ±nÄ± saÄŸla
    return np.array(xs), np.array(ys).reshape(-1, data.shape[1])

class BasePipeline(ABC):
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)

    @abstractmethod
    def run(self, callbacks: Optional[List[Callback]] = None) -> Dict[str, Any]:
        pass

class LivePredictionCallback(Callback):
    def __init__(self, pipeline: 'TimeSeriesPipeline', X_val: np.ndarray, y_val: np.ndarray, time_index_val: pd.Index):
        super().__init__()
        self.pipeline = pipeline
        self.X_val = X_val
        self.y_val = y_val
        self.time_index_val = time_index_val
        self.validate_every = self.pipeline.config.get("training_params", {}).get("validate_every", 5)
        self.last_results: Dict[str, Any] = {}

    def on_epoch_end(self, event: Event) -> None:
        epoch = event.payload.get("epoch", 0)
        total_epochs = event.payload.get("total_epochs", 1)

        if (epoch % self.validate_every == 0 and epoch > 0) or (epoch == total_epochs):
            if not self.learner: return

            y_pred_scaled = self.learner.predict(self.X_val)
            
            y_test_unscaled, y_pred_unscaled = self.pipeline._inverse_transform_all(
                self.y_val, y_pred_scaled
            )
            
            validation_payload = {
                "x_axis": [d.isoformat() for d in self.time_index_val],
                "y_true": y_test_unscaled.tolist(), "y_pred": y_pred_unscaled.tolist(),
                "x_label": "Tarih", "y_label": self.pipeline._get_target_and_feature_cols()[0]
            }
            event.payload['validation_data'] = validation_payload
            
            from sklearn.metrics import r2_score, mean_absolute_error
            self.last_results = {
                "history": self.learner.history,
                "metrics": {
                    'r2_score': r2_score(y_test_unscaled, y_pred_unscaled),
                    'mae': mean_absolute_error(y_test_unscaled, y_pred_unscaled)
                },
                "y_true": y_test_unscaled, "y_pred": y_pred_unscaled,
                "time_index": self.time_index_val, "y_label": self.pipeline._get_target_and_feature_cols()[0]
            }

class TimeSeriesPipeline(BasePipeline):
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.scaler = MinMaxScaler(feature_range=(-1, 1))
        self.feature_scaler = MinMaxScaler(feature_range=(-1, 1))
        self.learner: Optional[Learner] = None
        # Testlerin ve hata ayÄ±klamanÄ±n eriÅŸebilmesi iÃ§in sÄ±nÄ±f Ã¶zelliklerini tanÄ±mla
        self.X_train: Optional[np.ndarray] = None
        self.y_train: Optional[np.ndarray] = None
        self.X_test: Optional[np.ndarray] = None
        self.y_test: Optional[np.ndarray] = None
        self.time_index_test: Optional[pd.Index] = None

    @abstractmethod
    def _load_data_from_source(self) -> pd.DataFrame:
        pass
        
    def get_caching_params(self) -> Dict[str, Any]:
        return self.config.get("data_sourcing", {})

    @abstractmethod
    def _get_target_and_feature_cols(self) -> Tuple[str, List[str]]:
        pass
    
    @abstractmethod
    def _create_model(self, input_shape: Tuple) -> Sequential:
        pass

    def _create_learner(self, model: Sequential, callbacks: Optional[List[Callback]]) -> Learner:
        training_params = self.config.get("training_params", {})
        lr = float(training_params.get("lr", 0.001))
        optimizer_type = str(training_params.get("optimizer", "adam")).lower()
        optimizer = Adam(model.parameters(), lr=lr) if optimizer_type == "adam" else SGD(model.parameters(), lr=lr)
        return Learner(model, MSELoss(), optimizer, callbacks=callbacks)

    def _inverse_transform_all(self, y_true_scaled, y_pred_scaled):
        y_true_unscaled_transformed = self.scaler.inverse_transform(y_true_scaled)
        y_pred_unscaled_transformed = self.scaler.inverse_transform(y_pred_scaled)

        target_transform = self.config.get("feature_engineering", {}).get("target_col_transform")
        if target_transform == 'log':
            y_true_final = np.expm1(y_true_unscaled_transformed)
            y_pred_final = np.expm1(y_pred_unscaled_transformed)
        else:
            y_true_final = y_true_unscaled_transformed
            y_pred_final = y_pred_unscaled_transformed
            
        return y_true_final.flatten(), y_pred_final.flatten()

    def run(self, callbacks: Optional[List[Callback]] = None) -> Dict[str, Any]:
        self.logger.info(f"'{self.config.get('pipeline_name')}' pipeline baÅŸlatÄ±lÄ±yor...")
        
        system_config = self.config.get("system", {})
        cache_enabled = system_config.get("caching_enabled", True)
        cache_dir = os.getenv("CACHE_DIR", ".cache")
        cache_max_age = system_config.get("cache_max_age_hours", 24)
        
        cache_params = self.get_caching_params()
        cache_filepath = get_cache_filepath(cache_dir, self.config.get('pipeline_name', 'default_context'), cache_params)

        raw_data = None
        if cache_enabled: raw_data = load_from_cache(cache_filepath, cache_max_age)
        if raw_data is None:
            self.logger.info("Ã–nbellek boÅŸ veya geÃ§ersiz. Veri kaynaktan Ã§ekiliyor...")
            raw_data = self._load_data_from_source()
            if cache_enabled and isinstance(raw_data, pd.DataFrame) and not raw_data.empty:
                save_to_cache(raw_data, cache_filepath)

        target_col, feature_cols = self._get_target_and_feature_cols()
        
        features_df = raw_data[feature_cols].copy()
        target_series = raw_data[target_col].copy()

        target_transform = self.config.get("feature_engineering", {}).get("target_col_transform")
        if target_transform == 'log':
            self.logger.info(f"'{target_col}' sÃ¼tununa log(1+x) dÃ¶nÃ¼ÅŸÃ¼mÃ¼ uygulanÄ±yor.")
            target_series = np.log1p(target_series)
        
        scaled_features = self.feature_scaler.fit_transform(features_df)
        scaled_target = self.scaler.fit_transform(target_series.values.reshape(-1, 1))
        
        scaled_data = np.concatenate([scaled_features, scaled_target], axis=1)

        sequence_length = self.config.get("model_params", {}).get("sequence_length", 60)
        if len(scaled_data) <= sequence_length:
            return {"status": "failed", "message": "Sekans oluÅŸturmak iÃ§in yeterli veri yok."}
        
        X, y_unsequenced = _create_sequences(scaled_data, sequence_length)
        
        target_idx = feature_cols.index(target_col)
        y = y_unsequenced[:, target_idx].reshape(-1, 1)
        
        test_size = self.config.get("training_params", {}).get("test_size", 0.2)
        split_idx = int(len(X) * (1 - test_size))

        # === DEÄžÄ°ÅžÄ°KLÄ°K BURADA ===
        # DeÄŸiÅŸkenleri sÄ±nÄ±f Ã¶zelliÄŸi olarak atÄ±yoruz
        self.X_train, self.X_test = X[:split_idx], X[split_idx:]
        self.y_train, self.y_test = y[:split_idx], y[split_idx:]
        # === DEÄžÄ°ÅžÄ°KLÄ°K SONU ===
        
        self.time_index_test = raw_data.index[split_idx + sequence_length:]

        model = self._create_model(self.X_train.shape)
        
        live_predict_cb = LivePredictionCallback(pipeline=self, X_val=self.X_test, y_val=self.y_test, time_index_val=self.time_index_test)
        all_callbacks = (callbacks or []) + [live_predict_cb]
        
        self.learner = self._create_learner(model, all_callbacks)

        epochs = int(self.config.get("training_params", {}).get("epochs", 50))
        self.logger.info(f"{epochs} epoch iÃ§in model eÄŸitimi baÅŸlÄ±yor...")

        history = self.learner.fit(self.X_train, self.y_train, epochs=epochs, pipeline_name=self.config.get("pipeline_name"))
        
        final_results = live_predict_cb.last_results
        if not final_results:
            return {"status": "failed", "message": "EÄŸitim tamamlanamadÄ± veya hiÃ§ doÄŸrulama yapÄ±lmadÄ±."}

        self.logger.info("Rapor oluÅŸturuluyor...")
        generate_regression_report(final_results, self.config)
        
        final_loss = history['loss'][-1] if history.get('loss') else None
        
        return {
            "final_loss": final_loss,
            "metrics": final_results.get('metrics', {}),
            "history": final_results.get('history', {}),
            "y_true": final_results.get('y_true', np.array([])).tolist(),
            "y_pred": final_results.get('y_pred', np.array([])).tolist(),
            "time_index": [d.isoformat() for d in final_results.get('time_index', [])]
        }
========== FILE: learner/src/azuraforge_learner/reporting.py ==========
# learner/src/azuraforge_learner/reporting.py

import os
import logging
import json # EKSÄ°K OLAN IMPORT EKLENDÄ°
from datetime import datetime
from typing import Any, Dict, List, Optional
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

def set_professional_style():
    """Matplotlib iÃ§in profesyonel bir stil ayarlar."""
    try:
        plt.style.use('seaborn-v0_8-whitegrid')
        plt.rcParams.update({
            'font.family': 'sans-serif', 'font.sans-serif': 'DejaVu Sans',
            'figure.figsize': (12, 7), 'axes.labelweight': 'bold',
            'axes.titleweight': 'bold', 'grid.color': '#dddddd'
        })
    except Exception as e:
        logging.warning(f"Matplotlib stili yÃ¼klenemedi: {e}. VarsayÄ±lan kullanÄ±lacak.")

def plot_loss_history(history: Dict[str, List[float]], save_path: str):
    set_professional_style()
    fig, ax = plt.subplots()
    ax.plot(history.get('loss', []), label='EÄŸitim KaybÄ±')
    if 'val_loss' in history:
        ax.plot(history['val_loss'], label='DoÄŸrulama KaybÄ±')
    ax.set_title('Model Ã–ÄŸrenme EÄŸrisi')
    ax.set_xlabel('Epoch')
    ax.set_ylabel('KayÄ±p (Loss)')
    ax.legend()
    ax.grid(True)
    fig.savefig(save_path)
    plt.close(fig)

def plot_prediction_comparison(y_true: np.ndarray, y_pred: np.ndarray, time_index: pd.Index, save_path: str, y_label: str):
    set_professional_style()
    fig, ax = plt.subplots()
    ax.plot(time_index, y_true, label='GerÃ§ek DeÄŸerler', marker='.', markersize=4, linestyle='-')
    ax.plot(time_index, y_pred, label='Tahmin Edilen DeÄŸerler', linestyle='--')
    ax.set_title('Tahmin vs GerÃ§ek DeÄŸerler')
    ax.set_xlabel('Tarih')
    ax.set_ylabel(y_label)
    ax.legend()
    ax.grid(True)
    plt.xticks(rotation=45)
    fig.tight_layout()
    fig.savefig(save_path)
    plt.close(fig)

def generate_regression_report(results: Dict[str, Any], config: Dict[str, Any]):
    experiment_dir = config.get('experiment_dir')
    if not experiment_dir:
        logging.error("Rapor oluÅŸturmak iÃ§in 'experiment_dir' konfigÃ¼rasyonda bulunamadÄ±.")
        return
        
    report_name = config.get('pipeline_name', 'Bilinmeyen Deney')
    
    img_dir = os.path.join(experiment_dir, "images")
    os.makedirs(img_dir, exist_ok=True)
    report_path = os.path.join(experiment_dir, "report.md")
    logging.info(f"Regresyon raporu oluÅŸturuluyor: {report_path}")

    loss_img_path = os.path.join(img_dir, "loss_history.png")
    if 'history' in results and results['history'].get('loss'):
        plot_loss_history(results['history'], save_path=loss_img_path)

    comparison_img_path = os.path.join(img_dir, "prediction_comparison.png")
    if 'y_true' in results and 'y_pred' in results and 'time_index' in results:
        plot_prediction_comparison(
            y_true=np.asarray(results['y_true']), y_pred=np.asarray(results['y_pred']),
            time_index=results['time_index'], save_path=comparison_img_path,
            y_label=results.get('y_label', 'DeÄŸer')
        )

    metrics = results.get('metrics', {})
    r2 = metrics.get('r2_score')
    mae = metrics.get('mae')
    
    with open(report_path, "w", encoding="utf-8") as f:
        f.write(f"# Regresyon Analiz Raporu: {report_name}\n\n")
        f.write(f"**Rapor Tarihi:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write("## 1. Performans Ã–zeti\n\n")
        if r2 is not None:
            f.write(f"- **RÂ² Skoru:** `{r2:.4f}`\n")
        if mae is not None:
            f.write(f"- **Ortalama Mutlak Hata (MAE):** `{mae:.4f}`\n\n")

        f.write("## 2. Tahmin KarÅŸÄ±laÅŸtÄ±rmasÄ±\n\n")
        f.write("AÅŸaÄŸÄ±daki grafik, modelin test seti Ã¼zerindeki tahminlerini (turuncu) gerÃ§ek deÄŸerlerle (mavi) karÅŸÄ±laÅŸtÄ±rÄ±r.\n\n")
        if os.path.exists(comparison_img_path):
            f.write(f"![Tahmin KarÅŸÄ±laÅŸtÄ±rma GrafiÄŸi](images/{os.path.basename(comparison_img_path)})\n\n")
        
        f.write("## 3. EÄŸitim SÃ¼reci\n\n")
        f.write("Bu grafik, modelin eÄŸitim sÄ±rasÄ±ndaki kayÄ±p deÄŸerinin epoch'lara gÃ¶re deÄŸiÅŸimini gÃ¶sterir.\n\n")
        if os.path.exists(loss_img_path):
            f.write(f"![EÄŸitim KaybÄ±](images/{os.path.basename(loss_img_path)})\n\n")

        f.write("## 4. Deney KonfigÃ¼rasyonu\n\n")
        f.write("```json\n")
        f.write(json.dumps(config, indent=4, default=str))
        f.write("\n```\n")
========== FILE: learner/src/azuraforge_learner/__init__.py ==========
# learner/src/azuraforge_learner/__init__.py

from .events import Event
from .callbacks import Callback
from .losses import Loss, MSELoss
from .layers import Layer, Linear, ReLU, Sigmoid, LSTM
from .models import Sequential
from .optimizers import Optimizer, SGD, Adam
from .learner import Learner
from .pipelines import BasePipeline, TimeSeriesPipeline # YENÄ°

__all__ = [
    "Event", "Callback",
    "Loss", "MSELoss", 
    "Layer", "Linear", "ReLU", "Sigmoid", "LSTM",
    "Sequential", 
    "Optimizer", "SGD", "Adam",
    "Learner",
    "BasePipeline", 
    "TimeSeriesPipeline" # YENÄ°
]
========== FILE: learner/tests/azuraforge_learner/test_learner_components.py ==========
import pytest
import numpy as np

from azuraforge_learner import Learner, Sequential, Linear, ReLU, MSELoss, SGD

def test_learner_fit_simple_regression():
    X_train = np.array([[-1.0], [0.0], [1.0], [2.0]], dtype=np.float32)
    y_train = np.array([[-1.0], [1.0], [3.0], [5.0]], dtype=np.float32)
    
    model = Sequential(Linear(1, 1))
    criterion = MSELoss()
    optimizer = SGD(model.parameters(), lr=0.1)
    learner = Learner(model, criterion, optimizer)
    
    initial_loss = learner.evaluate(X_train, y_train)['val_loss']
    
    learner.fit(X_train, y_train, epochs=30)
    
    final_loss = learner.history['loss'][-1]
    
    print(f"Initial Loss: {initial_loss}, Final Loss: {final_loss}")
    assert final_loss < initial_loss / 5

def test_sequential_model_forward_pass():
    model = Sequential(Linear(2, 4), ReLU(), Linear(4, 1))
    from azuraforge_core import Tensor
    
    input_tensor = Tensor(np.random.randn(10, 2))
    output_tensor = model(input_tensor)
    
    assert output_tensor.data.shape == (10, 1)

========== FILE: learner/tests/azuraforge_learner/test_pipelines.py ==========
import pytest
import numpy as np
import pandas as pd
from unittest.mock import MagicMock, patch

from azuraforge_learner.pipelines import TimeSeriesPipeline, _create_sequences

# --- Helper Fonksiyon Testleri ---

def test_create_sequences():
    """_create_sequences fonksiyonunun doÄŸru ÅŸekil ve iÃ§erikte diziler oluÅŸturduÄŸunu test eder."""
    data = np.arange(10).reshape(-1, 1) # [0, 1, ..., 9]
    seq_length = 3
    
    X, y = _create_sequences(data, seq_length)
    
    # Beklenen Ã§Ä±ktÄ± sayÄ±sÄ±: 10 - 3 = 7
    assert X.shape == (7, 3, 1)
    assert y.shape == (7, 1)
    
    # Ä°lk sekansÄ± kontrol et
    assert np.array_equal(X[0], np.array([[0], [1], [2]]))
    assert np.array_equal(y[0], np.array([3]))
    
    # Son sekansÄ± kontrol et
    assert np.array_equal(X[-1], np.array([[6], [7], [8]]))
    assert np.array_equal(y[-1], np.array([9]))

# --- TimeSeriesPipeline Testleri ---

# Test iÃ§in somut bir Pipeline sÄ±nÄ±fÄ± oluÅŸturalÄ±m
class MockTimeSeriesPipeline(TimeSeriesPipeline):
    def _load_data_from_source(self) -> pd.DataFrame:
        dates = pd.to_datetime(pd.date_range(start="2023-01-01", periods=100))
        data = {'Close': np.linspace(100, 200, 100), 'Volume': np.random.rand(100) * 1000}
        return pd.DataFrame(data, index=dates)

    def _get_target_and_feature_cols(self) -> tuple[str, list[str]]:
        return "Close", ["Close", "Volume"]

    def _create_model(self, input_shape: tuple):
        # GerÃ§ek bir model oluÅŸturmaya gerek yok, sadece bir mock nesne dÃ¶ndÃ¼r
        return MagicMock()

@pytest.fixture
def pipeline_instance():
    """Her test iÃ§in taze bir pipeline Ã¶rneÄŸi oluÅŸturur."""
    config = {
        "pipeline_name": "test_pipeline",
        "model_params": {"sequence_length": 10},
        "training_params": {"test_size": 0.2},
        "feature_engineering": {"target_col_transform": "none"},
        "system": {"caching_enabled": False}
    }
    return MockTimeSeriesPipeline(config)

def test_pipeline_data_split(pipeline_instance):
    """Pipeline'Ä±n veriyi doÄŸru ÅŸekilde train/test olarak ayÄ±rdÄ±ÄŸÄ±nÄ± test eder."""
    
    # run metodunun iÃ§indeki ilgili kÄ±sÄ±mlarÄ± taklit ederek test edelim
    # Normalde run() metodunu Ã§aÄŸÄ±rÄ±rdÄ±k ama o Ã§ok fazla ÅŸey yapÄ±yor.
    # Bu yÃ¼zden sadece veri hazÄ±rlama adÄ±mlarÄ±nÄ± test ediyoruz.
    
    with patch.object(pipeline_instance, '_create_learner', return_value=MagicMock()) as mock_create_learner:
        with patch('azuraforge_learner.pipelines.generate_regression_report'): # RaporlamayÄ± mock'la
             # `run` metodunu Ã§aÄŸÄ±rdÄ±ÄŸÄ±mÄ±zda, iÃ§indeki bazÄ± adÄ±mlarÄ± doÄŸrulamak istiyoruz.
             # LivePredictionCallback'in sahte sonuÃ§lar dÃ¶ndÃ¼rmesini saÄŸlÄ±yoruz
            with patch('azuraforge_learner.pipelines.LivePredictionCallback') as mock_live_cb:
                # Callback'in last_results Ã¶zelliÄŸine sahte bir deÄŸer atÄ±yoruz
                mock_instance = mock_live_cb.return_value
                mock_instance.last_results = {'metrics': {}, 'history': {'loss': [0.1]}}

                pipeline_instance.run()

    # run() Ã§aÄŸrÄ±ldÄ±ktan sonra, pipeline'Ä±n iÃ§ state'ini kontrol edelim
    total_samples = 100 - pipeline_instance.config["model_params"]["sequence_length"] # 90
    expected_test_size = int(total_samples * 0.2) # 18
    expected_train_size = total_samples - expected_test_size # 72
    
    assert pipeline_instance.X_train.shape[0] == expected_train_size
    assert pipeline_instance.y_train.shape[0] == expected_train_size
    assert pipeline_instance.X_test.shape[0] == expected_test_size
    assert pipeline_instance.y_test.shape[0] == expected_test_size
    assert len(pipeline_instance.time_index_test) == expected_test_size


def test_pipeline_log_transform(pipeline_instance):
    """Pipeline'Ä±n logaritmik dÃ¶nÃ¼ÅŸÃ¼mÃ¼ ve ters dÃ¶nÃ¼ÅŸÃ¼mÃ¼ doÄŸru yaptÄ±ÄŸÄ±nÄ± test eder."""
    pipeline_instance.config["feature_engineering"]["target_col_transform"] = "log"
    
    # Orijinal deÄŸerler (Ã¶lÃ§eklenmiÅŸ ve log alÄ±nmÄ±ÅŸ gibi davranalÄ±m)
    y_true_scaled = np.array([[0.5], [0.6]])
    y_pred_scaled = np.array([[0.51], [0.59]])
    
    # Ters Ã¶lÃ§ekleme iÃ§in sahte scaler'lar
    mock_scaler = MagicMock()
    # np.log1p(100) -> 4.615, np.log1p(200) -> 5.303. Scaler bu aralÄ±kta Ã§alÄ±ÅŸsÄ±n.
    # inverse_transform'un un-log'lanmÄ±ÅŸ ama hala Ã¶lÃ§ekli deÄŸerler dÃ¶ndÃ¼rdÃ¼ÄŸÃ¼nÃ¼ varsayalÄ±m.
    mock_scaler.inverse_transform.side_effect = lambda x: np.expm1(x * 5) # Basit bir ters Ã¶lÃ§ekleme taklidi
    pipeline_instance.scaler = mock_scaler
    
    # Metodu Ã§aÄŸÄ±r
    y_true_final, y_pred_final = pipeline_instance._inverse_transform_all(y_true_scaled, y_pred_scaled)
    
    # SonuÃ§larÄ±n beklendiÄŸi gibi Ã¼ssÃ¼ alÄ±nmÄ±ÅŸ (un-logged) olduÄŸunu kontrol et
    # mock_scaler.inverse_transform'dan gelen deÄŸerlerin expm1'den geÃ§tiÄŸini doÄŸrulamalÄ±yÄ±z.
    # Beklenen davranÄ±ÅŸ:
    # 1. unscaled_transformed = mock_scaler.inverse_transform(y_true_scaled)
    # 2. y_true_final = np.expm1(unscaled_transformed)
    # Bizim testimizde mock_scaler.inverse_transform zaten expm1'i iÃ§eriyor gibi davrandÄ±k.
    # Bu yÃ¼zden doÄŸrudan sonuÃ§larÄ± kontrol edebiliriz.
    
    # Testi daha basit yapalÄ±m:
    # GerÃ§ek deÄŸerler
    original_value = np.array([[100]]) 
    # Log dÃ¶nÃ¼ÅŸÃ¼mÃ¼
    log_value = np.log1p(original_value) 
    # Ters dÃ¶nÃ¼ÅŸÃ¼m
    unlog_value = np.expm1(log_value)
    
    assert np.allclose(original_value, unlog_value)
========== FILE: tools/snapshot_generator.py ==========
import os
import sys
import json
import argparse
from typing import List, Dict, Any, Set, Optional, Tuple
import re

# Configuration for included/excluded paths and extensions
DEFAULT_INCLUDE_DIRS = ["."]
DEFAULT_INCLUDE_EXTENSIONS = [
    ".toml",
    ".py",
    ".yaml",
    ".yml",
    ".json",
    ".md",
    ".txt",
    "html",
    ".bat",
    ".sh",
    ".jsx",
    ".js",
    ".json",
    ".css"    
]
DEFAULT_EXCLUDE_PATTERNS = [
    "__pycache__",
    ".git",
    ".venv",
    ".vscode",
    ".idea",
    "build",
    "dist",
    "*.egg-info",
    "*.pyc",
    "*.so",
    "*.pyd",
    ".pytest_cache",
    ".mypy_cache",
    ".dataset",
    "dataset",
    ".logs",
    "logs",
    ".output",
    "output",
    "inputs",
    "outputs",
    ".tmp",
    "checkpoints",
    "reports",
    "docs/_build",
    "site",
    "node_modules",
    ".DS_Store",
    "Thumbs.db", # Windows thumbnail cache
    "*.lock", # npm lock dosyalarÄ± gibi
]

FILE_HEADER_TEMPLATE = "========== FILE: {file_path} =========="
SNAPSHOT_INFO_TEMPLATE = """PROJE KOD SNAPSHOT (TAM)
Toplam {total_files_placeholder} dosya bulundu ve eklendi.
Dahil Edilen Dizinler: {included_dirs_placeholder}
Dahil Edilen UzantÄ±lar: {included_extensions_placeholder}
HariÃ§ Tutulan Desenler/Yollar: {excluded_patterns_placeholder}
================================================================================
"""

def clean_code_comments(content: str, file_extension: str) -> str:
    """Removes most comments from code, attempting to preserve shebangs and type hints."""
    if file_extension not in [".py", ".sh", ".bat"]: return content
    lines = content.splitlines()
    cleaned_lines = []
    for line in lines:
        stripped_line = line.strip()
        if file_extension == ".py":
            # Preserve special comments like '# type:' and shebangs
            if stripped_line.startswith("# type:") or stripped_line.startswith("# noqa"): 
                cleaned_lines.append(line)
            elif stripped_line.startswith("#!/"): 
                cleaned_lines.append(line)
            # Remove inline comments
            elif "#" in line and not stripped_line.startswith("#"): 
                cleaned_lines.append(line.split("#", 1)[0].rstrip())
            # Remove full-line comments
            elif stripped_line.startswith("#"):
                continue # Skip full line comments
            else: 
                cleaned_lines.append(line)
        elif file_extension == ".sh":
            if stripped_line.startswith("#!/"): 
                cleaned_lines.append(line)
            elif not stripped_line.startswith("#"): 
                cleaned_lines.append(line)
        elif file_extension == ".bat":
            if not stripped_line.lower().startswith("rem "): 
                cleaned_lines.append(line)
        else: 
            cleaned_lines.append(line)
    return "\n".join(cleaned_lines)


def should_exclude(item_path: str, root_path: str, exclude_patterns: List[str]) -> bool:
    """Checks if a file or directory should be excluded based on the patterns."""
    normalized_item_path = os.path.normpath(item_path)
    normalized_root_path = os.path.normpath(os.path.abspath(root_path))
    
    try:
        relative_item_path = os.path.relpath(normalized_item_path, normalized_root_path)
    except ValueError:
        # If item_path is not relative to root_path (e.g., different drive on Windows)
        # or other path normalization issues, treat it as its own path.
        relative_item_path = normalized_item_path
    
    relative_item_path_slashes = relative_item_path.replace(os.sep, "/")

    for pattern in exclude_patterns:
        normalized_pattern = os.path.normpath(pattern)
        normalized_pattern_slashes = normalized_pattern.replace(os.sep, "/")

        # Wildcard extensions like "*.pyc"
        if pattern.startswith("*."):
            if relative_item_path_slashes.endswith(pattern[1:]): return True
        # Directory names or file names without path
        elif "/" not in pattern and "." not in pattern and not pattern.startswith("*"):
            path_segments = relative_item_path_slashes.split("/")
            if pattern in path_segments: return True
        # Exact file name match
        elif pattern == os.path.basename(normalized_item_path): return True
        # Full path prefix match or relative path match
        elif normalized_item_path.startswith(os.path.join(normalized_root_path, normalized_pattern)) or \
             relative_item_path_slashes.startswith(normalized_pattern_slashes): return True
        # Absolute path match
        elif os.path.isabs(normalized_pattern) and normalized_pattern == normalized_item_path: return True
    return False


def collect_project_files_full(
    output_file: str,
    include_dirs: Optional[List[str]] = None,
    include_extensions: Optional[List[str]] = None,
    exclude_patterns: Optional[List[str]] = None,
    base_dir: str = ".",
    clean_comments: bool = False,
) -> None:
    if include_dirs is None: include_dirs = DEFAULT_INCLUDE_DIRS
    if include_extensions is None: include_extensions = DEFAULT_INCLUDE_EXTENSIONS
    if exclude_patterns is None: exclude_patterns = DEFAULT_EXCLUDE_PATTERNS

    abs_base_dir = os.path.abspath(base_dir)
    
    snapshot_content_header = SNAPSHOT_INFO_TEMPLATE.format(
        total_files_placeholder="{total_files_counter}",
        included_dirs_placeholder=", ".join(include_dirs),
        included_extensions_placeholder=", ".join(include_extensions),
        excluded_patterns_placeholder=", ".join(exclude_patterns),
    )

    all_found_relative_paths: Set[str] = set() # This set stores relative paths to prevent duplicates
    content_parts: List[str] = [snapshot_content_header]
    processed_files_count = 0

    for inc_dir_pattern in include_dirs:
        current_scan_dir = os.path.abspath(os.path.join(abs_base_dir, inc_dir_pattern))
        if not os.path.exists(current_scan_dir):
            print(f"Warning: Include directory '{inc_dir_pattern}' (resolved to '{current_scan_dir}') does not exist. Skipping.")
            continue

        for root, dirs, files in os.walk(current_scan_dir, topdown=True):
            # Filter directories in-place to prevent os.walk from entering excluded ones
            dirs[:] = [
                d for d in dirs
                if not should_exclude(os.path.join(root, d), abs_base_dir, exclude_patterns)
            ]
            for file_name in files:
                file_path = os.path.join(root, file_name)

                relative_file_path = os.path.relpath(file_path, abs_base_dir)
                display_path = relative_file_path.replace(os.sep, "/")

                # Skip if already processed (e.g., if included by multiple patterns)
                if display_path in all_found_relative_paths:
                    continue 

                # Apply exclusion patterns to files
                if should_exclude(file_path, abs_base_dir, exclude_patterns):
                    continue

                _, file_extension = os.path.splitext(file_name)
                # For extension check, handle files without an explicit extension (like Dockerfile)
                name_part_for_ext_check = file_extension.lower() if file_extension else file_name.lower()

                # Check if file extension (or full name for extensionless files) is in include list
                if any(name_part_for_ext_check.endswith(ext.lower()) for ext in include_extensions) or \
                   (not file_extension and file_name.lower() in [ext.lower() for ext in include_extensions if not ext.startswith('.')]):
                    
                    all_found_relative_paths.add(display_path) # Add to set of found paths
                    try:
                        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                            file_content = f.read()
                        
                        if clean_comments:
                            file_content = clean_code_comments(file_content, file_extension)
                        
                        # Add a leading newline for separator, then the header, then content.
                        # This creates the format: \n========== FILE:PATH==========\nCONTENT
                        content_parts.append(f"\n{FILE_HEADER_TEMPLATE.format(file_path=display_path)}\n")
                        content_parts.append(file_content)
                        processed_files_count += 1
                    except Exception as e:
                        print(f"Error reading file {relative_file_path}: {e}")
                        content_parts.append(f"\nError reading file {relative_file_path}: {e}\n")

    final_header_with_count = content_parts[0].replace("{total_files_counter}", str(processed_files_count))
    content_parts[0] = final_header_with_count

    with open(output_file, "w", encoding="utf-8") as f:
        f.write("".join(content_parts))

    print(f"Project snapshot (full) generated: {output_file}")
    print(f"Total {processed_files_count} files included.")


def restore_from_full_snapshot(
    snapshot_file: str,
    target_dir: str = ".",
    dry_run: bool = False,
    overwrite_existing: bool = False,
) -> None:
    print(f"Restoring project from snapshot: {snapshot_file}")
    if dry_run: print("DRY RUN: No files will be written.")

    try:
        with open(snapshot_file, "r", encoding="utf-8") as f:
            full_content = f.read()
    except FileNotFoundError:
        print(f"Error: Snapshot file '{snapshot_file}' not found.")
        return
    except Exception as e:
        print(f"Error reading snapshot file: {e}")
        return

    # Regex to find file blocks. It captures the file path (group 1) and its content (group 2).
    # The crucial point is that group(2) captures ALL characters (including newlines due to re.DOTALL)
    # after the header line's trailing newline, until the START of the next file header or end of string.
    # The lookahead `(?=...)` is non-consuming, so the content is captured completely.
    file_block_pattern = re.compile(
        r"^========== FILE: (.*?) ==========\n"  # Match header line and its trailing newline
        r"(.*?)"                                 # Capture content (non-greedy)
        r"(?=\n========== FILE: |\Z)",           # Lookahead: followed by newline then next header, OR end of string.
                                                # \Z matches only at the end of the string.
        re.MULTILINE | re.DOTALL
    )
    
    # Find the end of the initial info header to start parsing file blocks
    info_header_last_line = SNAPSHOT_INFO_TEMPLATE.splitlines()[-1]
    content_start_index = full_content.find(info_header_last_line)
    if content_start_index == -1:
        print("Error: Could not find the end of the snapshot info header.")
        return
    
    # Slice the content to start exactly after the info header,
    # and then lstrip any *leading* newlines that might be left before the first file block.
    # This ensures the regex for the first file block can match correctly.
    content_to_parse = full_content[content_start_index + len(info_header_last_line):].lstrip('\n')

    files_restored = 0
    files_skipped = 0
    files_overwritten = 0
    
    matches = file_block_pattern.finditer(content_to_parse)

    for match in matches:
        relative_file_path = match.group(1).strip()
        # KRÄ°TÄ°K DÃœZELTME: match.group(2) Ã¼zerinde .strip() metodunu kaldÄ±rdÄ±k.
        # Bu, tÃ¼m boÅŸluk karakterlerinin (yeni satÄ±rlar dahil) korunmasÄ±nÄ± saÄŸlar.
        content_part = match.group(2) 

        os_specific_relative_path = relative_file_path.replace("/", os.sep)
        target_file_path = os.path.join(target_dir, os_specific_relative_path)
        
        # DEBUG YARDIMI: YazÄ±lacak iÃ§eriÄŸin uzunluÄŸunu gÃ¶ster
        print(f"Processing file: {relative_file_path} (Content length: {len(content_part)}) -> {target_file_path}")

        if os.path.exists(target_file_path) and not overwrite_existing:
            print(f"  SKIPPED: File '{target_file_path}' already exists (overwrite_existing is False).")
            files_skipped += 1
            continue

        if os.path.exists(target_file_path) and overwrite_existing:
            print(f"  OVERWRITING: File '{target_file_path}'.")
            files_overwritten += 1

        if not dry_run:
            try:
                os.makedirs(os.path.dirname(target_file_path), exist_ok=True)
                with open(target_file_path, "w", encoding="utf-8") as f:
                    f.write(content_part)
                files_restored += 1
            except Exception as e:
                print(f"  ERROR: Could not write file '{target_file_path}': {e}")
        else:
            if not os.path.exists(os.path.dirname(target_file_path)):
                print(f"  DRY RUN: Would create directory {os.path.dirname(target_file_path)}")
            print(f"  DRY RUN: Would write {len(content_part)} bytes to {target_file_path}")
            files_restored += 1

    print("\n--- Restoration Summary ---")
    print(f"Files processed for restoration: {files_restored}")
    if not dry_run:
        print(f"Files actually written/overwritten: {files_restored - files_skipped}")
        print(f"Files overwritten: {files_overwritten}")
    print(f"Files skipped (already exist and overwrite=False): {files_skipped}")


def main():
    parser = argparse.ArgumentParser(description="Project Snapshot Tool (Full Version)")
    subparsers = parser.add_subparsers(dest="command", required=True)

    parser_collect = subparsers.add_parser(
        "collect", help="Collect project files into a single snapshot file."
    )
    parser_collect.add_argument(
        "output_file",
        type=str,
        default="project_snapshot_full.txt",
        nargs="?",
        help="Path to the output snapshot file (default: project_snapshot_full.txt)",
    )
    parser_collect.add_argument(
        "--include-dir",
        action="append",
        dest="include_dirs",
        help="Directory to include (relative to base_dir or absolute). Can be used multiple times. Defaults to ['.']",
    )
    parser_collect.add_argument(
        "--include-ext",
        action="append",
        dest="include_extensions",
        help="File extension to include (e.g., .py, .md). Can be used multiple times. Defaults to common code/config extensions.",
    )
    parser_collect.add_argument(
        "--exclude-pattern",
        action="append",
        dest="exclude_patterns",
        help="Pattern/path to exclude. Can be used multiple times. Defaults to common ignores.",
    )
    parser_collect.add_argument(
        "--base-dir",
        type=str,
        default=".",
        help="Base directory for the project (default: current directory). Relative paths are resolved against this.",
    )
    parser_collect.add_argument(
        "--clean-comments",
        action="store_true",
        help="Attempt to remove comments from collected code files (.py, .sh, .bat).",
    )

    parser_restore = subparsers.add_parser(
        "restore", help="Restore project files from a snapshot."
    )
    parser_restore.add_argument(
        "snapshot_file", type=str, help="Path to the snapshot file to restore from."
    )
    parser_restore.add_argument(
        "--target-dir",
        type=str,
        default=".",
        help="Directory where files will be restored (default: current directory).",
    )
    parser_restore.add_argument(
        "--dry-run",
        action="store_true",
        help="Simulate restoration without writing any files.",
    )
    parser_restore.add_argument(
        "--overwrite",
        action="store_true",
        dest="overwrite_existing",
        help="Overwrite files if they already exist in the target directory.",
    )

    args = parser.parse_args()

    if args.command == "collect":
        final_include_dirs = (
            args.include_dirs if args.include_dirs is not None else DEFAULT_INCLUDE_DIRS
        )
        final_include_extensions = (
            args.include_extensions
            if args.include_extensions is not None
            else DEFAULT_INCLUDE_EXTENSIONS
        )
        final_exclude_patterns = (
            args.exclude_patterns
            if args.exclude_patterns is not None
            else DEFAULT_EXCLUDE_PATTERNS
        )
        collect_project_files_full(
            output_file=args.output_file,
            include_dirs=final_include_dirs,
            include_extensions=final_include_extensions,
            exclude_patterns=final_exclude_patterns,
            base_dir=args.base_dir,
            clean_comments=args.clean_comments,
        )
    elif args.command == "restore":
        restore_from_full_snapshot(
            snapshot_file=args.snapshot_file,
            target_dir=args.target_dir,
            dry_run=args.dry_run,
            overwrite_existing=args.overwrite_existing,
        )


if __name__ == "__main__":
    # Example usage:
    # python tools/snapshot_generator.py collect project_full_snapshot.txt
    # python tools/snapshot_generator.py restore project_full_snapshot.txt --dry-run   
    # python tools/snapshot_generator.py restore project_full_snapshot.txt --overwrite
    print("Project Snapshot Tool (Full Version)")
    print("Collects project files into a single snapshot file or restores from a snapshot.")
    print("Use 'collect' to create a snapshot and 'restore' to restore files from it.")    
    main()

========== FILE: worker/pyproject.toml ==========
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "azuraforge-worker"
version = "0.2.3" # Versiyonu artÄ±rÄ±yoruz
description = "The Celery worker for the AzuraForge Platform. Discovers and runs pipeline plugins."
requires-python = ">=3.8"
dependencies = [
    # === DEÄžÄ°ÅžÄ°KLÄ°K BURADA: app-stock-predictor'Ä±n en son versiyonuna iÅŸaret ediyor ===
    "azuraforge-app-stock-predictor @ git+https://github.com/AzuraForge/app-stock-predictor.git@v0.1.2",
    "celery[redis]",
    "pyyaml",
    "SQLAlchemy",
    "psycopg2-binary",
]

[project.scripts]
start-worker = "azuraforge_worker.main:run_celery_worker"
========== FILE: worker/README.md ==========
# AzuraForge Worker Servisi

Bu servis, AzuraForge platformunun aÄŸÄ±r iÅŸ yÃ¼kÃ¼nÃ¼ taÅŸÄ±yan, arka plan gÃ¶revlerini iÅŸleyen motorudur.

## ðŸŽ¯ Ana Sorumluluklar

1.  **GÃ¶rev Ä°ÅŸleyici (Celery Worker):**
    *   `Redis`'teki gÃ¶rev kuyruÄŸunu dinler ve `API` tarafÄ±ndan gÃ¶nderilen yeni gÃ¶revleri (Ã¶rn: model eÄŸitimi) alÄ±r.
    *   Platforma "eklenti" olarak kurulan AI pipeline'larÄ±nÄ± (`azuraforge-app-*`) keÅŸfeder ve Ã§alÄ±ÅŸtÄ±rÄ±r.

2.  **Raporlama ve SonuÃ§ Ãœretimi:**
    *   Tamamlanan her deney iÃ§in sonuÃ§larÄ± (`results.json`) ve gÃ¶rsel raporlarÄ± (`report.md`) oluÅŸturur ve paylaÅŸÄ±lan `/reports` dizinine yazar.

3.  **Redis Pub/Sub YayÄ±ncÄ±sÄ±:**
    *   EÄŸitim sÄ±rasÄ±nda, `RedisProgressCallback` aracÄ±lÄ±ÄŸÄ±yla, anlÄ±k ilerleme verilerini (epoch, kayÄ±p deÄŸeri vb.) ilgili Redis kanalÄ±na (`task-progress:*`) yayÄ±nlayarak `API` servisinin canlÄ± takip yapmasÄ±nÄ± saÄŸlar.

## ðŸ› ï¸ Yerel GeliÅŸtirme ve Test

Bu servisi yerel ortamda Ã§alÄ±ÅŸtÄ±rmak ve test etmek iÃ§in, ana `platform` reposundaki **[GeliÅŸtirme Rehberi](../../platform/docs/DEVELOPMENT_GUIDE.md)**'ni takip edin.

Servis baÄŸÄ±mlÄ±lÄ±klarÄ± kurulduktan ve sanal ortam aktive edildikten sonra, aÅŸaÄŸÄ±daki komutla Worker'Ä± baÅŸlatabilirsiniz:

```bash
# worker/ kÃ¶k dizinindeyken
start-worker
```

Worker, Redis'e baÄŸlanacak ve yeni gÃ¶revleri beklemeye baÅŸlayacaktÄ±r.

**Birim Testleri (YakÄ±nda):**
Birim testlerini Ã§alÄ±ÅŸtÄ±rmak iÃ§in:
```bash
pytest
```

========== FILE: worker/setup.py ==========
from setuptools import setup, find_packages

setup(
    package_dir={"": "src"},
    packages=find_packages(where="src"),
)

========== FILE: worker/.github/workflows/ci.yml ==========
name: AzuraForge Worker CI

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

jobs:
  build-and-test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.10", "3.11"]

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        # Worker'Ä±n baÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± kur (versiyon etiketleriyle)
        pip install git+https://github.com/AzuraForge/app-stock-predictor.git@v0.1.0
        # Worker'Ä±n kendisini kur (test baÄŸÄ±mlÄ±lÄ±klarÄ± varsa [dev] eklenir)
        pip install -e .

    - name: Check code format with Black
      run: |
        pip install black
        black --check .
    
    - name: Lint with flake8
      run: |
        pip install flake8
        flake8 src --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 src --count --max-complexity=10 --max-line-length=127 --statistics

    - name: Test with pytest
      run: |
        # Worker iÃ§in henÃ¼z test yazmadÄ±k, bu adÄ±m ÅŸimdilik sadece pytest'in Ã§alÄ±ÅŸÄ±p
        # hiÃ§ test bulamadÄ±ÄŸÄ±nÄ± doÄŸrulayacak ve hata vermeyecektir.
        pip install pytest
        pytest
========== FILE: worker/src/azuraforge_worker/callbacks.py ==========
# worker/src/azuraforge_worker/callbacks.py

import json
import os
from typing import Any, Optional
import redis
from azuraforge_learner import Callback

class RedisProgressCallback(Callback):
    """
    Learner'dan gelen olaylarÄ± dinler ve Redis Pub/Sub kanalÄ± Ã¼zerinden
    ilerleme durumunu yayÄ±nlar.
    """
    def __init__(self, task_id: str):
        super().__init__()
        self.task_id = task_id
        self._redis_client: Optional[redis.Redis] = None
        try:
            # Worker konteyneri iÃ§inden Redis URL'sini ortam deÄŸiÅŸkeninden alÄ±r.
            redis_url = os.environ.get("REDIS_URL", "redis://redis:6379/0")
            self._redis_client = redis.from_url(redis_url)
        except Exception as e:
            print(f"HATA: RedisProgressCallback iÃ§inde Redis'e baÄŸlanÄ±lamadÄ±: {e}")

    def on_epoch_end(self, event: Any) -> None:
        """
        Her epoch sonunda Learner tarafÄ±ndan tetiklenir ve
        ilerleme verisini ilgili Redis kanalÄ±na yayÄ±nlar.
        """
        if not self._redis_client or not self.task_id:
            return
            
        # Learner tarafÄ±ndan _publish metodu ile gÃ¶nderilen payload'u (epoch_logs) alÄ±r.
        payload = event.payload
        if not payload:
            return

        try:
            channel = f"task-progress:{self.task_id}"
            message = json.dumps(payload)
            self._redis_client.publish(channel, message)
        except Exception as e:
            # EÄŸitimi durdurmamak iÃ§in hatayÄ± sadece logluyoruz.
            print(f"HATA: Redis'e ilerleme durumu yayÄ±nlanamadÄ±: {e}")
========== FILE: worker/src/azuraforge_worker/celery_app.py ==========
# worker/src/azuraforge_worker/celery_app.py

import os
from celery import Celery
from celery.signals import worker_process_init, worker_process_shutdown

REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")

celery_app = Celery(
    "azuraforge_worker",
    broker=REDIS_URL,
    backend=REDIS_URL,
    include=["azuraforge_worker.tasks.training_tasks"]
)

# Bu deÄŸiÅŸken, her bir worker sÃ¼recinin kendi motorunu tutmasÄ±nÄ± saÄŸlar.
engine = None

@worker_process_init.connect
def init_worker_db_connection(**kwargs):
    """Her bir Celery alt sÃ¼reci baÅŸladÄ±ÄŸÄ±nda Ã§aÄŸrÄ±lÄ±r."""
    global engine
    print("Initializing DB connection for worker process...")
    # === DEÄžÄ°ÅžÄ°KLÄ°K BURADA ===
    # database.py'den 'sa_create_engine' olarak import edip ismini deÄŸiÅŸtiriyoruz.
    from .database import sa_create_engine as db_create_engine
    # === DEÄžÄ°ÅžÄ°KLÄ°K SONU ===
    
    # DATABASE_URL'yi doÄŸrudan ortamdan alÄ±yoruz.
    db_url = os.getenv("DATABASE_URL")
    if not db_url:
        raise RuntimeError("DATABASE_URL not set, cannot initialize DB engine.")
        
    engine = db_create_engine(db_url)
    print(f"DB connection for worker process {os.getpid()} initialized.")


@worker_process_shutdown.connect
def shutdown_worker_db_connection(**kwargs):
    """Her bir Celery alt sÃ¼reci kapandÄ±ÄŸÄ±nda Ã§aÄŸrÄ±lÄ±r."""
    global engine
    if engine:
        print(f"Disposing DB connection for worker process {os.getpid()}...")
        engine.dispose()
========== FILE: worker/src/azuraforge_worker/database.py ==========
# worker/src/azuraforge_worker/database.py

import os
from sqlalchemy import create_engine as sa_create_engine, Column, String, JSON, DateTime
from sqlalchemy.orm import sessionmaker, declarative_base
from sqlalchemy.sql import func

Base = declarative_base()

class Experiment(Base):
    __tablename__ = "experiments"
    id = Column(String, primary_key=True, index=True)
    task_id = Column(String, index=True, nullable=False)
    batch_id = Column(String, index=True, nullable=True)
    batch_name = Column(String, nullable=True)
    pipeline_name = Column(String, index=True, nullable=False)
    status = Column(String, index=True, default="PENDING")
    config = Column(JSON, nullable=True)
    results = Column(JSON, nullable=True)
    error = Column(JSON, nullable=True)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    completed_at = Column(DateTime(timezone=True), nullable=True)
    failed_at = Column(DateTime(timezone=True), nullable=True)

    def __repr__(self):
        return f"<Experiment(id='{self.id}', status='{self.status}')>"

_SessionLocal = None

def get_session_local():
    """SessionLocal fabrikasÄ±nÄ± yalnÄ±zca gerektiÄŸinde oluÅŸturur (singleton)."""
    global _SessionLocal
    if _SessionLocal is None:
        # celery_app'ten her sÃ¼reÃ§ iÃ§in Ã¶zel olarak oluÅŸturulmuÅŸ engine'i al
        from .celery_app import engine
        if engine is None:
            raise RuntimeError("Database engine not initialized for this worker process.")
        _SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
    return _SessionLocal

def init_db():
    """Ana sÃ¼reÃ§te veritabanÄ± tablolarÄ±nÄ± oluÅŸturur."""
    DATABASE_URL = os.getenv("DATABASE_URL")
    if not DATABASE_URL:
        raise ValueError("DATABASE_URL ortam deÄŸiÅŸkeni ayarlanmamÄ±ÅŸ!")
    
    # Tablo oluÅŸturma iÅŸlemi iÃ§in geÃ§ici bir motor oluÅŸtur.
    engine = sa_create_engine(DATABASE_URL)
    Base.metadata.create_all(bind=engine)
    engine.dispose()
========== FILE: worker/src/azuraforge_worker/main.py ==========
# worker/src/azuraforge_worker/main.py

import subprocess
import sys
import platform
import logging
import multiprocessing
import os # os'i import et

def determine_pool_and_concurrency():
    """Ä°ÅŸletim sistemine ve cihaz tÃ¼rÃ¼ne gÃ¶re uygun pool ve concurrency deÄŸerini belirler."""
    current_platform = platform.system()
    device = os.environ.get("AZURAFORGE_DEVICE", "cpu").lower()

    if current_platform == "Windows":
        pool_type = "solo"
        concurrency = 1
        logging.info("Windows platformu algÄ±landÄ±. 'solo' pool kullanÄ±lÄ±yor.")
    elif device == "gpu":
        # === DEÄžÄ°ÅžÄ°KLÄ°K BURADA: GPU iÃ§in Ã¶zel concurrency ayarÄ± ===
        pool_type = "prefork"
        # Tek bir GPU varken, Ã§ok fazla paralel sÃ¼reÃ§ baÅŸlatmak verimsizdir ve OOM'a yol aÃ§abilir.
        # 2 veya 4 gibi kÃ¼Ã§Ã¼k bir deÄŸerle baÅŸlayalÄ±m.
        concurrency = 4 
        logging.info(f"GPU modu aktif. 'prefork' pool ve {concurrency} (sabit) concurrency kullanÄ±lÄ±yor.")
        # === DEÄžÄ°ÅžÄ°KLÄ°K SONU ===
    else: # CPU-bound Linux
        pool_type = "prefork"
        concurrency = multiprocessing.cpu_count()
        logging.info(f"CPU-bound Linux/macOS platformu algÄ±landÄ±. 'prefork' pool ve {concurrency} concurrency kullanÄ±lÄ±yor.")
    
    return pool_type, concurrency


def run_celery_worker():
    """'start-worker' komutu iÃ§in giriÅŸ noktasÄ±."""

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s:%(lineno)d - %(levelname)s - %(message)s',
        stream=sys.stdout
    )

    logging.info("ðŸ‘·â€â™‚ï¸ Starting AzuraForge Worker...")

    pool_type, concurrency = determine_pool_and_concurrency()

    logging.info(f"Platform: {platform.system()} - Using pool: {pool_type}, concurrency: {concurrency}")

    command = [
        # python -m celery ... yerine doÄŸrudan celery komutunu kullanmak daha standarttÄ±r
        # ve PATH sorunlarÄ± artÄ±k Dockerfile'da Ã§Ã¶zÃ¼ldÃ¼.
        "celery",
        "-A", "azuraforge_worker.celery_app:celery_app",
        "worker",
        f"--pool={pool_type}",
        "--loglevel=INFO",
        f"--concurrency={concurrency}"
    ]

    subprocess.run(command)


if __name__ == "__main__":
    run_celery_worker()
========== FILE: worker/src/azuraforge_worker/__init__.py ==========
from .celery_app import celery_app

# Bu, diÄŸer projelerin 'from azuraforge_worker import celery_app' yapabilmesini saÄŸlar.
__all__ = ("celery_app",)

========== FILE: worker/src/azuraforge_worker/tasks/training_tasks.py ==========
# worker/src/azuraforge_worker/tasks/training_tasks.py

import logging
import os
import traceback
from datetime import datetime
from importlib.metadata import entry_points
from contextlib import contextmanager

from ..celery_app import celery_app
from ..callbacks import RedisProgressCallback
from ..database import Experiment, get_session_local # DEÄžÄ°ÅžTÄ°

# --- VeritabanÄ± Oturum YÃ¶netimi ---
@contextmanager
def get_db():
    """VeritabanÄ± oturumu iÃ§in bir context manager saÄŸlar."""
    # SessionLocal'Ä± dinamik olarak al
    SessionLocal = get_session_local()
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

# --- Pipeline KeÅŸfi (DeÄŸiÅŸiklik Yok) ---
def discover_pipelines():
    logging.info("Worker: Discovering installed AzuraForge pipeline plugins and configurations...")
    discovered = {}
    try:
        pipeline_entry_points = entry_points(group='azuraforge.pipelines')
        for ep in pipeline_entry_points:
            discovered[ep.name] = {'pipeline_class': ep.load()}
        
        config_entry_points = entry_points(group='azuraforge.configs')
        for ep in config_entry_points:
            if ep.name in discovered:
                discovered[ep.name]['get_config_func'] = ep.load()
    except Exception as e:
        logging.error(f"Worker: Error discovering pipelines or configs: {e}", exc_info=True)
    
    for p_id, p_info in discovered.items():
        logging.info(f"Worker: Discovered pipeline '{p_id}' (Config available: {'get_config_func' in p_info})")
    return discovered

AVAILABLE_PIPELINES_AND_CONFIGS = discover_pipelines()
REPORTS_BASE_DIR = os.path.abspath(os.getenv("REPORTS_DIR", "/app/reports"))
os.makedirs(REPORTS_BASE_DIR, exist_ok=True)

# --- Celery GÃ¶revi (Tamamen Yenilendi) ---
@celery_app.task(bind=True, name="start_training_pipeline")
def start_training_pipeline(self, config: dict):
    task_id = self.request.id
    pipeline_name = config.get("pipeline_name", "unknown_pipeline")
    run_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    experiment_id = f"{pipeline_name}_{run_timestamp}_{task_id[:8]}"

    # Raporlama iÃ§in hala dosya sistemini kullanabiliriz
    experiment_dir = os.path.join(REPORTS_BASE_DIR, pipeline_name, experiment_id)
    os.makedirs(experiment_dir, exist_ok=True)
    
    # KonfigÃ¼rasyonu zenginleÅŸtir
    config['experiment_id'] = experiment_id
    config['task_id'] = task_id
    config['experiment_dir'] = experiment_dir
    config['start_time'] = datetime.now().isoformat()

    try:
        if not pipeline_name or pipeline_name not in AVAILABLE_PIPELINES_AND_CONFIGS:
            raise ValueError(f"Pipeline '{pipeline_name}' not found or installed.")

        # --- VeritabanÄ± KaydÄ± BaÅŸlatma ---
        with get_db() as db:
            new_experiment = Experiment(
                id=experiment_id,
                task_id=task_id,
                pipeline_name=pipeline_name,
                status="STARTED",
                config=config
            )
            db.add(new_experiment)
            db.commit()
            logging.info(f"Worker: Experiment {experiment_id} 'STARTED' olarak veritabanÄ±na kaydedildi.")

        # Pipeline'Ä± Ã§alÄ±ÅŸtÄ±r
        PipelineClass = AVAILABLE_PIPELINES_AND_CONFIGS[pipeline_name]['pipeline_class']
        pipeline_instance = PipelineClass(config)
        redis_callback = RedisProgressCallback(task_id=task_id)
        results = pipeline_instance.run(callbacks=[redis_callback])

        # --- VeritabanÄ± KaydÄ±nÄ± BaÅŸarÄ±yla GÃ¼ncelleme ---
        with get_db() as db:
            exp_to_update = db.query(Experiment).filter(Experiment.id == experiment_id).first()
            if exp_to_update:
                exp_to_update.status = "SUCCESS"
                exp_to_update.results = results
                exp_to_update.completed_at = datetime.now(datetime.utcnow().tzinfo)
                db.commit()
                logging.info(f"Worker: Experiment {experiment_id} 'SUCCESS' olarak gÃ¼ncellendi.")
        
        logging.info(f"Worker: Task {task_id} completed successfully.")
        return {"experiment_id": experiment_id, "status": "SUCCESS"}

    except Exception as e:
        tb_str = traceback.format_exc()
        logging.error(f"PIPELINE CRITICAL FAILURE in task {task_id} (experiment: {experiment_id}): {e}")
        logging.error(f"FULL TRACEBACK:\n{tb_str}")
        
        # --- VeritabanÄ± KaydÄ±nÄ± Hatayla GÃ¼ncelleme ---
        with get_db() as db:
            exp_to_update = db.query(Experiment).filter(Experiment.id == experiment_id).first()
            if exp_to_update:
                exp_to_update.status = "FAILURE"
                exp_to_update.error = {"message": str(e), "traceback": tb_str}
                exp_to_update.failed_at = datetime.now(datetime.utcnow().tzinfo)
                db.commit()
                logging.error(f"Worker: Experiment {experiment_id} 'FAILURE' olarak gÃ¼ncellendi.")
        
        raise e
========== FILE: worker/src/azuraforge_worker/tasks/__init__.py ==========
